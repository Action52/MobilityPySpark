{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a0abf1-a460-406e-8d95-4259cda8a437",
   "metadata": {},
   "source": [
    "# MobilitySpark\n",
    "***\n",
    "Repository to extend support of MEOS and MobilityDB data types into Pyspark, and experimentation on different partition techniques for trajectory data.\n",
    "This repository was used as codebase for the thesis \"MobilitySpark: Big Mobility Data Management with PySpark and PyMEOS\".\n",
    "\n",
    "To cite this project, please use:\n",
    "\n",
    "```\n",
    "León, L. (2024). *MobilitySpark: Big Mobility Data Management with PySpark and PyMEOS (Master’s thesis)*. University of Padova and Université Libre de Bruxelles.\n",
    "```\n",
    "\n",
    "To review the experimental results obtained in the thesis, please go to the */experiments* folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4f785-33db-478c-a9d5-149464c043f0",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "### Through Docker\n",
    "\n",
    "The project can be run locally using Docker. This is method is preferred if you have a\n",
    "Mac with M-Series Chip.\n",
    "\n",
    "1. Download this repository.\n",
    "2. <code>cd</code> into the *container* folder.\n",
    "3. Build the image, name it as you prefer, for example *mobilityspark*:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba275f5-e890-4082-870b-700d5e89d378",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker build -t <LOCAL-NAME-OF-THE-IMAGE> <PATH-TO-Dockerfile>\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "docker build -t mobilityspark ./\n",
    "```\n",
    "\n",
    "4. Run the container:\n",
    "\n",
    "```bash\n",
    "docker run -v <VOLUME-PATH> -p <JUPYTERLAB-PORT-FORWARD> -p <SPARK-UI-PORT-FORWARD> <CONTAINER-NAME>\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "docker run -v $(pwd)/../../../data:/data -p 8887:8888 -p 4040:4040 -p 4041:4041 -p 4042:4042 pyrtitionfinal\n",
    "```\n",
    "\n",
    "This example, for instance, runs the project mapping the volume ../../../data to the /data folder inside the container.\n",
    "This is useful if you want to use local files inside your container. Also, you need to specify:\n",
    "- Ports for the JupyterLab (recommended <code>-p 8887:8888 </code>)\n",
    "- Ports for the Spark UI (recommended <code>-p 4040:4040 </code>)\n",
    "\n",
    "You can reserve any number of ports for different Spark sessions, as in the example.\n",
    "\n",
    "This command will run the container and will end up by printing a token to start the JupyterLab. Copy the token.\n",
    "\n",
    "5. Go to your browser, and go to localhost:8887, or the port corresponding to your JupyterLab.\n",
    "6. Insert the generated token where asked. The JupyterLab will prompt.\n",
    "7. Now you can run the notebooks of the project by going to /notebooks inside the JupyterLab.\n",
    "\n",
    "\n",
    "Overall the commands to install through Docker could be as follows:\n",
    "\n",
    "```bash\n",
    "# Clone first the repo and checkout to the latest tag.\n",
    "git clone https://github.com/Action52/MobilityPySpark\n",
    "git checkout v0.1.1\n",
    "\n",
    "cd MobilityPySpark\n",
    "\n",
    "# Build the image\n",
    "docker build -t mobilityspark ./\n",
    "\n",
    "# Run the container\n",
    "docker run -v $(pwd)/../../../data:/data -p 8887:8888 -p 4040:4040 -p 4041:4041 -p 4042:4042 pyrtitionfinal\n",
    "\n",
    "# Here you can optionally access the container directly through the console, or through the JupyterLab instance (recommended)\n",
    "docker exec -it mobilityspark /bin/bash\n",
    "\n",
    "# Run script or notebook with MobilitySpark\n",
    "```\n",
    "\n",
    "### Locally\n",
    "\n",
    "To install locally:\n",
    "\n",
    "1. Download this repository.\n",
    "2. <code>cd</code> into the main folder of the repository.\n",
    "3. In a clean Python environment, run:\n",
    "\n",
    "```bash\n",
    "pip install .\n",
    "```\n",
    "\n",
    "This will install MobilitySpark and the required packages in your environment.\n",
    "\n",
    "4. If you desire to run the notebooks, now you can do, with your env activated:\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "5. Now you can run the notebooks of the project by going to /notebooks inside your local Jupyter server.\n",
    "\n",
    "Overall, the commands to install locally could be as follows:\n",
    "\n",
    "```bash\n",
    "# Use a clean environment\n",
    "conda create --name mobilitypyspark python=3.9\n",
    "conda activate mobilitypyspark\n",
    "\n",
    "# Clone the repository and checkout to the latest tag\n",
    "git clone https://github.com/Action52/MobilityPySpark\n",
    "cd MobilityPySpark\n",
    "git checkout tags/v0.1.1\n",
    "\n",
    "# Install the package\n",
    "pip install .\n",
    "\n",
    "# Run script or notebook with MobilitySpark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c57f2-5b25-43f2-bcd5-a7fb11ae5b24",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "There are three example notebooks located at the <code>/notebooks/</code> folder:\n",
    "1. *UDTDemo.ipynb* - Provides an introduction to MobilitySpark, partitioning mobility data, and reading/writing these files using PySpark.\n",
    "2. *PartitioningSchemesWithPyMEOS.ipynb* - Recreates the OpenSky experiments performed in the thesis using a sample dataset.\n",
    "3. *BerlinMODQueries.ipynb* - Recreates the BerlinMOD experiments performed in the thesis using a sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62385985-c481-4a5b-a004-79b284980b1a",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Please refer to the examples section for a more detailed introduction to MobilitySpark, particularly *UDTDemo.ipynb*.\n",
    "\n",
    "### Initiating a MobilitySpark application\n",
    "\n",
    "This is started as any PySpark application. The only extra detail is calling the *pymeos_initialize*, *udt_append* and *register_udfs_under_spark_sql* methods.\n",
    "\n",
    "```python\n",
    "from pymeos import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pysparkmeos.UDT.MeosDatatype import *\n",
    "from pysparkmeos.utils.udt_appender import udt_append\n",
    "from pysparkmeos.utils.utils import *\n",
    "from pysparkmeos.UDF.udf import *\n",
    "from pysparkmeos.partitions.grid_partitioner import GridPartition\n",
    "from pysparkmeos.UDTF.base_partition_udtf import BasePartitionUDTF\n",
    "\n",
    "# Initialize PyMEOS\n",
    "pymeos_initialize(\"UTC\")\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark UDT Example with PyMEOS\") \\\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "\n",
    "# Append the UDT mapping to the PyMEOS classes\n",
    "udt_append()\n",
    "\n",
    "# Register the UDFs in spark\n",
    "register_udfs_under_spark_sql(spark)\n",
    "```\n",
    "\n",
    "### Declaring a schema with MobilitySpark Data Type\n",
    "\n",
    "Schema management is important. If your input dataset already contains MEOS data in *well-known text (wkt)* format, Spark can read the dataset and directly process it as mobility data type.\n",
    "\n",
    "```python\n",
    "schema = StructType([\n",
    "    StructField(\"icao24\", StringType()),\n",
    "    StructField(\"Point\", TGeomPointInstUDT())  \n",
    "])\n",
    "df = spark.read.csv(data_path, header=True, schema=schema, mode='PERMISSIVE')\n",
    "```\n",
    "\n",
    "### Defining a User-Defined Type\n",
    "Users may add extra MEOS data types. A way to do it is inheriting from *MeosDatatype*.\n",
    "\n",
    "```python\n",
    "class TBoolInstWrap(TBoolInst):\n",
    "    def __setstate__(self, state):\n",
    "        pymeos_initialize()\n",
    "        self._inner = TBoolInst(state)._inner\n",
    "\n",
    "    def __getstate__(self):\n",
    "        pymeos_initialize()\n",
    "        return self.__str__()\n",
    "\n",
    "class TBoolInstUDT(MeosDatatypeUDT[TBoolInstWrap]):\n",
    "    def simpleString(self) -> str:\n",
    "        return \"tinstant\"\n",
    "\n",
    "    def from_string(self, datum: str) -> TBoolInstWrap:\n",
    "        return TBoolInstWrap(datum)\n",
    "```\n",
    "\n",
    "Finally, use monkey patching in your notebook to relate the UDT with the PyMEOS class:\n",
    "\n",
    "```python\n",
    "TBoolInst.__UDT__ = TBoolInstUDT()\n",
    "```\n",
    "\n",
    "### Calculating the Spatiotemporal Bounds of a Dataset\n",
    "\n",
    "A common and essential task to handle and properly partition mobility data, is calculating the bounds of the dataset. MobilitySpark provides a straighforward MapReduce approach to calculate the bounds of the dataset. This method is actually very quick to use even with large datasets. These map and reduce functions can be accessed from the Utilities module.\n",
    "\n",
    "```python\n",
    "bounds = df.rdd \\\n",
    "    .mapPartitions(lambda x: bounds_calculate_map(x, colname='Point')) \\\n",
    "    .reduce(bounds_calculate_reduce)\n",
    "```\n",
    "\n",
    "### Calling a MobilitySpark User-Defined Function\n",
    "\n",
    "Most of the times, operations on PyMEOS data columns will be executed with UDF's.\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"STBox\", point_to_stbox(\"Point\"))\n",
    "```\n",
    "\n",
    "### Defining a User-Defined Function\n",
    "Users may define extra UDFs according to the use case scenario. **Always** initialize PyMEOS inside the UDF. This is because PySpark distribution triggers PyMEOS across multiple nodes, where it needs to be initialized each time.\n",
    "\n",
    "```python\n",
    "@F.udf(returnType=STBoxUDT())\n",
    "def point_to_stbox(tpoint: TPoint, utc=\"UTC\")\n",
    "    pymeos_initialize(utc)\n",
    "    return tpoint.bounding_box()\n",
    "```\n",
    "\n",
    "To be accessible through Spark SQL, register the function. Also save the dataframe as a view to make it accessible through Spark SQL:\n",
    "\n",
    "```python\n",
    "spark.udf.register(\"point_to_stbox\", point_to_stbox)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"SELECT point_to_stbox(point) FROM df\")\n",
    "```\n",
    "\n",
    "### Defining a UDTF\n",
    "\n",
    "User-defined table functions are quite useful as generator functions based on existing data. They require a predefined output schema. This example \"explodes\" a trajectory column into its instant values and registers the UDTF.\n",
    "\n",
    "```python\n",
    "# Materialize Instants in dataframe\n",
    "@F.udtf(returnType=StructType([\n",
    "    StructField(\"seqId\", IntegerType()),\n",
    "    StructField(\"instant\", TGeomPointInstUDT()),\n",
    "    StructField(\"x\", DoubleType()),\n",
    "    StructField(\"y\", DoubleType()),\n",
    "    StructField(\"z\", DoubleType()),\n",
    "    StructField(\"t\", TimestampType())\n",
    "]))\n",
    "class InstantsUDTF:\n",
    "    def eval(self, row, utc=\"UTC\"):\n",
    "        pymeos_initialize(utc)\n",
    "        instants = row.movingobject.instants()\n",
    "        for instant in instants:\n",
    "            z = None\n",
    "            if instant.has_z():\n",
    "                z = instant.z().start_value()\n",
    "            yield row.trajectory_id, TGeomPointInstWrap(instant.__str__()), instant.x().start_value(), instant.y().start_value(), z, instant.start_timestamp()\n",
    "\n",
    "spark.udtf.register(\"InstantsUDTF\", InstantsUDTF)\n",
    "```\n",
    "\n",
    "### Calling a UDTF\n",
    "\n",
    "To call a registered UDTF, the user may require to leverage Spark SQL. Here, trajectories is a dataset containing PyMEOS TGeomPointSeq data. Note how we rename the 'PointSeq' column to 'movingobject' as it is required by the UDTF definition:\n",
    "\n",
    "```python\n",
    "instants = spark.sql(f\"\"\"\n",
    "        SELECT * \n",
    "        FROM InstantsUDTF(\n",
    "            TABLE(\n",
    "                    SELECT \n",
    "                        seqId AS trajectory_id,\n",
    "                        PointSeq AS movingobject\n",
    "                    FROM trajectories\n",
    "            )\n",
    "        )\n",
    "\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### Defining a Partitioner\n",
    "\n",
    "The Partitioning module is the core module in MobilitySpark. The included Partitioners are Regular Grid, KD-Tree, Adaptive Bins, and Bisective K-Means. To create a custom partitioner, simply inherit from *MobiliyPartitioner*:\n",
    "\n",
    "```python\n",
    "class CustomPartitioner(MobilityPartitioner):\n",
    "    \"\"\"\n",
    "    Class to partition mobility data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, bounds, **kwargs\n",
    "    ):\n",
    "        self.grid = [tile for tile in self._generate_grid(bounds, **kwargs)]\n",
    "        self.tilesstr = [tile.__str__() for tile in self.grid] # Required to interact with Spark\n",
    "        self.numPartitions = len(self.tilesstr)\n",
    "        super().__init__(self.numPartitions, self.get_partition)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_grid(\n",
    "        bounds: STBox,\n",
    "        **kwargs\n",
    "    ):\n",
    "        pymeos_initialize(kwargs['utc'])\n",
    "        # Implement partitioning behavior\n",
    "        tiles = []\n",
    "        return tiles\n",
    "\n",
    "    def num_partitions(self) -> int:\n",
    "        \"\"\"Return the total number of partitions.\"\"\"\n",
    "        return self.numPartitions\n",
    "```\n",
    "\n",
    "### Calling a Partitioner\n",
    "\n",
    "Calling a Partitioner with MobilitySpark is straightforward:\n",
    "\n",
    "```python\n",
    "# Regular Grid\n",
    "gp = GridPartition(bounds=bounds, cells_per_side=3)\n",
    "grid = gp.as_spark_table() # Saves it in Spark\n",
    "grid.cache()\n",
    "grid.createOrReplaceTempView(\"RegularGrid\")\n",
    "gp.plot() # Plots the grid\n",
    "```\n",
    "\n",
    "### Defining a UDTF with BasePartitionUDTF\n",
    "\n",
    "MobilitySpark provides a *BasePartitionUDTF* class to generalize the UDTF generation with Mobility data and a Partitioner. Example:\n",
    "\n",
    "```python\n",
    "schema = StructType([\n",
    "        StructField(\"instantid\", IntegerType()),\n",
    "        StructField(\"tileid\", IntegerType()),\n",
    "        StructField(\"instant\", TBoolInstUDT())])\n",
    "\n",
    "@F.udtf(returnType=schema)\n",
    "class InstantsUDTF(BasePartitionUDTF):\n",
    "    def __init__(self):\n",
    "        check_function = lambda instant, tile: instant.temporally_overlaps(tile)\n",
    "        super().__init__(\n",
    "            response_extra_cols=[], check_function=check_function, return_full_traj=True\n",
    "        )\n",
    "\n",
    "    def eval(self, row: Row):\n",
    "        for val in super().eval_wrap(row):\n",
    "            yield val\n",
    "```\n",
    "\n",
    "Here, the output schema contains 3 columns, but the user may add more as needed. The *check_function* parameter allows to use any lambda function as checker to see if the trajectory or instant is at the tile. Here, for instance, the function used is *temporally_overlaps*. The user may also define is the trajectory should be partitioned or not, as well as the names of the extra columns to process and append.\n",
    "\n",
    "### Calling a UDTF defined with BasePartitionUDTF\n",
    "\n",
    "Assuming the existance of a instants table and a grid table, it can be used as:\n",
    "\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM InstantsUDTF(\n",
    "        TABLE(\n",
    "                SELECT \n",
    "                    instantid AS trajectory_id,\n",
    "                    tboolinst_from_base_time(instant, TRUE) AS movingobject, \n",
    "                    (SELECT collect_list(tile) FROM grid) AS tiles, \n",
    "                    (SELECT collect_list(tileid) FROM grid) AS tileids\n",
    "                FROM instantsRawNoCache\n",
    "        )\n",
    "    )\n",
    "\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aed119-a5fc-44a4-8c79-8d405dbeeceb",
   "metadata": {},
   "source": [
    "## Sample plots\n",
    "\n",
    "![](./sample_charts/jette_exp2.png)\n",
    "![](./sample_charts/jette_auderguem_exp2.png)\n",
    "![](./sample_charts/dots_subplot_4_large.svg)\n",
    "\n",
    "\n",
    "\n",
    "Copyright (c) 2024, see LICENSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
