{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fe117f-48aa-4ab6-987c-49039d119911",
   "metadata": {},
   "source": [
    "# BerlinMOD Queries\n",
    "\n",
    "So far we have replicated the BerlinMOD Pymeos tutorial using Pyspark. Now we will execute the BerlinMOD queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22dd950a-e0c5-4d0c-b9a2-b53527a91ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mobilitydb-berlinmod-sf0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd \"../mobilitydb-berlinmod-sf0.1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1076035-c1e1-44e3-a9dd-d2c8fffac29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32minstants.csv\u001b[0m*  \u001b[01;32mpoints.csv\u001b[0m*       \u001b[01;32mtrips.csv\u001b[0m*       \u001b[01;32mvehicles.csv\u001b[0m*\n",
      "\u001b[01;32mlicences.csv\u001b[0m*  \u001b[01;32mregions.csv\u001b[0m*      trips_small.csv  vehicles_small.csv\n",
      "\u001b[01;32mperiods.csv\u001b[0m*   \u001b[01;34mspark-warehouse\u001b[0m/  vehicle_ids.txt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fc1f9b-a67a-45df-8d9a-d1d82a581c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86057f-63c8-4fb4-9561-f20db763733d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468925bf-ca78-4cf0-aa80-bbb83894b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymeos import *\n",
    "from pymeos.plotters import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pysparkmeos.UDT.MeosDatatype import *\n",
    "from pysparkmeos.partitions.grid.grid_partitioner import GridPartition\n",
    "from pysparkmeos.utils.udt_appender import udt_append\n",
    "from pysparkmeos.utils.utils import *\n",
    "\n",
    "\n",
    "from pysparkmeos.UDF.udf import *\n",
    "from pysparkmeos.partitions.mobilityrdd import MobilityRDD\n",
    "\n",
    "\n",
    "import random, datetime, os, sys\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone\n",
    "import contextily as cx\n",
    "import distinctipy\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import shapely.geometry as shp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely import wkb, box, from_wkb\n",
    "from typing import Union\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ada590-2fc7-4b1b-bad3-df9609ccf51e",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4df066b-5a3f-483e-a5e8-7546c7d5068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/25 18:21:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/25 18:21:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.default.parallelism: 3\n"
     ]
    }
   ],
   "source": [
    "def startspark():\n",
    "    # Initialize PyMEOS\n",
    "    pymeos_initialize(\"UTC\")\n",
    "    \n",
    "    os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    \n",
    "    #.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    # Initialize a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BerlinMOD with PySpark\") \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .config(\"spark.default.parallelism\", 3) \\\n",
    "        .config(\"spark.executor.memory\", \"3g\") \\\n",
    "        .config(\"spark.executor.cores\", 1) \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", 0) \\\n",
    "        .config(\"spark.sql.allowMultipleTableArguments.enabled\", True) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # spark.sparkContext.setLogLevel(\"INFO\")\n",
    "    \n",
    "    # Append the UDT mapping to the PyMEOS classes\n",
    "    udt_append()\n",
    "    \n",
    "    # Get the value of 'spark.default.parallelism'\n",
    "    default_parallelism = spark.sparkContext.getConf().get(\"spark.default.parallelism\")\n",
    "    print(f\"spark.default.parallelism: {default_parallelism}\")\n",
    "\n",
    "    # Register udfs in Spark SQL\n",
    "    register_udfs_under_spark_sql(spark)\n",
    "    \n",
    "    return spark\n",
    "\n",
    "spark = startspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132474c-76cd-47b5-ab64-3cd314014e6e",
   "metadata": {},
   "source": [
    "## Load Tables\n",
    "We will use the power of Spark SQL to read in the raw dataframes and then create the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717e9053-a9b1-4760-b3da-400d05c0639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32minstants.csv\u001b[0m*  \u001b[01;32mperiods.csv\u001b[0m*  \u001b[01;32mregions.csv\u001b[0m*  trips_small.csv  \u001b[01;32mvehicles.csv\u001b[0m*\n",
      "\u001b[01;32mlicences.csv\u001b[0m*  \u001b[01;32mpoints.csv\u001b[0m*   \u001b[01;32mtrips.csv\u001b[0m*    vehicle_ids.txt  vehicles_small.csv\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "070db614-efc2-41eb-af34-397671ab8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(\n",
    "    spark, \n",
    "    path, \n",
    "    tablename, \n",
    "    partition_key=None, \n",
    "    transformation_query=None,\n",
    "    partition_query = None,\n",
    "    partitioner_class = None,\n",
    "    partitioner_args = {},\n",
    "    **kwargs\n",
    "):\n",
    "    print(\"Reading raw csv \", path)\n",
    "    rawdf = spark.read.csv(path, **kwargs)\n",
    "\n",
    "    print(\"Creating temp view of raw table\")\n",
    "    rawdf.createOrReplaceTempView(f\"{tablename}RawNoCache\")\n",
    "\n",
    "    print(\"Schema and statistics of raw table\")\n",
    "    rawdf.printSchema()\n",
    "    rawdf.describe().show()\n",
    "    print(f\"Creating final table {tablename} based on {tablename}Raw, partitioned by {partition_key}.\")\n",
    "    spark.sql(f\"\"\"DROP TABLE IF EXISTS {tablename}\"\"\")\n",
    "\n",
    "    if transformation_query:\n",
    "        rawdf = spark.sql(transformation_query)\n",
    "        rawdf.createOrReplaceTempView(f\"{tablename}RawNoCache\")\n",
    "        spark.sql(f\"CACHE TABLE {tablename}Raw SELECT * FROM {tablename}RawNoCache\")\n",
    "        spark.sql(f\"SELECT * FROM {tablename}Raw LIMIT 5\").show()\n",
    "        #spark.catalog.dropTempView(f\"{tablename}RawNoCache\")\n",
    "    else:\n",
    "        spark.sql(f\"CACHE TABLE {tablename}Raw SELECT * FROM {tablename}RawNoCache\")\n",
    "        #spark.catalog.dropTempView(f\"{tablename}RawNoCache\")\n",
    "    partitioner = None\n",
    "    if partition_query:\n",
    "        bounds = rawdf.rdd.mapPartitions(bounds_calculate_map).reduce(bounds_calculate_reduce)\n",
    "        print(\"Bounds: \", bounds)\n",
    "        partitioner = partitioner_class(bounds=bounds, **partitioner_args)\n",
    "        grid = partitioner.as_spark_table()\n",
    "        grid.cache()\n",
    "        grid.show()\n",
    "        grid.createOrReplaceTempView(\"grid\")\n",
    "        print(\"Creating partitioned table... \")\n",
    "        partitionedTable = spark.sql(partition_query)\n",
    "        partitionedTable.createOrReplaceTempView(f\"{tablename}Raw\")\n",
    "\n",
    "\n",
    "    start = time()\n",
    "    \n",
    "    if partition_key:\n",
    "        df = spark.sql(f\"\"\"\n",
    "        CREATE TABLE {tablename}NoCache\n",
    "        USING parquet\n",
    "        PARTITIONED BY ({partition_key})\n",
    "        AS SELECT * FROM {tablename}Raw\n",
    "        \"\"\")\n",
    "    else:\n",
    "        df = spark.sql(f\"\"\"\n",
    "        CREATE TABLE {tablename}NoCache\n",
    "        USING parquet\n",
    "        AS SELECT * FROM {tablename}Raw\n",
    "        \"\"\")\n",
    "        \n",
    "    end = time()\n",
    "    \n",
    "    if partition_key:\n",
    "        print(f\"{tablename} partitions:\")\n",
    "        spark.sql(f\"\"\"\n",
    "        SHOW PARTITIONS {tablename}NoCache\n",
    "        \"\"\").show()\n",
    "    print(f\"Final table created in {end-start} seconds\")\n",
    "\n",
    "    spark.sql(f\"CACHE TABLE {tablename} SELECT * FROM {tablename}NoCache\")\n",
    "\n",
    "    df = spark.table(f\"{tablename}\")\n",
    "    \n",
    "    print(f\"Final table {tablename} schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    #Drop the temporary view\n",
    "    #spark.catalog.dropTempView(f\"{tablename}Raw\")\n",
    "    return df, (start, end, end-start)\n",
    "\n",
    "\n",
    "def load_all_tables(configs):\n",
    "    tables = {}\n",
    "    stats = {}\n",
    "    for tablename, config in configs.items():\n",
    "        table, stat = load_table(**config)\n",
    "        tables[tablename] = table\n",
    "        stats[tablename] = stat\n",
    "    return tables, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd062d6-e9cb-45df-9e9b-be54620d1902",
   "metadata": {},
   "source": [
    "### Instants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbdb2a17-8371-48b2-90f0-9a24ce0d3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instants, statsinstants = load_table(spark, \"instants.csv\", 'instants', inferSchema=True, header=True)\n",
    "#instants.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb35ef8-6bec-4722-9a83-e8de93483991",
   "metadata": {},
   "source": [
    "### Licences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09afb698-b7a8-4e44-aa12-21251ae18523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#licences, statslicences = load_table(spark, \"licences.csv\", 'licences', inferSchema=True, header=True)\n",
    "#licences.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e82da0-5cfb-4596-95c9-f79e6c2bfa0b",
   "metadata": {},
   "source": [
    "### Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b12ebc3-4179-4f57-978a-fb3dbed526f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transperiod = \"\"\"\n",
    "SELECT periodid, beginp, endp, tstzspan(period) AS period FROM periodsRawNoCache\n",
    "\"\"\"\n",
    "#periods, statsperiods = load_table(spark, \"periods.csv\", 'periods', transformation_query=transperiod, inferSchema=True, header=True)\n",
    "#periods.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da518bb7-2aed-4329-8d31-2f6cb0dc9f00",
   "metadata": {},
   "source": [
    "### Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060e3f55-4db1-418b-aa90-0238c8034332",
   "metadata": {},
   "outputs": [],
   "source": [
    "transpoints = \"\"\"\n",
    "SELECT pointid, posx, posy, geometry_from_hexwkb(geom) AS geom FROM pointsRawNoCache\n",
    "\"\"\"\n",
    "#points, statspoints = load_table(spark, \"points.csv\", 'points', transformation_query=transpoints, inferSchema=True, header=True)\n",
    "#points.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f2df3-511c-4e55-8ea7-fb09f208e46f",
   "metadata": {},
   "source": [
    "### Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40a4a4fb-7bd1-41d7-9576-1388fcaf6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "transregions = \"SELECT regionid, geometry_from_hexwkb(geom) AS geom FROM regionsRawNoCache\"\n",
    "#regions, statsregions = load_table(spark, \"regions.csv\", 'regions', transformation_query=transregions, inferSchema=True, header=True)\n",
    "#regions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524b97d-8e8d-430e-b287-64c765fdfd90",
   "metadata": {},
   "source": [
    "### Trips\n",
    "Note: Use trips_small for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ab82ea9-9dd0-4363-b413-aa7e385c3de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.udtf.UserDefinedTableFunction at 0x7fffbcaef8b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from typing import Iterator\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"tripid\", IntegerType()),\n",
    "    StructField(\"vehid\", IntegerType()),\n",
    "    StructField(\"day\", IntegerType()),\n",
    "    StructField(\"seqno\", IntegerType()),\n",
    "    StructField(\"sourcenode\", IntegerType()),\n",
    "    StructField(\"targetnode\", StringType()),\n",
    "    StructField(\"trip\", TGeomPointSeqSetUDT()),\n",
    "    StructField(\"trajectory\", GeometryUDT()),\n",
    "    StructField(\"license\", StringType()),\n",
    "    StructField(\"partitionKey\", IntegerType())\n",
    "])\n",
    "\n",
    "@F.udtf(returnType=schema)\n",
    "class PartitionUDTF:\n",
    "    def eval(self, row: Row):\n",
    "        pymeos_initialize()\n",
    "        sequence_id = row.tripid\n",
    "        trajectory = row.trip\n",
    "        #print(trajectory,type(trajectory))\n",
    "        #print(row.grid[0])\n",
    "        #grid = [STBoxWrap(f\"SRID=4326;{tile.__str__().strip('SRID=4326;')}\") for tile in row.grid]\n",
    "        grid = row.grid\n",
    "        gridids = row.gridids\n",
    "        partitioned = [(key, trajectory.at(tile)) for key, tile in zip(gridids, grid)]\n",
    "        #print(trajectory)\n",
    "        #print(grid)\n",
    "        #print(partitioned)\n",
    "        count = 0\n",
    "        responses = []\n",
    "        for partition_key, partition_traj in partitioned:\n",
    "            count += 1\n",
    "            if partition_traj is None:\n",
    "                continue\n",
    "            else:\n",
    "                response = (sequence_id, row.vehid, row.day, row.seqno, row.sourcenode, row.targetnode, partition_traj, row.trajectory, row.licence, partition_key)\n",
    "                yield response\n",
    "                #seqs = partition_traj.segments()\n",
    "                #print(seqs)\n",
    "                #for partition_traj_seq in seqs:\n",
    "                #    response = (sequence_id, row.vehid, row.day, row.seqno, row.sourcenode, row.targetnode, partition_traj_seq, row.trajectory, row.licence, partition_key)\n",
    "                    #responses.append(response)\n",
    "                    #yield response\n",
    "        #for response in responses:\n",
    "        #    yield response\n",
    "\n",
    "\n",
    "spark.udtf.register(\"PartitionUDTF\", PartitionUDTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d90990-4b01-4939-a56f-7d39e6d0d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add the transformation query to the trips table and inject it to the config.\n",
    "parttrips = \"\"\"\n",
    "    SELECT * \n",
    "    FROM PartitionUDTF(\n",
    "        TABLE(\n",
    "                SELECT \n",
    "                    *, \n",
    "                    (SELECT collect_list(tile) FROM grid) AS grid, \n",
    "                    (SELECT collect_list(tileid) FROM grid) AS gridids\n",
    "                FROM tripsRaw\n",
    "        )\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321f6985-9bc1-44ad-b775-944777d89d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrips, statstrips = load_table(\\n    spark, \"trips_small.csv\", \\'trips\\', \\n    transformation_query=transtrips,\\n    partition_key= \\'partitionKey\\',\\n    partition_query=parttrips,\\n    partitioner_class=GridPartition,\\n    partitioner_args = {\\'cells_per_side\\': 3},\\n    inferSchema=True,\\n    header=True\\n)\\ntrips.show()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transtrips = \"SELECT tripid, vehid, day, seqno, sourcenode, targetnode, trip_from_hexwkb(trip) AS trip, geometry_from_hexwkb(trajectory) AS trajectory, licence FROM tripsRawNoCache\"\n",
    "\n",
    "\"\"\"\n",
    "trips, statstrips = load_table(\n",
    "    spark, \"trips_small.csv\", 'trips', \n",
    "    transformation_query=transtrips,\n",
    "    partition_key= 'partitionKey',\n",
    "    partition_query=parttrips,\n",
    "    partitioner_class=GridPartition,\n",
    "    partitioner_args = {'cells_per_side': 3},\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")\n",
    "trips.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8535d1-4cb6-4e79-92d4-f8cc31a5a6b2",
   "metadata": {},
   "source": [
    "### Vehicles\n",
    "Note: Also read vehicles_small for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a37c3a23-a245-4e74-9e62-558c1bd833fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vehicles, statsvehicles = load_table(spark, \"vehicles_small.csv\", 'vehicles', inferSchema=True, header=True)\n",
    "#vehicles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2ff80-cd8c-47f2-b539-a8dc53f17388",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "First queries take a general approach and are only used to measure overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "432c9616-31ae-4bd5-91de-cc1bca394c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def query_exec(query, spark, execute=True, explain=False, explainmode=''):\n",
    "    plan = None\n",
    "    if explain:\n",
    "        plan = spark.sql(f\"EXPLAIN {explainmode} {query}\").collect()[0].plan\n",
    "    result = spark.sql(query)\n",
    "    start = time()\n",
    "    if execute:\n",
    "        result.show()\n",
    "    end = time()\n",
    "    print(\"Query execution time: \", end-start, \" seconds.\")\n",
    "    return result, (start, end, end-start), plan\n",
    "\n",
    "\n",
    "def retrieve_exec_stats(queries, starts, ends, durations, plans):\n",
    "    return pd.DataFrame({\"queries\": queries, \"start\": starts, \"end\": ends, \"duration\": durations, \"plan\": plans})\n",
    "\n",
    "\n",
    "def run_all_queries(queries, spark, execute=True, explain=True, explainmode='', printplan=False):\n",
    "    \"\"\" Utility function to run all queries through subsequent experiments \"\"\"\n",
    "    qdfs = []\n",
    "    starts = []\n",
    "    ends = []\n",
    "    durations = []\n",
    "    plans = []\n",
    "    for querytext in queries:\n",
    "        qdf, qstats, plan = query_exec(querytext, spark, execute, explain, explainmode)\n",
    "        qdfs.append(qdf)\n",
    "        starts.append(qstats[0])\n",
    "        ends.append(qstats[1])\n",
    "        durations.append(qstats[2])\n",
    "        plans.append(plan)\n",
    "        if printplan:\n",
    "            print(plan)\n",
    "    exec_stats = retrieve_exec_stats(queries, starts, ends, durations, plans)\n",
    "    return qdfs, exec_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb526c-13c6-48ee-9b9b-6b906cc39558",
   "metadata": {},
   "source": [
    "### Query 1: What are the models of the vehicles with licence plate numbers from QueryLicences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "949ce841-db85-4913-8705-b1636a14ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytext1 = \"\"\"\n",
    "    SELECT l.licence, v.model\n",
    "    FROM licences l, vehicles v\n",
    "    WHERE l.licence = v.licence\n",
    "\"\"\"\n",
    "#q1, q1stats, plan1 = query_exec(querytext1, spark, explain=True)\n",
    "#if plan1:\n",
    "#    print(plan1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb68fd4-24d2-4146-9092-9d35f5a33142",
   "metadata": {},
   "source": [
    "### Query 2: How many vehicles exist that are 'passenger' cars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aeaa634-3b72-416c-93a5-45ee013df74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytext2 = \"\"\"\n",
    "    SELECT COUNT(licence) AS PassengerCarCount\n",
    "    FROM vehicles\n",
    "    WHERE type='passenger'\n",
    "\"\"\"\n",
    "#q2, q2stats, plan2 = query_exec(querytext2, spark, explain=True)\n",
    "#if plan2:\n",
    "#    print(plan2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17ee5d-5ab1-4018-8db6-1c2a7e61fb9c",
   "metadata": {},
   "source": [
    "### Query 3: Where have the vehicles with licences from QueryLicences1 been at each of the instants from QueryInstants1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0156d33-b304-4e1a-a917-331e6e0e3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "querytext3 = \"\"\"\n",
    "    WITH\n",
    "    veh_w_lic AS (\n",
    "        SELECT v.vehid, l.licence, v.model\n",
    "        FROM licences l, vehicles v\n",
    "        WHERE l.licence = v.licence\n",
    "    ),\n",
    "    veh_trips AS (\n",
    "        SELECT t.* \n",
    "        FROM veh_w_lic vw, trips t\n",
    "        WHERE t.vehid = vw.vehid\n",
    "    ),\n",
    "    tile_instants AS (\n",
    "        SELECT /*+ BROADCAST(gr) */ gr.tile, i.instant\n",
    "        FROM grid gr, instants i\n",
    "        WHERE contains_stbox_stbox(gr.tile, i.instant) = TRUE\n",
    "    )\n",
    "    SELECT /*+ BROADCAST(i) */ vt.vehid, vt.tripid, vt.trip, i.instant, tpoint_at(vt.trip, i.instant) AS pos\n",
    "    FROM veh_trips vt, tile_instants i\n",
    "\"\"\"\n",
    "#q3, q3stats, plan3 = query_exec(querytext3, spark, explain=True)\n",
    "#if plan3:\n",
    "#    print(plan3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f096b-ddd3-4130-a07d-844f39e6a0be",
   "metadata": {},
   "source": [
    "### Query 4: Which licence plate numbers belong to vehicles that have passed the points from QueryPoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dabe818-ac5b-4e65-96ff-55e251791161",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytext4 = \"\"\"\n",
    "    WITH \n",
    "    vehids_intersect AS (\n",
    "        SELECT t.vehid\n",
    "        FROM trips t, points p\n",
    "        WHERE ever_touches(t.trip, p.geom) = TRUE\n",
    "    )\n",
    "    SELECT vi.vehid, v.licence\n",
    "    FROM vehids_intersect vi, vehicles v\n",
    "\"\"\"\n",
    "#q4, q4stats, plan4 = query_exec(querytext4, spark, explain=True)\n",
    "#if plan4:\n",
    "#    print(plan4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0302e785-2667-4d07-ad31-b046a0f7aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 18:21:31 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|temp_clm|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df = spark.createDataFrame([\"0\"], \"string\").toDF(\"temp_clm\")\n",
    "dummy_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c9775-97ba-4e8e-b65b-15e35be83845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a4cff-57fb-4d63-8795-5f3372854252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebba818e-647c-4d2a-a4bd-4aa6abca9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [querytext1, querytext2] #, querytext3] #, querytext4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdffb7-9ce6-4891-8d02-bb87ef1c8df6",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b4f48-ab51-4aca-a460-d110fe1872f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment 1: Run Queries ASIS\n",
    "First we are going to run the queries without any improvement or partitioning strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3d0a7a8-085d-4c9c-b7fc-1013e163693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exp_1 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e7317-8ef6-4a00-9500-34c4fd56cb84",
   "metadata": {},
   "source": [
    "#### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f5451e8-2349-444f-ba0f-9a79710c3fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/rm: cannot remove 'spark-warehouse/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d732540-70d2-4da7-a458-d660cc7486ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp1 = {\n",
    "    'instants': {'spark': spark, 'path': 'instants.csv', 'tablename': 'instants', 'inferSchema': True, 'header': True},\n",
    "    'licences': {'spark': spark, 'path': 'licences.csv', 'tablename': 'licences', 'inferSchema': True, 'header': True},\n",
    "    'periods':  {'spark': spark, 'path': 'periods.csv', 'tablename': 'periods', 'transformation_query': transperiod, 'inferSchema': True, 'header': True},\n",
    "    'points':   {'spark': spark, 'path': 'points.csv', 'tablename': 'points', 'transformation_query': transpoints, 'inferSchema': True, 'header': True},\n",
    "    'regions':  {'spark': spark, 'path': 'regions.csv', 'tablename': 'regions', 'transformation_query':transregions, 'inferSchema': True, 'header': True},\n",
    "    'trips':    {'spark': spark, 'path': 'trips_small.csv', 'tablename': 'trips', 'transformation_query':transtrips, 'inferSchema': True, 'header': True},\n",
    "    'vehicles': {'spark': spark, 'path': 'vehicles_small.csv', 'tablename': 'vehicles', 'inferSchema': True, 'header': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2daf0c1-03dd-4813-9afd-62e3ab4b3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_1:\n",
    "    tables, stats = load_all_tables(configs_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd607345-7614-4fcf-94c6-3d273236a70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|num|squared|\n",
      "+---+-------+\n",
      "|  1|      1|\n",
      "|  2|      4|\n",
      "|  3|      9|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@F.udtf(returnType=\"num: int, squared: int\")\n",
    "class SquareNumbers:\n",
    "    def eval(self, start: int, end: int):\n",
    "        for num in range(start, end + 1):\n",
    "            yield (num, num * num)\n",
    "\n",
    "SquareNumbers(F.lit(1), F.lit(3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "860d705f-3abd-4290-9edd-cd4520d1d837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.udtf.UserDefinedTableFunction at 0x7fffbca9ddf0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " |-- tripid: integer (nullable = true)\n",
    " |-- vehid: integer (nullable = true)\n",
    " |-- day: date (nullable = true)\n",
    " |-- seqno: integer (nullable = true)\n",
    " |-- sourcenode: integer (nullable = true)\n",
    " |-- targetnode: integer (nullable = true)\n",
    " |-- trip: pythonuserdefined (nullable = true)\n",
    " |-- trajectory: pythonuserdefined (nullable = true)\n",
    "\"\"\"\n",
    "schema = StructType([\n",
    "    StructField(\"point\", TGeomPointInstUDT())\n",
    "])\n",
    "@F.udtf(returnType=schema)\n",
    "class ExplodeGeomSeq:\n",
    "    def eval(self, trip: TGeomPointSeqWrap):\n",
    "        #print(trip['trip'])\n",
    "        #trip = trip.trip\n",
    "        pymeos_initialize()\n",
    "        instants = trip['trip'].instants()\n",
    "        for i in instants:\n",
    "            yield i,\n",
    "\n",
    "spark.udtf.register(\"explodeGeomSeq\", ExplodeGeomSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68e5f9b1-9f49-472a-94a4-ba7ffba68132",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_1:\n",
    "    spark.sql(\"SELECT * FROM explodeGeomSeq(TABLE(SELECT trip FROM trips))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf04af-6edb-4119-aa86-133a3afd1d5b",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ba12c8d-0d99-447a-b278-3b7276a3c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_1:\n",
    "    qdfs_exp1, stats_exp1 = run_all_queries(queries, spark, explain=True, printplan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a70d15ac-93c5-4e9e-9418-2b8424c70ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_1:\n",
    "    for (_id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "        rdd.unpersist()\n",
    "        print(\"Unpersisted {} rdd\".format(_id))\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b770606-3841-401a-b721-eafabd3e7cbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment 2: Partition Trips by vehid, HashPartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1be2ccf-4cca-42c3-8158-e178904c0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exp_2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba00958e-6711-4cc8-9924-2d5f3f829dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/rm: cannot remove 'rm': No such file or directory\n",
      "/bin/rm: cannot remove 'spark-warehouse/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm -R rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d4ddff5-99bf-4e67-9612-f4b3bc3d30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_2:\n",
    "    spark = startspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10871d-410c-48f3-98a6-53700dc7450c",
   "metadata": {},
   "source": [
    "#### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "985b9d3b-84d7-4934-adf0-83046a8bf7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp2 = {\n",
    "    'instants': {'spark': spark, 'path': 'instants.csv', 'tablename': 'instants', 'inferSchema': True, 'header': True},\n",
    "    'licences': {'spark': spark, 'path': 'licences.csv', 'tablename': 'licences', 'inferSchema': True, 'header': True},\n",
    "    'periods':  {'spark': spark, 'path': 'periods.csv', 'tablename': 'periods', 'transformation_query': transperiod, 'inferSchema': True, 'header': True},\n",
    "    'points':   {'spark': spark, 'path': 'points.csv', 'tablename': 'points', 'transformation_query': transpoints, 'inferSchema': True, 'header': True},\n",
    "    'regions':  {'spark': spark, 'path': 'regions.csv', 'tablename': 'regions', 'transformation_query':transregions, 'inferSchema': True, 'header': True},\n",
    "    'trips':    {'spark': spark, 'path': 'trips_small.csv', 'tablename': 'trips', 'partition_key': 'vehid', 'transformation_query':transtrips, 'inferSchema': True, 'header': True},\n",
    "    'vehicles': {'spark': spark, 'path': 'vehicles_small.csv', 'tablename': 'vehicles', 'inferSchema': True, 'header': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd33623a-819a-4f8b-83fe-a5348df6b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_2:\n",
    "    tables, stats = load_all_tables(configs_exp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed2844-15fd-4b75-ab07-b70995d3ab5c",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "514e9f39-27de-4e99-86c4-0d002f859c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_2:\n",
    "    qdfs_exp2, stats_exp2 = run_all_queries(queries, spark, explain=True, printplan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e154ab01-e8f4-4992-b6a3-147a421ede36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d13d9a-4412-4518-bfc2-2014b610a08e",
   "metadata": {},
   "source": [
    "### Experiment 3: Partition by Trip, RegularGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a08c76bd-bb70-4e69-8eb0-0dff674eece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exp_3 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfd4c6b5-2428-4fae-98ca-a9678045b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/rm: cannot remove 'rm': No such file or directory\n",
      "/bin/rm: cannot remove 'spark-warehouse/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm -R rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a908c86-6cf2-4ec8-87f3-2c0ad8989163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 18:21:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.default.parallelism: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39980)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if run_exp_3:\n",
    "    spark = startspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bfdded-cc45-4b53-84fc-6e8077659f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad60f1d8-e0c5-483e-9b8e-fd3e1e414bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf26e353-b645-45ca-b994-c556010549e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp3 = {\n",
    "    'instants': {'spark': spark, 'path': 'instants.csv', 'tablename': 'instants', 'inferSchema': True, 'header': True},\n",
    "    'licences': {'spark': spark, 'path': 'licences.csv', 'tablename': 'licences', 'inferSchema': True, 'header': True},\n",
    "    'periods':  {'spark': spark, 'path': 'periods.csv', 'tablename': 'periods', 'transformation_query': transperiod, 'inferSchema': True, 'header': True},\n",
    "    'points':   {'spark': spark, 'path': 'points.csv', 'tablename': 'points', 'transformation_query': transpoints, 'inferSchema': True, 'header': True},\n",
    "    'regions':  {'spark': spark, 'path': 'regions.csv', 'tablename': 'regions', 'transformation_query':transregions, 'inferSchema': True, 'header': True},\n",
    "    'trips':    {\n",
    "        'spark': spark, \n",
    "        'path': 'trips_small.csv', \n",
    "        'tablename': 'trips', \n",
    "        'partition_key': 'partitionKey', \n",
    "        'transformation_query':transtrips,\n",
    "        'partition_query': parttrips,\n",
    "        'partitioner_class': GridPartition,\n",
    "        'partitioner_args': {'cells_per_side': 3},\n",
    "        'inferSchema': True, \n",
    "        'header': True},\n",
    "    'vehicles': {'spark': spark, 'path': 'vehicles_small.csv', 'tablename': 'vehicles', 'inferSchema': True, 'header': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c314039-c87a-43fb-911f-375fa35e4dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw csv  instants.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- instantid: integer (nullable = true)\n",
      " |-- instant: timestamp (nullable = true)\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|         instantid|\n",
      "+-------+------------------+\n",
      "|  count|               100|\n",
      "|   mean|              50.5|\n",
      "| stddev|29.011491975882016|\n",
      "|    min|                 1|\n",
      "|    max|               100|\n",
      "+-------+------------------+\n",
      "\n",
      "Creating final table instants based on instantsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 4.297498464584351 seconds\n",
      "Final table instants schema:\n",
      "root\n",
      " |-- instantid: integer (nullable = true)\n",
      " |-- instant: timestamp (nullable = true)\n",
      "\n",
      "Reading raw csv  licences.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- licenceid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------+------------------+\n",
      "|summary|         licenceid| licence|             vehid|\n",
      "+-------+------------------+--------+------------------+\n",
      "|  count|               101|     101|               101|\n",
      "|   mean|              51.0|    NULL|319.46534653465346|\n",
      "| stddev|29.300170647967224|    NULL| 175.0106604956644|\n",
      "|    min|                 1|B-BJ 115|                 9|\n",
      "|    max|               101|B-[U 177|               622|\n",
      "+-------+------------------+--------+------------------+\n",
      "\n",
      "Creating final table licences based on licencesRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 2.0499801635742188 seconds\n",
      "Final table licences schema:\n",
      "root\n",
      " |-- licenceid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      "\n",
      "Reading raw csv  periods.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- periodid: integer (nullable = true)\n",
      " |-- beginp: timestamp (nullable = true)\n",
      " |-- endp: timestamp (nullable = true)\n",
      " |-- period: string (nullable = true)\n",
      "\n",
      "+-------+------------------+--------------------+\n",
      "|summary|          periodid|              period|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|               100|                 100|\n",
      "|   mean|              50.5|                NULL|\n",
      "| stddev|29.011491975882016|                NULL|\n",
      "|    min|                 1|[2020-06-01 00:45...|\n",
      "|    max|               100|[2020-06-11 21:18...|\n",
      "+-------+------------------+--------------------+\n",
      "\n",
      "Creating final table periods based on periodsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "|periodid|              beginp|                endp|              period|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|       1|2020-06-09 11:15:...|2020-06-09 20:38:...|[2020-06-09 11:15...|\n",
      "|       2|2020-06-10 10:55:...|2020-06-11 01:01:...|[2020-06-10 10:55...|\n",
      "|       3|2020-06-04 06:42:...|2020-06-05 02:50:...|[2020-06-04 06:42...|\n",
      "|       4|2020-06-05 04:39:...|2020-06-06 05:48:...|[2020-06-05 04:39...|\n",
      "|       5|2020-06-06 09:10:...|2020-06-07 03:59:...|[2020-06-06 09:10...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 2.219373941421509 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table periods schema:\n",
      "root\n",
      " |-- periodid: integer (nullable = true)\n",
      " |-- beginp: timestamp (nullable = true)\n",
      " |-- endp: timestamp (nullable = true)\n",
      " |-- period: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  points.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- pointid: integer (nullable = true)\n",
      " |-- posx: double (nullable = true)\n",
      " |-- posy: double (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 18:22:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "|summary|           pointid|             posx|             posy|                geom|\n",
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "|  count|               100|              100|              100|                 100|\n",
      "|   mean|              50.5|486384.3413598945|6594038.933758076|                NULL|\n",
      "| stddev|29.011491975882016|7200.526060474747|6552.156274876073|                NULL|\n",
      "|    min|                 1|472428.0634008836|6577421.541139536|0101000020110F000...|\n",
      "|    max|               100| 498913.875699313|6607119.513588189|0101000020110F000...|\n",
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "\n",
      "Creating final table points based on pointsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+--------------------+\n",
      "|pointid|              posx|             posy|                geom|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "|      1| 476191.0852037612|6589454.831155596|POINT (476191.085...|\n",
      "|      2| 485998.9668637461|6580934.403927697|POINT (485998.966...|\n",
      "|      3|486927.13764603145|  6584864.3484669|POINT (486927.137...|\n",
      "|      4|491514.42461848777|6594412.284642856|POINT (491514.424...|\n",
      "|      5| 493018.1394320724|6602300.271879816|POINT (493018.139...|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 1.8431105613708496 seconds\n",
      "Final table points schema:\n",
      "root\n",
      " |-- pointid: integer (nullable = true)\n",
      " |-- posx: double (nullable = true)\n",
      " |-- posy: double (nullable = true)\n",
      " |-- geom: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  regions.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- regionid: integer (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      "\n",
      "+-------+------------------+--------------------+\n",
      "|summary|          regionid|                geom|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|               100|                 100|\n",
      "|   mean|              50.5|                NULL|\n",
      "| stddev|29.011491975882016|                NULL|\n",
      "|    min|                 1|0103000020110F000...|\n",
      "|    max|               100|0103000020110F000...|\n",
      "+-------+------------------+--------------------+\n",
      "\n",
      "Creating final table regions based on regionsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|regionid|                geom|\n",
      "+--------+--------------------+\n",
      "|       1|POLYGON ((483571....|\n",
      "|       2|POLYGON ((485438....|\n",
      "|       3|POLYGON ((486542....|\n",
      "|       4|POLYGON ((488077....|\n",
      "|       5|POLYGON ((482151....|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 1.9060332775115967 seconds\n",
      "Final table regions schema:\n",
      "root\n",
      " |-- regionid: integer (nullable = true)\n",
      " |-- geom: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  trips_small.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- day: date (nullable = true)\n",
      " |-- seqno: integer (nullable = true)\n",
      " |-- sourcenode: integer (nullable = true)\n",
      " |-- targetnode: integer (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- trajectory: string (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+-----------------+--------------------+--------------------+-------+\n",
      "|summary|           tripid|             vehid|             seqno|        sourcenode|       targetnode|                trip|          trajectory|licence|\n",
      "+-------+-----------------+------------------+------------------+------------------+-----------------+--------------------+--------------------+-------+\n",
      "|  count|               91|                91|                91|                91|               91|                  91|                  91|      0|\n",
      "|   mean|304.3296703296703|10.956043956043956|2.5934065934065935| 39454.89010989011|39454.89010989011|                NULL|                NULL|   NULL|\n",
      "| stddev|204.9905176638967| 7.067786213065726| 1.666520140079134|28341.445037436115|28341.44503743612|                NULL|                NULL|   NULL|\n",
      "|    min|                1|                 1|                 1|              1160|             1160|[0101000020110F00...|0102000020110F000...|   NULL|\n",
      "|    max|              489|                17|                 9|             79113|            79113|[0101000020110F00...|0102000020110F000...|   NULL|\n",
      "+-------+-----------------+------------------+------------------+------------------+-----------------+--------------------+--------------------+-------+\n",
      "\n",
      "Creating final table trips based on tripsRaw, partitioned by partitionKey.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "|tripid|vehid|       day|seqno|sourcenode|targetnode|                trip|          trajectory|licence|\n",
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "|     1|    1|2020-06-01|    1|     79113|     66276|[POINT(496253.840...|LINESTRING (49625...|   NULL|\n",
      "|     2|    1|2020-06-01|    2|     66276|     79113|[POINT(481241.171...|LINESTRING (48124...|   NULL|\n",
      "|     3|    1|2020-06-02|    1|     79113|     66276|[POINT(496253.840...|LINESTRING (49625...|   NULL|\n",
      "|     4|    1|2020-06-02|    2|     66276|     79113|[POINT(481241.171...|LINESTRING (48124...|   NULL|\n",
      "|     5|    1|2020-06-03|    1|     79113|     66276|[POINT(496253.840...|LINESTRING (49625...|   NULL|\n",
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounds:  STBOX XT(((473277.05262936745,6579811.389156611),(498784.34433982597,6606871.682578203)),[2020-06-01 06:01:41.054+00, 2020-06-11 19:30:26.096307+00])\n",
      "+------+--------------------+\n",
      "|tileid|                tile|\n",
      "+------+--------------------+\n",
      "|     0|STBOX XT(((473277...|\n",
      "|     1|STBOX XT(((473277...|\n",
      "|     2|STBOX XT(((473277...|\n",
      "|     3|STBOX XT(((473277...|\n",
      "|     4|STBOX XT(((473277...|\n",
      "|     5|STBOX XT(((473277...|\n",
      "|     6|STBOX XT(((473277...|\n",
      "|     7|STBOX XT(((473277...|\n",
      "|     8|STBOX XT(((473277...|\n",
      "|     9|STBOX XT(((481779...|\n",
      "|    10|STBOX XT(((481779...|\n",
      "|    11|STBOX XT(((481779...|\n",
      "|    12|STBOX XT(((481779...|\n",
      "|    13|STBOX XT(((481779...|\n",
      "|    14|STBOX XT(((481779...|\n",
      "|    15|STBOX XT(((481779...|\n",
      "|    16|STBOX XT(((481779...|\n",
      "|    17|STBOX XT(((481779...|\n",
      "|    18|STBOX XT(((490281...|\n",
      "|    19|STBOX XT(((490281...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Creating partitioned table... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trips partitions:\n",
      "+---------------+\n",
      "|      partition|\n",
      "+---------------+\n",
      "| partitionKey=0|\n",
      "| partitionKey=1|\n",
      "|partitionKey=10|\n",
      "|partitionKey=12|\n",
      "|partitionKey=13|\n",
      "|partitionKey=14|\n",
      "|partitionKey=17|\n",
      "| partitionKey=2|\n",
      "|partitionKey=21|\n",
      "|partitionKey=22|\n",
      "|partitionKey=23|\n",
      "|partitionKey=24|\n",
      "|partitionKey=25|\n",
      "|partitionKey=26|\n",
      "| partitionKey=3|\n",
      "| partitionKey=4|\n",
      "| partitionKey=5|\n",
      "| partitionKey=6|\n",
      "| partitionKey=7|\n",
      "| partitionKey=8|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Final table created in 22.16624641418457 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table trips schema:\n",
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- seqno: integer (nullable = true)\n",
      " |-- sourcenode: integer (nullable = true)\n",
      " |-- targetnode: string (nullable = true)\n",
      " |-- trip: pythonuserdefined (nullable = true)\n",
      " |-- trajectory: pythonuserdefined (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- partitionKey: integer (nullable = true)\n",
      "\n",
      "Reading raw csv  vehicles_small.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n",
      "+-------+-----------------+-------+-----+--------+\n",
      "|summary|            vehid|licence| type|   model|\n",
      "+-------+-----------------+-------+-----+--------+\n",
      "|  count|                3|      3|    3|       3|\n",
      "|   mean|             11.0|   NULL| NULL|    NULL|\n",
      "| stddev|8.717797887081348|   NULL| NULL|    NULL|\n",
      "|    min|                1|B-CJ 17|  bus|    Opel|\n",
      "|    max|               17|B-PZ 15|truck|Wartburg|\n",
      "+-------+-----------------+-------+-----+--------+\n",
      "\n",
      "Creating final table vehicles based on vehiclesRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 2.4541289806365967 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table vehicles schema:\n",
      "root\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run_exp_3:\n",
    "    spark.udtf.register(\"PartitionUDTF\", PartitionUDTF)\n",
    "    tables, stats = load_all_tables(configs_exp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c92113c5-615b-47ee-9c71-da7cf10b64bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|tripid|cnt|\n",
      "+------+---+\n",
      "|   471|  3|\n",
      "|   481|  3|\n",
      "|   472|  2|\n",
      "|    28|  5|\n",
      "|   436|  1|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT tripid, COUNT(trip) AS cnt FROM trips GROUP BY tripid LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81bf9dda-ef14-4153-be55-42e33b61c1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "trip = spark.sql(\"SELECT * FROM trips LIMIT 1\").collect()[0].trip\n",
    "instant = spark.sql(\"SELECT * FROM instants LIMIT 1\").collect()[0].instant\n",
    "\n",
    "print(trip.at(instant))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133ce92-9f13-4b87-a2fe-2a971fc6857a",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a61da44d-f821-48ff-85a9-aa9a14c92632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|licence|model|\n",
      "+-------+-----+\n",
      "|B-CJ 17| Opel|\n",
      "+-------+-----+\n",
      "\n",
      "Query execution time:  0.7265965938568115  seconds.\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [licence#6268, model#6273]\n",
      "   +- BroadcastHashJoin [licence#6268], [licence#6271], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(licence#6268)\n",
      "      :  +- Scan In-memory table licences [licence#6268], [isnotnull(licence#6268)]\n",
      "      :        +- InMemoryRelation [licenceid#6267, licence#6268, vehid#6269], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :              +- *(1) ColumnarToRow\n",
      "      :                 +- FileScan parquet spark_catalog.default.licencesnocache[licenceid#823,licence#824,vehid#825] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/data/mobilitydb-berlinmod-sf0.1/spark-warehouse/licencesnocache], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<licenceid:int,licence:string,vehid:int>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=2018]\n",
      "         +- Filter isnotnull(licence#6271)\n",
      "            +- Scan In-memory table vehicles [licence#6271, model#6273], [isnotnull(licence#6271)]\n",
      "                  +- InMemoryRelation [vehid#6270, licence#6271, type#6272, model#6273], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                        +- *(1) ColumnarToRow\n",
      "                           +- FileScan parquet spark_catalog.default.vehiclesnocache[vehid#5569,licence#5570,type#5571,model#5572] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/data/mobilitydb-berlinmod-sf0.1/spark-warehouse/vehiclesnocache], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<vehid:int,licence:string,type:string,model:string>\n",
      "\n",
      "\n",
      "+-----------------+\n",
      "|PassengerCarCount|\n",
      "+-----------------+\n",
      "|                1|\n",
      "+-----------------+\n",
      "\n",
      "Query execution time:  0.36557579040527344  seconds.\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[count(licence#6565)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=2145]\n",
      "      +- HashAggregate(keys=[], functions=[partial_count(licence#6565)])\n",
      "         +- Project [licence#6565]\n",
      "            +- Filter (isnotnull(type#6566) AND (type#6566 = passenger))\n",
      "               +- Scan In-memory table vehicles [licence#6565, type#6566], [isnotnull(type#6566), (type#6566 = passenger)]\n",
      "                     +- InMemoryRelation [vehid#6564, licence#6565, type#6566, model#6567], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- *(1) ColumnarToRow\n",
      "                              +- FileScan parquet spark_catalog.default.vehiclesnocache[vehid#5569,licence#5570,type#5571,model#5572] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/data/mobilitydb-berlinmod-sf0.1/spark-warehouse/vehiclesnocache], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<vehid:int,licence:string,type:string,model:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run_exp_3:\n",
    "    qdfs_exp3, stats_exp3 = run_all_queries(queries, spark, explain=True, printplan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4419940b-85a9-40b7-b120-a68758101cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 18:30:23 WARN ExtractPythonUDFFromJoinCondition: The join condition:contains_stbox_stbox(tile#4306, instant#9473)#9474 of the join plan contains PythonUDF only, it will be moved out and the join plan will be turned to cross join.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+--------------------+----+\n",
      "|vehid|tripid|                trip|             instant| pos|\n",
      "+-----+------+--------------------+--------------------+----+\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-01 19:44:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-01 19:44:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-01 19:44:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-01 19:44:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-01 19:44:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-01 19:44:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-01 19:44:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-01 19:44:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-01 19:44:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-05 23:09:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-05 23:09:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-05 23:09:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-05 23:09:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-05 23:09:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-05 23:09:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-05 23:09:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-05 23:09:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-05 23:09:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-02 07:23:...|NULL|\n",
      "|   17|   464|{[POINT(479113.66...|2020-06-02 07:23:...|NULL|\n",
      "+-----+------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 18:31:11 WARN ExtractPythonUDFFromJoinCondition: The join condition:contains_stbox_stbox(tile#4306, instant#9473)#9474 of the join plan contains PythonUDF only, it will be moved out and the join plan will be turned to cross join.\n",
      "ERROR:root:Exception while sending command.                         (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:33\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "querytext3 = \"\"\"\n",
    "    WITH\n",
    "    littletrips AS (\n",
    "        SELECT * FROM trips\n",
    "    ),\n",
    "    veh_w_lic AS (\n",
    "        SELECT v.vehid, l.licence, v.model\n",
    "        FROM licences l, vehicles v\n",
    "        WHERE l.licence = v.licence\n",
    "    ),\n",
    "    veh_trips AS (\n",
    "        SELECT t.* \n",
    "        FROM veh_w_lic vw, littletrips t\n",
    "        WHERE t.vehid = vw.vehid\n",
    "    ),\n",
    "    tile_instants AS (\n",
    "        SELECT /*+ BROADCAST(gr) */ gr.tile, i.instant\n",
    "        FROM grid gr, instants i\n",
    "        WHERE contains_stbox_stbox(gr.tile, i.instant) = TRUE\n",
    "    )\n",
    "    SELECT /*+ BROADCAST(i) */ vt.vehid, vt.tripid, vt.trip, i.instant, tpoint_at(vt.trip, i.instant) AS pos\n",
    "    FROM veh_trips vt, tile_instants i\n",
    "\"\"\"\n",
    "\n",
    "q3 = spark.sql(querytext3)\n",
    "q3.show()\n",
    "\n",
    "def delete_nulls(partition):\n",
    "    for row in partition:\n",
    "        if row.pos != None:\n",
    "            yield row\n",
    "    \n",
    "q3.rdd.mapPartitions(delete_nulls).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6753150c-3ef4-4aab-88d4-a6b5da88e5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 ms, sys: 5.83 ms, total: 34.1 ms\n",
      "Wall time: 132 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.udf.UserDefinedFunction at 0x7fffbcb42f40>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "@F.udf(returnType=BooleanType())\n",
    "def contains_stbox_stbox(stbox, other):\n",
    "    pymeos_initialize()\n",
    "    return stbox.contains(other)\n",
    "spark.udf.register(\"contains_stbox_stbox\", contains_stbox_stbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df1719ab-ae55-417a-b5e7-7a7153ee94f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tileid|tile                                                                                                                                         |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0     |STBOX XT(((-179.99978893675143,-89.99724849779159),(-60.00591066159541,-30.00028948827336)),[2020-06-01 06:01:41+00, 2020-06-04 18:31:16+00])|\n",
      "|1     |STBOX XT(((-179.99978893675143,-89.99724849779159),(-60.00591066159541,-30.00028948827336)),[2020-06-04 18:31:16+00, 2020-06-08 07:00:51+00])|\n",
      "|2     |STBOX XT(((-179.99978893675143,-89.99724849779159),(-60.00591066159541,-30.00028948827336)),[2020-06-08 07:00:51+00, 2020-06-11 19:30:26+00])|\n",
      "|3     |STBOX XT(((-179.99978893675143,-30.00028948827336),(-60.00591066159541,29.99666952124487)),[2020-06-01 06:01:41+00, 2020-06-04 18:31:16+00]) |\n",
      "|4     |STBOX XT(((-179.99978893675143,-30.00028948827336),(-60.00591066159541,29.99666952124487)),[2020-06-04 18:31:16+00, 2020-06-08 07:00:51+00]) |\n",
      "|5     |STBOX XT(((-179.99978893675143,-30.00028948827336),(-60.00591066159541,29.99666952124487)),[2020-06-08 07:00:51+00, 2020-06-11 19:30:26+00]) |\n",
      "|6     |STBOX XT(((-179.99978893675143,29.99666952124487),(-60.00591066159541,89.99362853076309)),[2020-06-01 06:01:41+00, 2020-06-04 18:31:16+00])  |\n",
      "|7     |STBOX XT(((-179.99978893675143,29.99666952124487),(-60.00591066159541,89.99362853076309)),[2020-06-04 18:31:16+00, 2020-06-08 07:00:51+00])  |\n",
      "|8     |STBOX XT(((-179.99978893675143,29.99666952124487),(-60.00591066159541,89.99362853076309)),[2020-06-08 07:00:51+00, 2020-06-11 19:30:26+00])  |\n",
      "|9     |STBOX XT(((-60.00591066159541,-89.99724849779159),(59.98796761356061,-30.00028948827336)),[2020-06-01 06:01:41+00, 2020-06-04 18:31:16+00])  |\n",
      "|10    |STBOX XT(((-60.00591066159541,-89.99724849779159),(59.98796761356061,-30.00028948827336)),[2020-06-04 18:31:16+00, 2020-06-08 07:00:51+00])  |\n",
      "|11    |STBOX XT(((-60.00591066159541,-89.99724849779159),(59.98796761356061,-30.00028948827336)),[2020-06-08 07:00:51+00, 2020-06-11 19:30:26+00])  |\n",
      "|12    |STBOX XT(((-60.00591066159541,-30.00028948827336),(59.98796761356061,29.99666952124487)),[2020-06-01 06:01:41+00, 2020-06-04 18:31:16+00])   |\n",
      "|13    |STBOX XT(((-60.00591066159541,-30.00028948827336),(59.98796761356061,29.99666952124487)),[2020-06-04 18:31:16+00, 2020-06-08 07:00:51+00])   |\n",
      "|14    |STBOX XT(((-60.00591066159541,-30.00028948827336),(59.98796761356061,29.99666952124487)),[2020-06-08 07:00:51+00, 2020-06-11 19:30:26+00])   |\n",
      "|15    |STBOX XT(((-60.00591066159541,29.99666952124487),(59.98796761356061,89.99362853076309)),[2020-06-01 06:01:41+00, 2020-06-04 18:31:16+00])    |\n",
      "|16    |STBOX XT(((-60.00591066159541,29.99666952124487),(59.98796761356061,89.99362853076309)),[2020-06-04 18:31:16+00, 2020-06-08 07:00:51+00])    |\n",
      "|17    |STBOX XT(((-60.00591066159541,29.99666952124487),(59.98796761356061,89.99362853076309)),[2020-06-08 07:00:51+00, 2020-06-11 19:30:26+00])    |\n",
      "|18    |STBOX XT(((59.98796761356061,-89.99724849779159),(179.98184588871663,-30.00028948827336)),[2020-06-01 06:01:41+00, 2020-06-04 18:31:16+00])  |\n",
      "|19    |STBOX XT(((59.98796761356061,-89.99724849779159),(179.98184588871663,-30.00028948827336)),[2020-06-04 18:31:16+00, 2020-06-08 07:00:51+00])  |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM grid\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5995c3c4-daa9-4c5e-add1-9111b1f89789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+\n",
      "|instantid|instant                   |\n",
      "+---------+--------------------------+\n",
      "|1        |2020-06-01 19:44:49.709717|\n",
      "|2        |2020-06-05 23:09:46.756569|\n",
      "|3        |2020-06-02 07:23:47.01265 |\n",
      "|4        |2020-06-03 17:45:26.880351|\n",
      "|5        |2020-06-06 21:50:37.821979|\n",
      "|6        |2020-06-05 04:20:39.881892|\n",
      "|7        |2020-06-03 21:27:35.094863|\n",
      "|8        |2020-06-06 23:55:02.695356|\n",
      "|9        |2020-06-10 01:19:31.530931|\n",
      "|10       |2020-06-02 05:29:31.40646 |\n",
      "|11       |2020-05-31 23:17:56.475136|\n",
      "|12       |2020-06-10 14:09:38.90966 |\n",
      "|13       |2020-06-07 04:35:05.556855|\n",
      "|14       |2020-06-08 08:21:40.758555|\n",
      "|15       |2020-06-06 10:06:55.084251|\n",
      "|16       |2020-06-01 08:40:40.189497|\n",
      "|17       |2020-06-03 19:43:24.094119|\n",
      "|18       |2020-06-07 14:21:51.717996|\n",
      "|19       |2020-06-01 01:31:49.319809|\n",
      "|20       |2020-06-10 00:09:16.655234|\n",
      "+---------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM instants\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e89231-362e-4327-b91f-22fb65d49157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
