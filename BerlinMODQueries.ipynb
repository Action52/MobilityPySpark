{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fe117f-48aa-4ab6-987c-49039d119911",
   "metadata": {},
   "source": [
    "# BerlinMOD Queries\n",
    "\n",
    "So far we have replicated the BerlinMOD Pymeos tutorial using Pyspark. Now we will execute the BerlinMOD queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22dd950a-e0c5-4d0c-b9a2-b53527a91ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mobilitydb-berlinmod-sf0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd \"../mobilitydb-berlinmod-sf0.1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1076035-c1e1-44e3-a9dd-d2c8fffac29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32minstants.csv\u001b[0m*  \u001b[01;32mpoints.csv\u001b[0m*       \u001b[01;32mtrips.csv\u001b[0m*       \u001b[01;32mvehicles.csv\u001b[0m*\n",
      "\u001b[01;32mlicences.csv\u001b[0m*  \u001b[01;32mregions.csv\u001b[0m*      trips_small.csv  vehicles_small.csv\n",
      "\u001b[01;32mperiods.csv\u001b[0m*   \u001b[01;34mspark-warehouse\u001b[0m/  vehicle_ids.txt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fc1f9b-a67a-45df-8d9a-d1d82a581c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86057f-63c8-4fb4-9561-f20db763733d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468925bf-ca78-4cf0-aa80-bbb83894b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymeos import *\n",
    "from pymeos.plotters import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pysparkmeos.UDT.MeosDatatype import *\n",
    "from pysparkmeos.partitions.grid.grid_partitioner import GridPartition\n",
    "from pysparkmeos.utils.udt_appender import udt_append\n",
    "from pysparkmeos.utils.utils import *\n",
    "\n",
    "\n",
    "from pysparkmeos.UDF.udf import *\n",
    "from pysparkmeos.partitions.mobilityrdd import MobilityRDD\n",
    "\n",
    "\n",
    "import random, datetime, os, sys\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone\n",
    "import contextily as cx\n",
    "import distinctipy\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import shapely.geometry as shp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely import wkb, box, from_wkb\n",
    "from typing import Union\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ada590-2fc7-4b1b-bad3-df9609ccf51e",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4df066b-5a3f-483e-a5e8-7546c7d5068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/26 12:14:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/26 12:14:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.default.parallelism: 3\n"
     ]
    }
   ],
   "source": [
    "def startspark():\n",
    "    # Initialize PyMEOS\n",
    "    pymeos_initialize(\"UTC\")\n",
    "    \n",
    "    os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    \n",
    "    #.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    # Initialize a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BerlinMOD with PySpark\") \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .config(\"spark.default.parallelism\", 3) \\\n",
    "        .config(\"spark.executor.memory\", \"3g\") \\\n",
    "        .config(\"spark.executor.cores\", 1) \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", 0) \\\n",
    "        .config(\"spark.sql.allowMultipleTableArguments.enabled\", True) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # spark.sparkContext.setLogLevel(\"INFO\")\n",
    "    \n",
    "    # Append the UDT mapping to the PyMEOS classes\n",
    "    udt_append()\n",
    "    \n",
    "    # Get the value of 'spark.default.parallelism'\n",
    "    default_parallelism = spark.sparkContext.getConf().get(\"spark.default.parallelism\")\n",
    "    print(f\"spark.default.parallelism: {default_parallelism}\")\n",
    "\n",
    "    # Register udfs in Spark SQL\n",
    "    register_udfs_under_spark_sql(spark)\n",
    "    \n",
    "    return spark\n",
    "\n",
    "spark = startspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132474c-76cd-47b5-ab64-3cd314014e6e",
   "metadata": {},
   "source": [
    "## Load Tables\n",
    "We will use the power of Spark SQL to read in the raw dataframes and then create the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717e9053-a9b1-4760-b3da-400d05c0639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32minstants.csv\u001b[0m*  \u001b[01;32mperiods.csv\u001b[0m*  \u001b[01;32mregions.csv\u001b[0m*  trips_small.csv  \u001b[01;32mvehicles.csv\u001b[0m*\n",
      "\u001b[01;32mlicences.csv\u001b[0m*  \u001b[01;32mpoints.csv\u001b[0m*   \u001b[01;32mtrips.csv\u001b[0m*    vehicle_ids.txt  vehicles_small.csv\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "070db614-efc2-41eb-af34-397671ab8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(\n",
    "    spark, \n",
    "    path, \n",
    "    tablename, \n",
    "    partition_key=None, \n",
    "    transformation_query=None,\n",
    "    partition_query = None,\n",
    "    partitioner_class = None,\n",
    "    partitioner_args = {},\n",
    "    **kwargs\n",
    "):\n",
    "    print(\"Reading raw csv \", path)\n",
    "    rawdf = spark.read.csv(path, **kwargs)\n",
    "\n",
    "    print(\"Creating temp view of raw table\")\n",
    "    rawdf.createOrReplaceTempView(f\"{tablename}RawNoCache\")\n",
    "\n",
    "    print(\"Schema and statistics of raw table\")\n",
    "    rawdf.printSchema()\n",
    "    rawdf.describe().show()\n",
    "    print(f\"Creating final table {tablename} based on {tablename}Raw, partitioned by {partition_key}.\")\n",
    "    spark.sql(f\"\"\"DROP TABLE IF EXISTS {tablename}\"\"\")\n",
    "\n",
    "    if transformation_query:\n",
    "        rawdf = spark.sql(transformation_query)\n",
    "        rawdf.createOrReplaceTempView(f\"{tablename}RawNoCache\")\n",
    "        spark.sql(f\"CACHE TABLE {tablename}Raw SELECT * FROM {tablename}RawNoCache\")\n",
    "        spark.sql(f\"SELECT * FROM {tablename}Raw LIMIT 5\").show()\n",
    "        #spark.catalog.dropTempView(f\"{tablename}RawNoCache\")\n",
    "    else:\n",
    "        spark.sql(f\"CACHE TABLE {tablename}Raw SELECT * FROM {tablename}RawNoCache\")\n",
    "        #spark.catalog.dropTempView(f\"{tablename}RawNoCache\")\n",
    "    partitioner = None\n",
    "    if partition_query:\n",
    "        bounds = rawdf.rdd.mapPartitions(bounds_calculate_map).reduce(bounds_calculate_reduce)\n",
    "        print(\"Bounds: \", bounds)\n",
    "        partitioner = partitioner_class(bounds=bounds, **partitioner_args)\n",
    "        grid = partitioner.as_spark_table()\n",
    "        grid.cache()\n",
    "        grid.show()\n",
    "        grid.createOrReplaceTempView(\"grid\")\n",
    "        print(\"Creating partitioned table... \")\n",
    "        partitionedTable = spark.sql(partition_query)\n",
    "        partitionedTable.createOrReplaceTempView(f\"{tablename}Raw\")\n",
    "\n",
    "\n",
    "    start = time()\n",
    "    \n",
    "    if partition_key:\n",
    "        df = spark.sql(f\"\"\"\n",
    "        CREATE TABLE {tablename}NoCache\n",
    "        USING parquet\n",
    "        PARTITIONED BY ({partition_key})\n",
    "        AS SELECT * FROM {tablename}Raw\n",
    "        \"\"\")\n",
    "    else:\n",
    "        df = spark.sql(f\"\"\"\n",
    "        CREATE TABLE {tablename}NoCache\n",
    "        USING parquet\n",
    "        AS SELECT * FROM {tablename}Raw\n",
    "        \"\"\")\n",
    "        \n",
    "    end = time()\n",
    "    \n",
    "    if partition_key:\n",
    "        print(f\"{tablename} partitions:\")\n",
    "        spark.sql(f\"\"\"\n",
    "        SHOW PARTITIONS {tablename}NoCache\n",
    "        \"\"\").show()\n",
    "    print(f\"Final table created in {end-start} seconds\")\n",
    "\n",
    "    spark.sql(f\"CACHE TABLE {tablename} SELECT * FROM {tablename}NoCache\")\n",
    "\n",
    "    df = spark.table(f\"{tablename}\")\n",
    "    \n",
    "    print(f\"Final table {tablename} schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    #Drop the temporary view\n",
    "    #spark.catalog.dropTempView(f\"{tablename}Raw\")\n",
    "    return df, (start, end, end-start)\n",
    "\n",
    "\n",
    "def load_all_tables(configs):\n",
    "    tables = {}\n",
    "    stats = {}\n",
    "    for tablename, config in configs.items():\n",
    "        table, stat = load_table(**config)\n",
    "        tables[tablename] = table\n",
    "        stats[tablename] = stat\n",
    "    return tables, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd062d6-e9cb-45df-9e9b-be54620d1902",
   "metadata": {},
   "source": [
    "### Instants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbdb2a17-8371-48b2-90f0-9a24ce0d3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instants, statsinstants = load_table(spark, \"instants.csv\", 'instants', inferSchema=True, header=True)\n",
    "#instants.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb35ef8-6bec-4722-9a83-e8de93483991",
   "metadata": {},
   "source": [
    "### Licences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09afb698-b7a8-4e44-aa12-21251ae18523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#licences, statslicences = load_table(spark, \"licences.csv\", 'licences', inferSchema=True, header=True)\n",
    "#licences.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e82da0-5cfb-4596-95c9-f79e6c2bfa0b",
   "metadata": {},
   "source": [
    "### Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b12ebc3-4179-4f57-978a-fb3dbed526f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transperiod = \"\"\"\n",
    "SELECT periodid, beginp, endp, tstzspan(period) AS period FROM periodsRawNoCache\n",
    "\"\"\"\n",
    "#periods, statsperiods = load_table(spark, \"periods.csv\", 'periods', transformation_query=transperiod, inferSchema=True, header=True)\n",
    "#periods.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da518bb7-2aed-4329-8d31-2f6cb0dc9f00",
   "metadata": {},
   "source": [
    "### Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060e3f55-4db1-418b-aa90-0238c8034332",
   "metadata": {},
   "outputs": [],
   "source": [
    "transpoints = \"\"\"\n",
    "SELECT pointid, posx, posy, geometry_from_hexwkb(geom) AS geom FROM pointsRawNoCache\n",
    "\"\"\"\n",
    "#points, statspoints = load_table(spark, \"points.csv\", 'points', transformation_query=transpoints, inferSchema=True, header=True)\n",
    "#points.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f2df3-511c-4e55-8ea7-fb09f208e46f",
   "metadata": {},
   "source": [
    "### Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40a4a4fb-7bd1-41d7-9576-1388fcaf6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "transregions = \"SELECT regionid, geometry_from_hexwkb(geom) AS geom FROM regionsRawNoCache\"\n",
    "#regions, statsregions = load_table(spark, \"regions.csv\", 'regions', transformation_query=transregions, inferSchema=True, header=True)\n",
    "#regions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524b97d-8e8d-430e-b287-64c765fdfd90",
   "metadata": {},
   "source": [
    "### Trips\n",
    "Note: Use trips_small for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ab82ea9-9dd0-4363-b413-aa7e385c3de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.udtf.UserDefinedTableFunction at 0x7fffbcaf0220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from typing import Iterator\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"tripid\", IntegerType()),\n",
    "    StructField(\"vehid\", IntegerType()),\n",
    "    StructField(\"day\", IntegerType()),\n",
    "    StructField(\"seqno\", IntegerType()),\n",
    "    StructField(\"sourcenode\", IntegerType()),\n",
    "    StructField(\"targetnode\", StringType()),\n",
    "    StructField(\"trip\", TGeomPointSeqSetUDT()),\n",
    "    StructField(\"trajectory\", GeometryUDT()),\n",
    "    StructField(\"license\", StringType()),\n",
    "    StructField(\"partitionKey\", IntegerType())\n",
    "])\n",
    "\n",
    "@F.udtf(returnType=schema)\n",
    "class PartitionUDTF:\n",
    "    def eval(self, row: Row):\n",
    "        pymeos_initialize()\n",
    "        sequence_id = row.tripid\n",
    "        trajectory = row.trip\n",
    "        #print(trajectory,type(trajectory))\n",
    "        #print(row.grid[0])\n",
    "        #grid = [STBoxWrap(f\"SRID=4326;{tile.__str__().strip('SRID=4326;')}\") for tile in row.grid]\n",
    "        grid = row.grid\n",
    "        gridids = row.gridids\n",
    "        partitioned = [(key, trajectory.at(tile)) for key, tile in zip(gridids, grid)]\n",
    "        #print(trajectory)\n",
    "        #print(grid)\n",
    "        #print(partitioned)\n",
    "        count = 0\n",
    "        responses = []\n",
    "        for partition_key, partition_traj in partitioned:\n",
    "            count += 1\n",
    "            if partition_traj is None:\n",
    "                continue\n",
    "            else:\n",
    "                response = (sequence_id, row.vehid, row.day, row.seqno, row.sourcenode, row.targetnode, partition_traj, row.trajectory, row.licence, partition_key)\n",
    "                yield response\n",
    "                #seqs = partition_traj.segments()\n",
    "                #print(seqs)\n",
    "                #for partition_traj_seq in seqs:\n",
    "                #    response = (sequence_id, row.vehid, row.day, row.seqno, row.sourcenode, row.targetnode, partition_traj_seq, row.trajectory, row.licence, partition_key)\n",
    "                    #responses.append(response)\n",
    "                    #yield response\n",
    "        #for response in responses:\n",
    "        #    yield response\n",
    "\n",
    "\n",
    "spark.udtf.register(\"PartitionUDTF\", PartitionUDTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d90990-4b01-4939-a56f-7d39e6d0d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add the transformation query to the trips table and inject it to the config.\n",
    "parttrips = \"\"\"\n",
    "    SELECT * \n",
    "    FROM PartitionUDTF(\n",
    "        TABLE(\n",
    "                SELECT \n",
    "                    *, \n",
    "                    (SELECT collect_list(tile) FROM grid) AS grid, \n",
    "                    (SELECT collect_list(tileid) FROM grid) AS gridids\n",
    "                FROM tripsRaw\n",
    "        )\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321f6985-9bc1-44ad-b775-944777d89d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrips, statstrips = load_table(\\n    spark, \"trips_small.csv\", \\'trips\\', \\n    transformation_query=transtrips,\\n    partition_key= \\'partitionKey\\',\\n    partition_query=parttrips,\\n    partitioner_class=GridPartition,\\n    partitioner_args = {\\'cells_per_side\\': 3},\\n    inferSchema=True,\\n    header=True\\n)\\ntrips.show()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transtrips = \"SELECT tripid, vehid, day, seqno, sourcenode, targetnode, trip_from_hexwkb(trip) AS trip, geometry_from_hexwkb(trajectory) AS trajectory, licence FROM tripsRawNoCache\"\n",
    "\n",
    "\"\"\"\n",
    "trips, statstrips = load_table(\n",
    "    spark, \"trips_small.csv\", 'trips', \n",
    "    transformation_query=transtrips,\n",
    "    partition_key= 'partitionKey',\n",
    "    partition_query=parttrips,\n",
    "    partitioner_class=GridPartition,\n",
    "    partitioner_args = {'cells_per_side': 3},\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")\n",
    "trips.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8535d1-4cb6-4e79-92d4-f8cc31a5a6b2",
   "metadata": {},
   "source": [
    "### Vehicles\n",
    "Note: Also read vehicles_small for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a37c3a23-a245-4e74-9e62-558c1bd833fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vehicles, statsvehicles = load_table(spark, \"vehicles_small.csv\", 'vehicles', inferSchema=True, header=True)\n",
    "#vehicles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2ff80-cd8c-47f2-b539-a8dc53f17388",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "First queries take a general approach and are only used to measure overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "432c9616-31ae-4bd5-91de-cc1bca394c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def query_exec(query, spark, execute=True, explain=False, explainmode=''):\n",
    "    plan = None\n",
    "    if explain:\n",
    "        plan = spark.sql(f\"EXPLAIN {explainmode} {query}\").collect()[0].plan\n",
    "    result = spark.sql(query)\n",
    "    start = time()\n",
    "    if execute:\n",
    "        result.show()\n",
    "    end = time()\n",
    "    print(\"Query execution time: \", end-start, \" seconds.\")\n",
    "    return result, (start, end, end-start), plan\n",
    "\n",
    "\n",
    "def retrieve_exec_stats(queries, starts, ends, durations, plans):\n",
    "    return pd.DataFrame({\"queries\": queries, \"start\": starts, \"end\": ends, \"duration\": durations, \"plan\": plans})\n",
    "\n",
    "\n",
    "def run_all_queries(queries, spark, execute=True, explain=True, explainmode='', printplan=False):\n",
    "    \"\"\" Utility function to run all queries through subsequent experiments \"\"\"\n",
    "    qdfs = []\n",
    "    starts = []\n",
    "    ends = []\n",
    "    durations = []\n",
    "    plans = []\n",
    "    for querytext in queries:\n",
    "        qdf, qstats, plan = query_exec(querytext, spark, execute, explain, explainmode)\n",
    "        qdfs.append(qdf)\n",
    "        starts.append(qstats[0])\n",
    "        ends.append(qstats[1])\n",
    "        durations.append(qstats[2])\n",
    "        plans.append(plan)\n",
    "        if printplan:\n",
    "            print(plan)\n",
    "    exec_stats = retrieve_exec_stats(queries, starts, ends, durations, plans)\n",
    "    return qdfs, exec_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb526c-13c6-48ee-9b9b-6b906cc39558",
   "metadata": {},
   "source": [
    "### Query 1: What are the models of the vehicles with licence plate numbers from QueryLicences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "949ce841-db85-4913-8705-b1636a14ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytext1 = \"\"\"\n",
    "    SELECT l.licence, v.model\n",
    "    FROM licences l, vehicles v\n",
    "    WHERE l.licence = v.licence\n",
    "\"\"\"\n",
    "#q1, q1stats, plan1 = query_exec(querytext1, spark, explain=True)\n",
    "#if plan1:\n",
    "#    print(plan1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb68fd4-24d2-4146-9092-9d35f5a33142",
   "metadata": {},
   "source": [
    "### Query 2: How many vehicles exist that are 'passenger' cars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aeaa634-3b72-416c-93a5-45ee013df74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytext2 = \"\"\"\n",
    "    SELECT COUNT(licence) AS PassengerCarCount\n",
    "    FROM vehicles\n",
    "    WHERE type='passenger'\n",
    "\"\"\"\n",
    "#q2, q2stats, plan2 = query_exec(querytext2, spark, explain=True)\n",
    "#if plan2:\n",
    "#    print(plan2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17ee5d-5ab1-4018-8db6-1c2a7e61fb9c",
   "metadata": {},
   "source": [
    "### Query 3: Where have the vehicles with licences from QueryLicences1 been at each of the instants from QueryInstants1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0156d33-b304-4e1a-a917-331e6e0e3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "querytext3 = \"\"\"\n",
    "    WITH\n",
    "    veh_w_lic AS (\n",
    "        SELECT v.vehid, l.licence, v.model\n",
    "        FROM licences l, vehicles v\n",
    "        WHERE l.licence = v.licence\n",
    "    ),\n",
    "    veh_trips AS (\n",
    "        SELECT t.* \n",
    "        FROM veh_w_lic vw, trips t\n",
    "        WHERE t.vehid = vw.vehid\n",
    "    ),\n",
    "    tile_instants AS (\n",
    "        SELECT /*+ BROADCAST(gr) */ gr.tile, i.instant\n",
    "        FROM grid gr, instants i\n",
    "        WHERE contains_stbox_stbox(gr.tile, i.instant) = TRUE\n",
    "    )\n",
    "    SELECT /*+ BROADCAST(i) */ vt.vehid, vt.tripid, vt.trip, i.instant, tpoint_at(vt.trip, i.instant) AS pos\n",
    "    FROM veh_trips vt, tile_instants i\n",
    "\"\"\"\n",
    "#q3, q3stats, plan3 = query_exec(querytext3, spark, explain=True)\n",
    "#if plan3:\n",
    "#    print(plan3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f096b-ddd3-4130-a07d-844f39e6a0be",
   "metadata": {},
   "source": [
    "### Query 4: Which licence plate numbers belong to vehicles that have passed the points from QueryPoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dabe818-ac5b-4e65-96ff-55e251791161",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytext4 = \"\"\"\n",
    "    WITH \n",
    "    vehids_intersect AS (\n",
    "        SELECT t.vehid\n",
    "        FROM trips t, points p\n",
    "        WHERE ever_touches(t.trip, p.geom) = TRUE\n",
    "    )\n",
    "    SELECT vi.vehid, v.licence\n",
    "    FROM vehids_intersect vi, vehicles v\n",
    "\"\"\"\n",
    "#q4, q4stats, plan4 = query_exec(querytext4, spark, explain=True)\n",
    "#if plan4:\n",
    "#    print(plan4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0302e785-2667-4d07-ad31-b046a0f7aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 12:14:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|temp_clm|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df = spark.createDataFrame([\"0\"], \"string\").toDF(\"temp_clm\")\n",
    "dummy_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c9775-97ba-4e8e-b65b-15e35be83845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a4cff-57fb-4d63-8795-5f3372854252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebba818e-647c-4d2a-a4bd-4aa6abca9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [querytext1, querytext2] #, querytext3] #, querytext4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdffb7-9ce6-4891-8d02-bb87ef1c8df6",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b4f48-ab51-4aca-a460-d110fe1872f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment 1: Run Queries ASIS\n",
    "First we are going to run the queries without any improvement or partitioning strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3d0a7a8-085d-4c9c-b7fc-1013e163693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exp_1 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e7317-8ef6-4a00-9500-34c4fd56cb84",
   "metadata": {},
   "source": [
    "#### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f5451e8-2349-444f-ba0f-9a79710c3fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/rm: cannot remove 'spark-warehouse/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d732540-70d2-4da7-a458-d660cc7486ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp1 = {\n",
    "    'instants': {'spark': spark, 'path': 'instants.csv', 'tablename': 'instants', 'inferSchema': True, 'header': True},\n",
    "    'licences': {'spark': spark, 'path': 'licences.csv', 'tablename': 'licences', 'inferSchema': True, 'header': True},\n",
    "    'periods':  {'spark': spark, 'path': 'periods.csv', 'tablename': 'periods', 'transformation_query': transperiod, 'inferSchema': True, 'header': True},\n",
    "    'points':   {'spark': spark, 'path': 'points.csv', 'tablename': 'points', 'transformation_query': transpoints, 'inferSchema': True, 'header': True},\n",
    "    'regions':  {'spark': spark, 'path': 'regions.csv', 'tablename': 'regions', 'transformation_query':transregions, 'inferSchema': True, 'header': True},\n",
    "    'trips':    {'spark': spark, 'path': 'trips_small.csv', 'tablename': 'trips', 'transformation_query':transtrips, 'inferSchema': True, 'header': True},\n",
    "    'vehicles': {'spark': spark, 'path': 'vehicles_small.csv', 'tablename': 'vehicles', 'inferSchema': True, 'header': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2daf0c1-03dd-4813-9afd-62e3ab4b3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_1:\n",
    "    tables, stats = load_all_tables(configs_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd607345-7614-4fcf-94c6-3d273236a70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|num|squared|\n",
      "+---+-------+\n",
      "|  1|      1|\n",
      "|  2|      4|\n",
      "|  3|      9|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@F.udtf(returnType=\"num: int, squared: int\")\n",
    "class SquareNumbers:\n",
    "    def eval(self, start: int, end: int):\n",
    "        for num in range(start, end + 1):\n",
    "            yield (num, num * num)\n",
    "\n",
    "SquareNumbers(F.lit(1), F.lit(3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "860d705f-3abd-4290-9edd-cd4520d1d837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.udtf.UserDefinedTableFunction at 0x7fffdb7ef250>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " |-- tripid: integer (nullable = true)\n",
    " |-- vehid: integer (nullable = true)\n",
    " |-- day: date (nullable = true)\n",
    " |-- seqno: integer (nullable = true)\n",
    " |-- sourcenode: integer (nullable = true)\n",
    " |-- targetnode: integer (nullable = true)\n",
    " |-- trip: pythonuserdefined (nullable = true)\n",
    " |-- trajectory: pythonuserdefined (nullable = true)\n",
    "\"\"\"\n",
    "schema = StructType([\n",
    "    StructField(\"point\", TGeomPointInstUDT())\n",
    "])\n",
    "@F.udtf(returnType=schema)\n",
    "class ExplodeGeomSeq:\n",
    "    def eval(self, trip: TGeomPointSeqWrap):\n",
    "        #print(trip['trip'])\n",
    "        #trip = trip.trip\n",
    "        pymeos_initialize()\n",
    "        instants = trip['trip'].instants()\n",
    "        for i in instants:\n",
    "            yield i,\n",
    "\n",
    "spark.udtf.register(\"explodeGeomSeq\", ExplodeGeomSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68e5f9b1-9f49-472a-94a4-ba7ffba68132",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_1:\n",
    "    spark.sql(\"SELECT * FROM explodeGeomSeq(TABLE(SELECT trip FROM trips))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf04af-6edb-4119-aa86-133a3afd1d5b",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ba12c8d-0d99-447a-b278-3b7276a3c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_1:\n",
    "    qdfs_exp1, stats_exp1 = run_all_queries(queries, spark, explain=True, printplan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a70d15ac-93c5-4e9e-9418-2b8424c70ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_1:\n",
    "    for (_id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "        rdd.unpersist()\n",
    "        print(\"Unpersisted {} rdd\".format(_id))\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b770606-3841-401a-b721-eafabd3e7cbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment 2: Partition Trips by vehid, HashPartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1be2ccf-4cca-42c3-8158-e178904c0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exp_2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba00958e-6711-4cc8-9924-2d5f3f829dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/rm: cannot remove 'rm': No such file or directory\n",
      "/bin/rm: cannot remove 'spark-warehouse/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm -R rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d4ddff5-99bf-4e67-9612-f4b3bc3d30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_2:\n",
    "    spark = startspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10871d-410c-48f3-98a6-53700dc7450c",
   "metadata": {},
   "source": [
    "#### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "985b9d3b-84d7-4934-adf0-83046a8bf7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp2 = {\n",
    "    'instants': {'spark': spark, 'path': 'instants.csv', 'tablename': 'instants', 'inferSchema': True, 'header': True},\n",
    "    'licences': {'spark': spark, 'path': 'licences.csv', 'tablename': 'licences', 'inferSchema': True, 'header': True},\n",
    "    'periods':  {'spark': spark, 'path': 'periods.csv', 'tablename': 'periods', 'transformation_query': transperiod, 'inferSchema': True, 'header': True},\n",
    "    'points':   {'spark': spark, 'path': 'points.csv', 'tablename': 'points', 'transformation_query': transpoints, 'inferSchema': True, 'header': True},\n",
    "    'regions':  {'spark': spark, 'path': 'regions.csv', 'tablename': 'regions', 'transformation_query':transregions, 'inferSchema': True, 'header': True},\n",
    "    'trips':    {'spark': spark, 'path': 'trips_small.csv', 'tablename': 'trips', 'partition_key': 'vehid', 'transformation_query':transtrips, 'inferSchema': True, 'header': True},\n",
    "    'vehicles': {'spark': spark, 'path': 'vehicles_small.csv', 'tablename': 'vehicles', 'inferSchema': True, 'header': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd33623a-819a-4f8b-83fe-a5348df6b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_2:\n",
    "    tables, stats = load_all_tables(configs_exp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed2844-15fd-4b75-ab07-b70995d3ab5c",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "514e9f39-27de-4e99-86c4-0d002f859c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_exp_2:\n",
    "    qdfs_exp2, stats_exp2 = run_all_queries(queries, spark, explain=True, printplan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e154ab01-e8f4-4992-b6a3-147a421ede36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d13d9a-4412-4518-bfc2-2014b610a08e",
   "metadata": {},
   "source": [
    "### Experiment 3: Partition by Trip, RegularGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a08c76bd-bb70-4e69-8eb0-0dff674eece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exp_3 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfd4c6b5-2428-4fae-98ca-a9678045b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/rm: cannot remove 'rm': No such file or directory\n",
      "/bin/rm: cannot remove 'spark-warehouse/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm -R rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a908c86-6cf2-4ec8-87f3-2c0ad8989163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 12:15:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.default.parallelism: 3\n"
     ]
    }
   ],
   "source": [
    "if run_exp_3:\n",
    "    spark = startspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bfdded-cc45-4b53-84fc-6e8077659f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad60f1d8-e0c5-483e-9b8e-fd3e1e414bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf26e353-b645-45ca-b994-c556010549e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp3 = {\n",
    "    'instants': {'spark': spark, 'path': 'instants.csv', 'tablename': 'instants', 'inferSchema': True, 'header': True},\n",
    "    'licences': {'spark': spark, 'path': 'licences.csv', 'tablename': 'licences', 'inferSchema': True, 'header': True},\n",
    "    'periods':  {'spark': spark, 'path': 'periods.csv', 'tablename': 'periods', 'transformation_query': transperiod, 'inferSchema': True, 'header': True},\n",
    "    'points':   {'spark': spark, 'path': 'points.csv', 'tablename': 'points', 'transformation_query': transpoints, 'inferSchema': True, 'header': True},\n",
    "    'regions':  {'spark': spark, 'path': 'regions.csv', 'tablename': 'regions', 'transformation_query':transregions, 'inferSchema': True, 'header': True},\n",
    "    'trips':    {\n",
    "        'spark': spark, \n",
    "        'path': 'trips_small.csv', \n",
    "        'tablename': 'trips', \n",
    "        'partition_key': 'partitionKey', \n",
    "        'transformation_query':transtrips,\n",
    "        'partition_query': parttrips,\n",
    "        'partitioner_class': GridPartition,\n",
    "        'partitioner_args': {'cells_per_side': 3},\n",
    "        'inferSchema': True, \n",
    "        'header': True},\n",
    "    'vehicles': {'spark': spark, 'path': 'vehicles_small.csv', 'tablename': 'vehicles', 'inferSchema': True, 'header': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c314039-c87a-43fb-911f-375fa35e4dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw csv  instants.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- instantid: integer (nullable = true)\n",
      " |-- instant: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         instantid|\n",
      "+-------+------------------+\n",
      "|  count|               100|\n",
      "|   mean|              50.5|\n",
      "| stddev|29.011491975882016|\n",
      "|    min|                 1|\n",
      "|    max|               100|\n",
      "+-------+------------------+\n",
      "\n",
      "Creating final table instants based on instantsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 3.3694961071014404 seconds\n",
      "Final table instants schema:\n",
      "root\n",
      " |-- instantid: integer (nullable = true)\n",
      " |-- instant: timestamp (nullable = true)\n",
      "\n",
      "Reading raw csv  licences.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- licenceid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------+------------------+\n",
      "|summary|         licenceid| licence|             vehid|\n",
      "+-------+------------------+--------+------------------+\n",
      "|  count|               101|     101|               101|\n",
      "|   mean|              51.0|    NULL|319.46534653465346|\n",
      "| stddev|29.300170647967224|    NULL| 175.0106604956644|\n",
      "|    min|                 1|B-BJ 115|                 9|\n",
      "|    max|               101|B-[U 177|               622|\n",
      "+-------+------------------+--------+------------------+\n",
      "\n",
      "Creating final table licences based on licencesRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 1.516155481338501 seconds\n",
      "Final table licences schema:\n",
      "root\n",
      " |-- licenceid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      "\n",
      "Reading raw csv  periods.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- periodid: integer (nullable = true)\n",
      " |-- beginp: timestamp (nullable = true)\n",
      " |-- endp: timestamp (nullable = true)\n",
      " |-- period: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+\n",
      "|summary|          periodid|              period|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|               100|                 100|\n",
      "|   mean|              50.5|                NULL|\n",
      "| stddev|29.011491975882016|                NULL|\n",
      "|    min|                 1|[2020-06-01 00:45...|\n",
      "|    max|               100|[2020-06-11 21:18...|\n",
      "+-------+------------------+--------------------+\n",
      "\n",
      "Creating final table periods based on periodsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "|periodid|              beginp|                endp|              period|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|       1|2020-06-09 11:15:...|2020-06-09 20:38:...|[2020-06-09 11:15...|\n",
      "|       2|2020-06-10 10:55:...|2020-06-11 01:01:...|[2020-06-10 10:55...|\n",
      "|       3|2020-06-04 06:42:...|2020-06-05 02:50:...|[2020-06-04 06:42...|\n",
      "|       4|2020-06-05 04:39:...|2020-06-06 05:48:...|[2020-06-05 04:39...|\n",
      "|       5|2020-06-06 09:10:...|2020-06-07 03:59:...|[2020-06-06 09:10...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 1.6822783946990967 seconds\n",
      "Final table periods schema:\n",
      "root\n",
      " |-- periodid: integer (nullable = true)\n",
      " |-- beginp: timestamp (nullable = true)\n",
      " |-- endp: timestamp (nullable = true)\n",
      " |-- period: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  points.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- pointid: integer (nullable = true)\n",
      " |-- posx: double (nullable = true)\n",
      " |-- posy: double (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 12:15:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "|summary|           pointid|             posx|             posy|                geom|\n",
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "|  count|               100|              100|              100|                 100|\n",
      "|   mean|              50.5|486384.3413598945|6594038.933758076|                NULL|\n",
      "| stddev|29.011491975882016|7200.526060474747|6552.156274876073|                NULL|\n",
      "|    min|                 1|472428.0634008836|6577421.541139536|0101000020110F000...|\n",
      "|    max|               100| 498913.875699313|6607119.513588189|0101000020110F000...|\n",
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "\n",
      "Creating final table points based on pointsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+--------------------+\n",
      "|pointid|              posx|             posy|                geom|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "|      1| 476191.0852037612|6589454.831155596|POINT (476191.085...|\n",
      "|      2| 485998.9668637461|6580934.403927697|POINT (485998.966...|\n",
      "|      3|486927.13764603145|  6584864.3484669|POINT (486927.137...|\n",
      "|      4|491514.42461848777|6594412.284642856|POINT (491514.424...|\n",
      "|      5| 493018.1394320724|6602300.271879816|POINT (493018.139...|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 2.600412607192993 seconds\n",
      "Final table points schema:\n",
      "root\n",
      " |-- pointid: integer (nullable = true)\n",
      " |-- posx: double (nullable = true)\n",
      " |-- posy: double (nullable = true)\n",
      " |-- geom: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  regions.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- regionid: integer (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      "\n",
      "+-------+------------------+--------------------+\n",
      "|summary|          regionid|                geom|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|               100|                 100|\n",
      "|   mean|              50.5|                NULL|\n",
      "| stddev|29.011491975882016|                NULL|\n",
      "|    min|                 1|0103000020110F000...|\n",
      "|    max|               100|0103000020110F000...|\n",
      "+-------+------------------+--------------------+\n",
      "\n",
      "Creating final table regions based on regionsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|regionid|                geom|\n",
      "+--------+--------------------+\n",
      "|       1|POLYGON ((483571....|\n",
      "|       2|POLYGON ((485438....|\n",
      "|       3|POLYGON ((486542....|\n",
      "|       4|POLYGON ((488077....|\n",
      "|       5|POLYGON ((482151....|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 1.785264015197754 seconds\n",
      "Final table regions schema:\n",
      "root\n",
      " |-- regionid: integer (nullable = true)\n",
      " |-- geom: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  trips_small.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- day: date (nullable = true)\n",
      " |-- seqno: integer (nullable = true)\n",
      " |-- sourcenode: integer (nullable = true)\n",
      " |-- targetnode: integer (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- trajectory: string (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+-----------------+--------------------+--------------------+-------+\n",
      "|summary|           tripid|             vehid|             seqno|        sourcenode|       targetnode|                trip|          trajectory|licence|\n",
      "+-------+-----------------+------------------+------------------+------------------+-----------------+--------------------+--------------------+-------+\n",
      "|  count|               91|                91|                91|                91|               91|                  91|                  91|      0|\n",
      "|   mean|304.3296703296703|10.956043956043956|2.5934065934065935| 39454.89010989011|39454.89010989011|                NULL|                NULL|   NULL|\n",
      "| stddev|204.9905176638967| 7.067786213065726| 1.666520140079134|28341.445037436115|28341.44503743612|                NULL|                NULL|   NULL|\n",
      "|    min|                1|                 1|                 1|              1160|             1160|[0101000020110F00...|0102000020110F000...|   NULL|\n",
      "|    max|              489|                17|                 9|             79113|            79113|[0101000020110F00...|0102000020110F000...|   NULL|\n",
      "+-------+-----------------+------------------+------------------+------------------+-----------------+--------------------+--------------------+-------+\n",
      "\n",
      "Creating final table trips based on tripsRaw, partitioned by partitionKey.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "|tripid|vehid|       day|seqno|sourcenode|targetnode|                trip|          trajectory|licence|\n",
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "|     1|    1|2020-06-01|    1|     79113|     66276|[POINT(496253.840...|LINESTRING (49625...|   NULL|\n",
      "|     2|    1|2020-06-01|    2|     66276|     79113|[POINT(481241.171...|LINESTRING (48124...|   NULL|\n",
      "|     3|    1|2020-06-02|    1|     79113|     66276|[POINT(496253.840...|LINESTRING (49625...|   NULL|\n",
      "|     4|    1|2020-06-02|    2|     66276|     79113|[POINT(481241.171...|LINESTRING (48124...|   NULL|\n",
      "|     5|    1|2020-06-03|    1|     79113|     66276|[POINT(496253.840...|LINESTRING (49625...|   NULL|\n",
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounds:  STBOX XT(((473277.05262936745,6579811.389156611),(498784.34433982597,6606871.682578203)),[2020-06-01 06:01:41.054+00, 2020-06-11 19:30:26.096307+00])\n",
      "+------+--------------------+\n",
      "|tileid|                tile|\n",
      "+------+--------------------+\n",
      "|     0|STBOX XT(((473277...|\n",
      "|     1|STBOX XT(((473277...|\n",
      "|     2|STBOX XT(((473277...|\n",
      "|     3|STBOX XT(((473277...|\n",
      "|     4|STBOX XT(((473277...|\n",
      "|     5|STBOX XT(((473277...|\n",
      "|     6|STBOX XT(((473277...|\n",
      "|     7|STBOX XT(((473277...|\n",
      "|     8|STBOX XT(((473277...|\n",
      "|     9|STBOX XT(((481779...|\n",
      "|    10|STBOX XT(((481779...|\n",
      "|    11|STBOX XT(((481779...|\n",
      "|    12|STBOX XT(((481779...|\n",
      "|    13|STBOX XT(((481779...|\n",
      "|    14|STBOX XT(((481779...|\n",
      "|    15|STBOX XT(((481779...|\n",
      "|    16|STBOX XT(((481779...|\n",
      "|    17|STBOX XT(((481779...|\n",
      "|    18|STBOX XT(((490281...|\n",
      "|    19|STBOX XT(((490281...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Creating partitioned table... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trips partitions:\n",
      "+---------------+\n",
      "|      partition|\n",
      "+---------------+\n",
      "| partitionKey=0|\n",
      "| partitionKey=1|\n",
      "|partitionKey=10|\n",
      "|partitionKey=12|\n",
      "|partitionKey=13|\n",
      "|partitionKey=14|\n",
      "|partitionKey=17|\n",
      "| partitionKey=2|\n",
      "|partitionKey=21|\n",
      "|partitionKey=22|\n",
      "|partitionKey=23|\n",
      "|partitionKey=24|\n",
      "|partitionKey=25|\n",
      "|partitionKey=26|\n",
      "| partitionKey=3|\n",
      "| partitionKey=4|\n",
      "| partitionKey=5|\n",
      "| partitionKey=6|\n",
      "| partitionKey=7|\n",
      "| partitionKey=8|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Final table created in 21.399942636489868 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table trips schema:\n",
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- seqno: integer (nullable = true)\n",
      " |-- sourcenode: integer (nullable = true)\n",
      " |-- targetnode: string (nullable = true)\n",
      " |-- trip: pythonuserdefined (nullable = true)\n",
      " |-- trajectory: pythonuserdefined (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- partitionKey: integer (nullable = true)\n",
      "\n",
      "Reading raw csv  vehicles_small.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------+-----+--------+\n",
      "|summary|            vehid|licence| type|   model|\n",
      "+-------+-----------------+-------+-----+--------+\n",
      "|  count|                3|      3|    3|       3|\n",
      "|   mean|             11.0|   NULL| NULL|    NULL|\n",
      "| stddev|8.717797887081348|   NULL| NULL|    NULL|\n",
      "|    min|                1|B-CJ 17|  bus|    Opel|\n",
      "|    max|               17|B-PZ 15|truck|Wartburg|\n",
      "+-------+-----------------+-------+-----+--------+\n",
      "\n",
      "Creating final table vehicles based on vehiclesRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 1.659388780593872 seconds\n",
      "Final table vehicles schema:\n",
      "root\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run_exp_3:\n",
    "    spark.udtf.register(\"PartitionUDTF\", PartitionUDTF)\n",
    "    tables, stats = load_all_tables(configs_exp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c92113c5-615b-47ee-9c71-da7cf10b64bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|tripid|cnt|\n",
      "+------+---+\n",
      "|   471|  3|\n",
      "|   481|  3|\n",
      "|   472|  2|\n",
      "|    28|  5|\n",
      "|   436|  1|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT tripid, COUNT(trip) AS cnt FROM trips GROUP BY tripid LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81bf9dda-ef14-4153-be55-42e33b61c1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "trip = spark.sql(\"SELECT * FROM trips LIMIT 1\").collect()[0].trip\n",
    "instant = spark.sql(\"SELECT * FROM instants LIMIT 1\").collect()[0].instant\n",
    "\n",
    "print(trip.at(instant))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133ce92-9f13-4b87-a2fe-2a971fc6857a",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a61da44d-f821-48ff-85a9-aa9a14c92632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|licence|model|\n",
      "+-------+-----+\n",
      "|B-CJ 17| Opel|\n",
      "+-------+-----+\n",
      "\n",
      "Query execution time:  0.3990590572357178  seconds.\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [licence#6268, model#6273]\n",
      "   +- BroadcastHashJoin [licence#6268], [licence#6271], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(licence#6268)\n",
      "      :  +- Scan In-memory table licences [licence#6268], [isnotnull(licence#6268)]\n",
      "      :        +- InMemoryRelation [licenceid#6267, licence#6268, vehid#6269], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :              +- *(1) ColumnarToRow\n",
      "      :                 +- FileScan parquet spark_catalog.default.licencesnocache[licenceid#823,licence#824,vehid#825] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/data/mobilitydb-berlinmod-sf0.1/spark-warehouse/licencesnocache], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<licenceid:int,licence:string,vehid:int>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=2018]\n",
      "         +- Filter isnotnull(licence#6271)\n",
      "            +- Scan In-memory table vehicles [licence#6271, model#6273], [isnotnull(licence#6271)]\n",
      "                  +- InMemoryRelation [vehid#6270, licence#6271, type#6272, model#6273], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                        +- *(1) ColumnarToRow\n",
      "                           +- FileScan parquet spark_catalog.default.vehiclesnocache[vehid#5569,licence#5570,type#5571,model#5572] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/data/mobilitydb-berlinmod-sf0.1/spark-warehouse/vehiclesnocache], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<vehid:int,licence:string,type:string,model:string>\n",
      "\n",
      "\n",
      "+-----------------+\n",
      "|PassengerCarCount|\n",
      "+-----------------+\n",
      "|                1|\n",
      "+-----------------+\n",
      "\n",
      "Query execution time:  0.37279438972473145  seconds.\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[count(licence#6565)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=2145]\n",
      "      +- HashAggregate(keys=[], functions=[partial_count(licence#6565)])\n",
      "         +- Project [licence#6565]\n",
      "            +- Filter (isnotnull(type#6566) AND (type#6566 = passenger))\n",
      "               +- Scan In-memory table vehicles [licence#6565, type#6566], [isnotnull(type#6566), (type#6566 = passenger)]\n",
      "                     +- InMemoryRelation [vehid#6564, licence#6565, type#6566, model#6567], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- *(1) ColumnarToRow\n",
      "                              +- FileScan parquet spark_catalog.default.vehiclesnocache[vehid#5569,licence#5570,type#5571,model#5572] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/data/mobilitydb-berlinmod-sf0.1/spark-warehouse/vehiclesnocache], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<vehid:int,licence:string,type:string,model:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run_exp_3:\n",
    "    qdfs_exp3, stats_exp3 = run_all_queries(queries, spark, explain=True, printplan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4419940b-85a9-40b7-b120-a68758101cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 12:50:53 WARN ExtractPythonUDFFromJoinCondition: The join condition:temporally_overlaps(datetime_to_tinstant(instant#8134)#8135, tile#4306)#8136 of the join plan contains PythonUDF only, it will be moved out and the join plan will be turned to cross join.\n",
      "24/05/26 12:50:53 WARN ExtractPythonUDFFromJoinCondition: The join condition:isnotnull(tpoint_at(trip#8129, instant#8134)#8137) of the join plan contains PythonUDF only, it will be moved out and the join plan will be turned to cross join.\n",
      "24/05/26 13:00:34 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/05/26 13:00:34 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/05/26 13:00:35 ERROR Executor: Exception in task 0.0 in stage 141.0 (TID 136)\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/05/26 13:00:35 WARN TaskSetManager: Lost task 0.0 in stage 141.0 (TID 136) (336d95f83d14 executor driver): java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "24/05/26 13:00:35 ERROR TaskSetManager: Task 0 in stage 141.0 failed 1 times; aborting job\n",
      "[Stage 141:>                                                        (0 + 1) / 2]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o736.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 141.0 failed 1 times, most recent failure: Lost task 0.0 in stage 141.0 (TID 136) (336d95f83d14 executor driver): java.net.SocketException: Connection reset\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor102.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.net.SocketException: Connection reset\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:24\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o736.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 141.0 failed 1 times, most recent failure: Lost task 0.0 in stage 141.0 (TID 136) (336d95f83d14 executor driver): java.net.SocketException: Connection reset\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor102.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.net.SocketException: Connection reset\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 13:00:39 WARN PythonUDFRunner: Incomplete task 2.0 in stage 141 (TID 137) interrupted: Attempting to kill Python Worker\n",
      "24/05/26 13:00:39 WARN PythonUDFRunner: Incomplete task 2.0 in stage 141 (TID 137) interrupted: Attempting to kill Python Worker\n",
      "24/05/26 13:00:39 WARN TaskSetManager: Lost task 1.0 in stage 141.0 (TID 137) (336d95f83d14 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 141.0 failed 1 times, most recent failure: Lost task 0.0 in stage 141.0 (TID 136) (336d95f83d14 executor driver): java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "querytext3 = \"\"\"\n",
    "    WITH\n",
    "    veh_w_lic AS (\n",
    "        SELECT v.vehid, l.licence, v.model\n",
    "        FROM licences l, vehicles v\n",
    "        WHERE l.licence = v.licence\n",
    "    ),\n",
    "    veh_trips AS (\n",
    "        SELECT t.* \n",
    "        FROM veh_w_lic vw, trips t\n",
    "        WHERE t.vehid = vw.vehid\n",
    "    ),\n",
    "    tile_instants AS (\n",
    "        SELECT /*+ BROADCAST(gr) */ gr.tile, i.instant\n",
    "        FROM grid gr, instants i\n",
    "        WHERE temporally_overlaps(datetime_to_tinstant(i.instant), gr.tile) = TRUE\n",
    "    )\n",
    "    SELECT /*+ BROADCAST(i) */ vt.vehid, vt.tripid, vt.trip, i.instant, tpoint_at(vt.trip, i.instant) AS pos\n",
    "    FROM veh_trips vt, tile_instants i\n",
    "    WHERE tpoint_at(vt.trip, i.instant) IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "q3 = spark.sql(querytext3)\n",
    "q3.show()\n",
    "\n",
    "#def delete_nulls(partition):\n",
    "#    for row in partition:\n",
    "#        if row.pos != None:\n",
    "#            yield row\n",
    "    \n",
    "#q3.rdd.mapPartitions(delete_nulls).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6753150c-3ef4-4aab-88d4-a6b5da88e5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58 ms, sys: 17.5 ms, total: 75.5 ms\n",
      "Wall time: 305 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.udf.UserDefinedFunction at 0x7fffbcaedf10>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "@F.udf(returnType=BooleanType())\n",
    "def contains_stbox_stbox(stbox, other):\n",
    "    pymeos_initialize()\n",
    "    return stbox.contains(other)\n",
    "spark.udf.register(\"contains_stbox_stbox\", contains_stbox_stbox)\n",
    "\n",
    "@F.udf(returnType=BooleanType())\n",
    "def temporally_overlaps(temporal, other):\n",
    "    pymeos_initialize()\n",
    "    return temporal.temporally_overlaps(other)\n",
    "spark.udf.register(\"temporally_overlaps\", temporally_overlaps)\n",
    "\n",
    "@F.udf(returnType=TBoolInstUDT())\n",
    "def datetime_to_tinstant(instant):\n",
    "    pymeos_initialize()\n",
    "    return TBoolInst.from_base_time(value=True, base=instant)\n",
    "spark.udf.register(\"datetime_to_tinstant\", datetime_to_tinstant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "df1719ab-ae55-417a-b5e7-7a7153ee94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips481 = spark.sql(\"SELECT * FROM trips\").collect()\n",
    "instantss = spark.sql(\"SELECT * FROM instants\").collect()\n",
    "gridd = spark.sql(\"SELECT * FROM grid\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85cd9f59-0dbc-4800-a552-505a7b62496d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 3 0 POINT(486076.9615656639 6593385.494335708)@2020-06-02 07:23:47+00\n",
      "True 3 3 POINT(486076.9615656639 6593385.494335708)@2020-06-02 07:23:47+00\n",
      "True 3 6 POINT(486076.9615656639 6593385.494335708)@2020-06-02 07:23:47+00\n",
      "True 3 9 POINT(486076.9615656639 6593385.494335708)@2020-06-02 07:23:47+00\n",
      "True 3 12 POINT(486076.9615656639 6593385.494335708)@2020-06-02 07:23:47+00\n",
      "True 3 15 POINT(486076.9615656639 6593385.494335708)@2020-06-02 07:23:47+00\n",
      "True 3 18 POINT(486076.9615656639 6593385.494335708)@2020-06-02 07:23:47+00\n",
      "True 3 21 POINT(486076.9615656639 6593385.494335708)@2020-06-02 07:23:47+00\n",
      "True 3 24 POINT(486076.9615656639 6593385.494335708)@2020-06-02 07:23:47+00\n",
      "True 468 0 POINT(474047.34709978715 6600073.318555428)@2020-06-02 07:23:47+00\n",
      "True 468 3 POINT(474047.34709978715 6600073.318555428)@2020-06-02 07:23:47+00\n",
      "True 468 6 POINT(474047.34709978715 6600073.318555428)@2020-06-02 07:23:47+00\n",
      "True 468 9 POINT(474047.34709978715 6600073.318555428)@2020-06-02 07:23:47+00\n",
      "True 468 12 POINT(474047.34709978715 6600073.318555428)@2020-06-02 07:23:47+00\n",
      "True 468 15 POINT(474047.34709978715 6600073.318555428)@2020-06-02 07:23:47+00\n",
      "True 468 18 POINT(474047.34709978715 6600073.318555428)@2020-06-02 07:23:47+00\n",
      "True 468 21 POINT(474047.34709978715 6600073.318555428)@2020-06-02 07:23:47+00\n",
      "True 468 24 POINT(474047.34709978715 6600073.318555428)@2020-06-02 07:23:47+00\n",
      "True 486 2 POINT(479077.3319673762 6595642.131457843)@2020-06-10 07:52:03+00\n",
      "True 486 5 POINT(479077.3319673762 6595642.131457843)@2020-06-10 07:52:03+00\n",
      "True 486 8 POINT(479077.3319673762 6595642.131457843)@2020-06-10 07:52:03+00\n",
      "True 486 11 POINT(479077.3319673762 6595642.131457843)@2020-06-10 07:52:03+00\n",
      "True 486 14 POINT(479077.3319673762 6595642.131457843)@2020-06-10 07:52:03+00\n",
      "True 486 17 POINT(479077.3319673762 6595642.131457843)@2020-06-10 07:52:03+00\n",
      "True 486 20 POINT(479077.3319673762 6595642.131457843)@2020-06-10 07:52:03+00\n",
      "True 486 23 POINT(479077.3319673762 6595642.131457843)@2020-06-10 07:52:03+00\n",
      "True 486 26 POINT(479077.3319673762 6595642.131457843)@2020-06-10 07:52:03+00\n",
      "True 479 1 POINT(480068.1952894789 6597250.3044536915)@2020-06-05 19:37:01+00\n",
      "True 479 4 POINT(480068.1952894789 6597250.3044536915)@2020-06-05 19:37:01+00\n",
      "True 479 7 POINT(480068.1952894789 6597250.3044536915)@2020-06-05 19:37:01+00\n",
      "True 479 10 POINT(480068.1952894789 6597250.3044536915)@2020-06-05 19:37:01+00\n",
      "True 479 13 POINT(480068.1952894789 6597250.3044536915)@2020-06-05 19:37:01+00\n",
      "True 479 16 POINT(480068.1952894789 6597250.3044536915)@2020-06-05 19:37:01+00\n",
      "True 479 19 POINT(480068.1952894789 6597250.3044536915)@2020-06-05 19:37:01+00\n",
      "True 479 22 POINT(480068.1952894789 6597250.3044536915)@2020-06-05 19:37:01+00\n",
      "True 479 25 POINT(480068.1952894789 6597250.3044536915)@2020-06-05 19:37:01+00\n",
      "True 423 1 POINT(476464.00097352004 6583799.716135974)@2020-06-07 10:27:09+00\n",
      "True 423 4 POINT(476464.00097352004 6583799.716135974)@2020-06-07 10:27:09+00\n",
      "True 423 7 POINT(476464.00097352004 6583799.716135974)@2020-06-07 10:27:09+00\n",
      "True 423 10 POINT(476464.00097352004 6583799.716135974)@2020-06-07 10:27:09+00\n",
      "True 423 13 POINT(476464.00097352004 6583799.716135974)@2020-06-07 10:27:09+00\n",
      "True 423 16 POINT(476464.00097352004 6583799.716135974)@2020-06-07 10:27:09+00\n",
      "True 423 19 POINT(476464.00097352004 6583799.716135974)@2020-06-07 10:27:09+00\n",
      "True 423 22 POINT(476464.00097352004 6583799.716135974)@2020-06-07 10:27:09+00\n",
      "True 423 25 POINT(476464.00097352004 6583799.716135974)@2020-06-07 10:27:09+00\n",
      "True 12 1 POINT(484206.63030196505 6591847.850560098)@2020-06-05 15:52:40+00\n",
      "True 12 4 POINT(484206.63030196505 6591847.850560098)@2020-06-05 15:52:40+00\n",
      "True 12 7 POINT(484206.63030196505 6591847.850560098)@2020-06-05 15:52:40+00\n",
      "True 12 10 POINT(484206.63030196505 6591847.850560098)@2020-06-05 15:52:40+00\n",
      "True 12 13 POINT(484206.63030196505 6591847.850560098)@2020-06-05 15:52:40+00\n",
      "True 12 16 POINT(484206.63030196505 6591847.850560098)@2020-06-05 15:52:40+00\n",
      "True 12 19 POINT(484206.63030196505 6591847.850560098)@2020-06-05 15:52:40+00\n",
      "True 12 22 POINT(484206.63030196505 6591847.850560098)@2020-06-05 15:52:40+00\n",
      "True 12 25 POINT(484206.63030196505 6591847.850560098)@2020-06-05 15:52:40+00\n",
      "True 5 0 POINT(488061.49399596895 6593497.774580877)@2020-06-03 07:09:40+00\n",
      "True 5 3 POINT(488061.49399596895 6593497.774580877)@2020-06-03 07:09:40+00\n",
      "True 5 6 POINT(488061.49399596895 6593497.774580877)@2020-06-03 07:09:40+00\n",
      "True 5 9 POINT(488061.49399596895 6593497.774580877)@2020-06-03 07:09:40+00\n",
      "True 5 12 POINT(488061.49399596895 6593497.774580877)@2020-06-03 07:09:40+00\n",
      "True 5 15 POINT(488061.49399596895 6593497.774580877)@2020-06-03 07:09:40+00\n",
      "True 5 18 POINT(488061.49399596895 6593497.774580877)@2020-06-03 07:09:40+00\n",
      "True 5 21 POINT(488061.49399596895 6593497.774580877)@2020-06-03 07:09:40+00\n",
      "True 5 24 POINT(488061.49399596895 6593497.774580877)@2020-06-03 07:09:40+00\n",
      "True 17 2 POINT(481439.3264626756 6588556.780657483)@2020-06-08 07:02:35+00\n",
      "True 17 5 POINT(481439.3264626756 6588556.780657483)@2020-06-08 07:02:35+00\n",
      "True 17 8 POINT(481439.3264626756 6588556.780657483)@2020-06-08 07:02:35+00\n",
      "True 17 11 POINT(481439.3264626756 6588556.780657483)@2020-06-08 07:02:35+00\n",
      "True 17 14 POINT(481439.3264626756 6588556.780657483)@2020-06-08 07:02:35+00\n",
      "True 17 17 POINT(481439.3264626756 6588556.780657483)@2020-06-08 07:02:35+00\n",
      "True 17 20 POINT(481439.3264626756 6588556.780657483)@2020-06-08 07:02:35+00\n",
      "True 17 23 POINT(481439.3264626756 6588556.780657483)@2020-06-08 07:02:35+00\n",
      "True 17 26 POINT(481439.3264626756 6588556.780657483)@2020-06-08 07:02:35+00\n",
      "True 482 2 POINT(478787.2126500084 6596943.844546013)@2020-06-08 07:02:35+00\n",
      "True 482 5 POINT(478787.2126500084 6596943.844546013)@2020-06-08 07:02:35+00\n",
      "True 482 8 POINT(478787.2126500084 6596943.844546013)@2020-06-08 07:02:35+00\n",
      "True 482 11 POINT(478787.2126500084 6596943.844546013)@2020-06-08 07:02:35+00\n",
      "True 482 14 POINT(478787.2126500084 6596943.844546013)@2020-06-08 07:02:35+00\n",
      "True 482 17 POINT(478787.2126500084 6596943.844546013)@2020-06-08 07:02:35+00\n",
      "True 482 20 POINT(478787.2126500084 6596943.844546013)@2020-06-08 07:02:35+00\n",
      "True 482 23 POINT(478787.2126500084 6596943.844546013)@2020-06-08 07:02:35+00\n",
      "True 482 26 POINT(478787.2126500084 6596943.844546013)@2020-06-08 07:02:35+00\n",
      "True 11 1 POINT(497189.18226655765 6598303.990789301)@2020-06-05 06:48:41+00\n",
      "True 11 4 POINT(497189.18226655765 6598303.990789301)@2020-06-05 06:48:41+00\n",
      "True 11 7 POINT(497189.18226655765 6598303.990789301)@2020-06-05 06:48:41+00\n",
      "True 11 10 POINT(497189.18226655765 6598303.990789301)@2020-06-05 06:48:41+00\n",
      "True 11 13 POINT(497189.18226655765 6598303.990789301)@2020-06-05 06:48:41+00\n",
      "True 11 16 POINT(497189.18226655765 6598303.990789301)@2020-06-05 06:48:41+00\n",
      "True 11 19 POINT(497189.18226655765 6598303.990789301)@2020-06-05 06:48:41+00\n",
      "True 11 22 POINT(497189.18226655765 6598303.990789301)@2020-06-05 06:48:41+00\n",
      "True 11 25 POINT(497189.18226655765 6598303.990789301)@2020-06-05 06:48:41+00\n",
      "True 6 0 POINT(497022.24998224626 6597938.150603392)@2020-06-03 15:08:29+00\n",
      "True 6 3 POINT(497022.24998224626 6597938.150603392)@2020-06-03 15:08:29+00\n",
      "True 6 6 POINT(497022.24998224626 6597938.150603392)@2020-06-03 15:08:29+00\n",
      "True 6 9 POINT(497022.24998224626 6597938.150603392)@2020-06-03 15:08:29+00\n",
      "True 6 12 POINT(497022.24998224626 6597938.150603392)@2020-06-03 15:08:29+00\n",
      "True 6 15 POINT(497022.24998224626 6597938.150603392)@2020-06-03 15:08:29+00\n",
      "True 6 18 POINT(497022.24998224626 6597938.150603392)@2020-06-03 15:08:29+00\n",
      "True 6 21 POINT(497022.24998224626 6597938.150603392)@2020-06-03 15:08:29+00\n",
      "True 6 24 POINT(497022.24998224626 6597938.150603392)@2020-06-03 15:08:29+00\n",
      "True 6 0 None\n",
      "True 6 3 None\n",
      "True 6 6 None\n",
      "True 6 9 None\n",
      "True 6 12 None\n",
      "True 6 15 None\n",
      "True 6 18 None\n",
      "True 6 21 None\n",
      "True 6 24 None\n",
      "True 427 2 POINT(484579.29599021055 6589136.500011157)@2020-06-08 15:56:49+00\n",
      "True 427 5 POINT(484579.29599021055 6589136.500011157)@2020-06-08 15:56:49+00\n",
      "True 427 8 POINT(484579.29599021055 6589136.500011157)@2020-06-08 15:56:49+00\n",
      "True 427 11 POINT(484579.29599021055 6589136.500011157)@2020-06-08 15:56:49+00\n",
      "True 427 14 POINT(484579.29599021055 6589136.500011157)@2020-06-08 15:56:49+00\n",
      "True 427 17 POINT(484579.29599021055 6589136.500011157)@2020-06-08 15:56:49+00\n",
      "True 427 20 POINT(484579.29599021055 6589136.500011157)@2020-06-08 15:56:49+00\n",
      "True 427 23 POINT(484579.29599021055 6589136.500011157)@2020-06-08 15:56:49+00\n",
      "True 427 26 POINT(484579.29599021055 6589136.500011157)@2020-06-08 15:56:49+00\n",
      "True 483 2 POINT(479147.39655927784 6595476.865384283)@2020-06-08 15:56:49+00\n",
      "True 483 5 POINT(479147.39655927784 6595476.865384283)@2020-06-08 15:56:49+00\n",
      "True 483 8 POINT(479147.39655927784 6595476.865384283)@2020-06-08 15:56:49+00\n",
      "True 483 11 POINT(479147.39655927784 6595476.865384283)@2020-06-08 15:56:49+00\n",
      "True 483 14 POINT(479147.39655927784 6595476.865384283)@2020-06-08 15:56:49+00\n",
      "True 483 17 POINT(479147.39655927784 6595476.865384283)@2020-06-08 15:56:49+00\n",
      "True 483 20 POINT(479147.39655927784 6595476.865384283)@2020-06-08 15:56:49+00\n",
      "True 483 23 POINT(479147.39655927784 6595476.865384283)@2020-06-08 15:56:49+00\n",
      "True 483 26 POINT(479147.39655927784 6595476.865384283)@2020-06-08 15:56:49+00\n",
      "True 19 2 POINT(492937.03304914286 6605447.156496184)@2020-06-08 19:18:39+00\n",
      "True 19 5 POINT(492937.03304914286 6605447.156496184)@2020-06-08 19:18:39+00\n",
      "True 19 8 POINT(492937.03304914286 6605447.156496184)@2020-06-08 19:18:39+00\n",
      "True 19 11 POINT(492937.03304914286 6605447.156496184)@2020-06-08 19:18:39+00\n",
      "True 19 14 POINT(492937.03304914286 6605447.156496184)@2020-06-08 19:18:39+00\n",
      "True 19 17 POINT(492937.03304914286 6605447.156496184)@2020-06-08 19:18:39+00\n",
      "True 19 20 POINT(492937.03304914286 6605447.156496184)@2020-06-08 19:18:39+00\n",
      "True 19 23 POINT(492937.03304914286 6605447.156496184)@2020-06-08 19:18:39+00\n",
      "True 19 26 POINT(492937.03304914286 6605447.156496184)@2020-06-08 19:18:39+00\n",
      "True 23 2 POINT(481463.90721817396 6588849.097729764)@2020-06-10 07:48:39+00\n",
      "True 23 5 POINT(481463.90721817396 6588849.097729764)@2020-06-10 07:48:39+00\n",
      "True 23 8 POINT(481463.90721817396 6588849.097729764)@2020-06-10 07:48:39+00\n",
      "True 23 11 POINT(481463.90721817396 6588849.097729764)@2020-06-10 07:48:39+00\n",
      "True 23 14 POINT(481463.90721817396 6588849.097729764)@2020-06-10 07:48:39+00\n",
      "True 23 17 POINT(481463.90721817396 6588849.097729764)@2020-06-10 07:48:39+00\n",
      "True 23 20 POINT(481463.90721817396 6588849.097729764)@2020-06-10 07:48:39+00\n",
      "True 23 23 POINT(481463.90721817396 6588849.097729764)@2020-06-10 07:48:39+00\n",
      "True 23 26 POINT(481463.90721817396 6588849.097729764)@2020-06-10 07:48:39+00\n",
      "True 484 2 POINT(479133.8218773338 6596218.165344885)@2020-06-09 07:54:01+00\n",
      "True 484 5 POINT(479133.8218773338 6596218.165344885)@2020-06-09 07:54:01+00\n",
      "True 484 8 POINT(479133.8218773338 6596218.165344885)@2020-06-09 07:54:01+00\n",
      "True 484 11 POINT(479133.8218773338 6596218.165344885)@2020-06-09 07:54:01+00\n",
      "True 484 14 POINT(479133.8218773338 6596218.165344885)@2020-06-09 07:54:01+00\n",
      "True 484 17 POINT(479133.8218773338 6596218.165344885)@2020-06-09 07:54:01+00\n",
      "True 484 20 POINT(479133.8218773338 6596218.165344885)@2020-06-09 07:54:01+00\n",
      "True 484 23 POINT(479133.8218773338 6596218.165344885)@2020-06-09 07:54:01+00\n",
      "True 484 26 POINT(479133.8218773338 6596218.165344885)@2020-06-09 07:54:01+00\n"
     ]
    }
   ],
   "source": [
    "for j, instantrow in enumerate(instantss):\n",
    "    inst = instantrow.instant\n",
    "    for k, triprow in enumerate(trips481):\n",
    "        tp = triprow.trip\n",
    "        tpid = triprow.tripid\n",
    "        if tp.temporally_contains(inst):\n",
    "            #print(tile.tmin(), tile.tmax(), inst, tile.contains(inst))\n",
    "            #print(tpid, tp.at(inst))\n",
    "            for i, griddrow in enumerate(gridd):\n",
    "                tile = griddrow.tile\n",
    "                tileid = griddrow.tileid\n",
    "                if tile.overlaps(inst):\n",
    "                    print(tile.contains(inst), tpid, tileid, tp.at(inst))\n",
    "        #if k == 2: break\n",
    "    #if j == 1: break\n",
    "#if i == 1: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5995c3c4-daa9-4c5e-add1-9111b1f89789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 12:44:43 WARN ExtractPythonUDFFromJoinCondition: The join condition:temporally_overlaps(datetime_to_tinstant(instant#7990)#7991, tile#4306)#7993 of the join plan contains PythonUDF only, it will be moved out and the join plan will be turned to cross join.\n",
      "[Stage 133:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|837     |\n",
      "+--------+\n",
      "\n",
      "CPU times: user 240 ms, sys: 18.5 ms, total: 259 ms\n",
      "Wall time: 1min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"\"\"\n",
    "        SELECT /*+ BROADCAST(gr) */ COUNT(*)\n",
    "        FROM grid gr, instants i\n",
    "        WHERE temporally_overlaps(datetime_to_tinstant(i.instant), gr.tile) = TRUE\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46e89231-362e-4327-b91f-22fb65d49157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|datetime_to_tinstant(instant)|\n",
      "+-----------------------------+\n",
      "|         t@2020-06-01 19:4...|\n",
      "+-----------------------------+\n",
      "\n",
      "CPU times: user 17.6 ms, sys: 10.6 ms, total: 28.2 ms\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\"SELECT datetime_to_tinstant(instant) FROM instants LIMIT 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203c47c-4a5b-4367-8ef5-437b19a0bf97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
