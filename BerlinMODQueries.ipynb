{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fe117f-48aa-4ab6-987c-49039d119911",
   "metadata": {},
   "source": [
    "# BerlinMOD Queries\n",
    "\n",
    "So far we have replicated the BerlinMOD Pymeos tutorial using Pyspark. Now we will execute the BerlinMOD queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22dd950a-e0c5-4d0c-b9a2-b53527a91ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mobilitydb-berlinmod-sf0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd \"../mobilitydb-berlinmod-sf0.1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1076035-c1e1-44e3-a9dd-d2c8fffac29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32minstants.csv\u001b[0m*  \u001b[01;32mpoints.csv\u001b[0m*       \u001b[01;32mtrips.csv\u001b[0m*       \u001b[01;32mvehicles.csv\u001b[0m*\n",
      "\u001b[01;32mlicences.csv\u001b[0m*  \u001b[01;32mregions.csv\u001b[0m*      trips_small.csv  vehicles_small.csv\n",
      "\u001b[01;32mperiods.csv\u001b[0m*   \u001b[01;34mspark-warehouse\u001b[0m/  vehicle_ids.txt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fc1f9b-a67a-45df-8d9a-d1d82a581c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86057f-63c8-4fb4-9561-f20db763733d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468925bf-ca78-4cf0-aa80-bbb83894b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymeos import *\n",
    "from pymeos.plotters import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pysparkmeos.UDT.MeosDatatype import *\n",
    "from pysparkmeos.partitions.grid.grid_partitioner import GridPartition\n",
    "from pysparkmeos.utils.udt_appender import udt_append\n",
    "from pysparkmeos.utils.utils import register_udfs_under_spark_sql\n",
    "\n",
    "\n",
    "from pysparkmeos.UDF.udf import *\n",
    "from pysparkmeos.partitions.mobilityrdd import MobilityRDD\n",
    "\n",
    "\n",
    "import random, datetime, os, sys\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone\n",
    "import contextily as cx\n",
    "import distinctipy\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import shapely.geometry as shp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely import wkb, box, from_wkb\n",
    "from typing import Union\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ada590-2fc7-4b1b-bad3-df9609ccf51e",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4df066b-5a3f-483e-a5e8-7546c7d5068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/08 19:46:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/08 19:46:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.default.parallelism: 50\n"
     ]
    }
   ],
   "source": [
    "def startspark():\n",
    "    # Initialize PyMEOS\n",
    "    pymeos_initialize(\"UTC\")\n",
    "    \n",
    "    os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    \n",
    "    #.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    # Initialize a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PySpark UDF Example with PyMEOS\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.default.parallelism\", 50) \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # spark.sparkContext.setLogLevel(\"INFO\")\n",
    "    \n",
    "    # Append the UDT mapping to the PyMEOS classes\n",
    "    udt_append()\n",
    "    \n",
    "    # Get the value of 'spark.default.parallelism'\n",
    "    default_parallelism = spark.sparkContext.getConf().get(\"spark.default.parallelism\")\n",
    "    print(f\"spark.default.parallelism: {default_parallelism}\")\n",
    "\n",
    "    # Register udfs in Spark SQL\n",
    "    register_udfs_under_spark_sql(spark)\n",
    "    \n",
    "    return spark\n",
    "\n",
    "spark = startspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132474c-76cd-47b5-ab64-3cd314014e6e",
   "metadata": {},
   "source": [
    "## Load Tables\n",
    "We will use the power of Spark SQL to read in the raw dataframes and then create the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717e9053-a9b1-4760-b3da-400d05c0639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32minstants.csv\u001b[0m*  \u001b[01;32mperiods.csv\u001b[0m*  \u001b[01;32mregions.csv\u001b[0m*  trips_small.csv  \u001b[01;32mvehicles.csv\u001b[0m*\n",
      "\u001b[01;32mlicences.csv\u001b[0m*  \u001b[01;32mpoints.csv\u001b[0m*   \u001b[01;32mtrips.csv\u001b[0m*    vehicle_ids.txt  vehicles_small.csv\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "070db614-efc2-41eb-af34-397671ab8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(spark, path, tablename, partition_key=None, transformation_query=None, **kwargs):\n",
    "    print(\"Reading raw csv \", path)\n",
    "    rawdf = spark.read.csv(path, **kwargs)\n",
    "\n",
    "    print(\"Creating temp view of raw table\")\n",
    "    rawdf.createOrReplaceTempView(f\"{tablename}Raw\")\n",
    "\n",
    "    print(\"Schema and statistics of raw table\")\n",
    "    rawdf.printSchema()\n",
    "    rawdf.describe().show()\n",
    "    print(f\"Creating final table {tablename} based on {tablename}Raw, partitioned by {partition_key}.\")\n",
    "    spark.sql(f\"\"\"DROP TABLE IF EXISTS {tablename}\"\"\")\n",
    "\n",
    "    if transformation_query:\n",
    "        rawdf = spark.sql(transformation_query)\n",
    "        rawdf.createOrReplaceTempView(f\"{tablename}Raw\")\n",
    "\n",
    "    start = time()\n",
    "    \n",
    "    if partition_key:\n",
    "        df = spark.sql(f\"\"\"\n",
    "        CREATE TABLE {tablename}\n",
    "        USING parquet\n",
    "        PARTITIONED BY ({partition_key})\n",
    "        AS SELECT * FROM {tablename}Raw\n",
    "        \"\"\")\n",
    "    else:\n",
    "        df = spark.sql(f\"\"\"\n",
    "        CREATE TABLE {tablename}\n",
    "        USING parquet\n",
    "        AS SELECT * FROM {tablename}Raw\n",
    "        \"\"\")\n",
    "        \n",
    "    end = time()\n",
    "    \n",
    "    if partition_key:\n",
    "        print(f\"{tablename} partitions:\")\n",
    "        spark.sql(f\"\"\"\n",
    "        SHOW PARTITIONS {tablename}\n",
    "        \"\"\").show()\n",
    "    print(f\"Final table created in {end-start} seconds\")\n",
    "\n",
    "    df = spark.sql(f\"SELECT * FROM {tablename}\")\n",
    "\n",
    "    print(f\"Final table {tablename} schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    #Drop the temporary view\n",
    "    spark.catalog.dropTempView(f\"{tablename}Raw\")\n",
    "    return df, (start, end, end-start)\n",
    "\n",
    "\n",
    "def load_all_tables(configs):\n",
    "    tables = {}\n",
    "    stats = {}\n",
    "    for tablename, config in configs.items():\n",
    "        table, stat = load_table(**config)\n",
    "        tables[tablename] = table\n",
    "        stats[tablename] = stat\n",
    "    return tables, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd062d6-e9cb-45df-9e9b-be54620d1902",
   "metadata": {},
   "source": [
    "### Instants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbdb2a17-8371-48b2-90f0-9a24ce0d3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instants, statsinstants = load_table(spark, \"instants.csv\", 'instants', inferSchema=True, header=True)\n",
    "#instants.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb35ef8-6bec-4722-9a83-e8de93483991",
   "metadata": {},
   "source": [
    "### Licences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09afb698-b7a8-4e44-aa12-21251ae18523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#licences, statslicences = load_table(spark, \"licences.csv\", 'licences', inferSchema=True, header=True)\n",
    "#licences.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e82da0-5cfb-4596-95c9-f79e6c2bfa0b",
   "metadata": {},
   "source": [
    "### Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b12ebc3-4179-4f57-978a-fb3dbed526f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transperiod = \"\"\"\n",
    "SELECT periodid, beginp, endp, tstzspan(period) AS period FROM periodsRaw\n",
    "\"\"\"\n",
    "#periods, statsperiods = load_table(spark, \"periods.csv\", 'periods', transformation_query=transperiod, inferSchema=True, header=True)\n",
    "#periods.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da518bb7-2aed-4329-8d31-2f6cb0dc9f00",
   "metadata": {},
   "source": [
    "### Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060e3f55-4db1-418b-aa90-0238c8034332",
   "metadata": {},
   "outputs": [],
   "source": [
    "transpoints = \"\"\"\n",
    "SELECT pointid, posx, posy, geometry_from_hexwkb(geom) AS geom FROM pointsRaw\n",
    "\"\"\"\n",
    "#points, statspoints = load_table(spark, \"points.csv\", 'points', transformation_query=transpoints, inferSchema=True, header=True)\n",
    "#points.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f2df3-511c-4e55-8ea7-fb09f208e46f",
   "metadata": {},
   "source": [
    "### Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40a4a4fb-7bd1-41d7-9576-1388fcaf6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "transregions = \"SELECT regionid, geometry_from_hexwkb(geom) AS geom FROM regionsRaw\"\n",
    "#regions, statsregions = load_table(spark, \"regions.csv\", 'regions', transformation_query=transregions, inferSchema=True, header=True)\n",
    "#regions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524b97d-8e8d-430e-b287-64c765fdfd90",
   "metadata": {},
   "source": [
    "### Trips\n",
    "Note: Use trips_small for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321f6985-9bc1-44ad-b775-944777d89d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "transtrips = \"SELECT tripid, vehid, day, seqno, sourcenode, targetnode, trip_from_hexwkb(trip) AS trip, geometry_from_hexwkb(trajectory) AS trajectory FROM tripsRaw\"\n",
    "\n",
    "#trips, statstrips = load_table(\n",
    "#    spark, \"trips_small.csv\", 'trips', \n",
    "#    transformation_query=transtrips,\n",
    "#    partition_key= 'vehid',\n",
    "#    inferSchema=True, \n",
    "#    header=True\n",
    "#)\n",
    "#trips.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8535d1-4cb6-4e79-92d4-f8cc31a5a6b2",
   "metadata": {},
   "source": [
    "### Vehicles\n",
    "Note: Also read vehicles_small for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a37c3a23-a245-4e74-9e62-558c1bd833fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vehicles, statsvehicles = load_table(spark, \"vehicles_small.csv\", 'vehicles', inferSchema=True, header=True)\n",
    "#vehicles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2ff80-cd8c-47f2-b539-a8dc53f17388",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "First queries take a general approach and are only used to measure overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "432c9616-31ae-4bd5-91de-cc1bca394c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def query_exec(query, spark, execute=True, explain=False, explainmode=''):\n",
    "    plan = None\n",
    "    if explain:\n",
    "        plan = spark.sql(f\"EXPLAIN {explainmode} {query}\").collect()[0].plan\n",
    "    result = spark.sql(query)\n",
    "    start = time()\n",
    "    if execute:\n",
    "        result.show()\n",
    "    end = time()\n",
    "    print(\"Query execution time: \", end-start, \" seconds.\")\n",
    "    return result, (start, end, end-start), plan\n",
    "\n",
    "\n",
    "def retrieve_exec_stats(queries, starts, ends, durations, plans):\n",
    "    return pd.DataFrame({\"queries\": queries, \"start\": starts, \"end\": ends, \"duration\": durations, \"plan\": plans})\n",
    "\n",
    "\n",
    "def run_all_queries(queries, spark, execute=True, explain=True, explainmode='', printplan=False):\n",
    "    \"\"\" Utility function to run all queries through subsequent experiments \"\"\"\n",
    "    qdfs = []\n",
    "    starts = []\n",
    "    ends = []\n",
    "    durations = []\n",
    "    plans = []\n",
    "    for querytext in queries:\n",
    "        qdf, qstats, plan = query_exec(querytext, spark, execute, explain, explainmode)\n",
    "        qdfs.append(qdf)\n",
    "        starts.append(qstats[0])\n",
    "        ends.append(qstats[1])\n",
    "        durations.append(qstats[2])\n",
    "        plans.append(plan)\n",
    "        if printplan:\n",
    "            print(plan)\n",
    "    exec_stats = retrieve_exec_stats(queries, starts, ends, durations, plans)\n",
    "    return qdfs, exec_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb526c-13c6-48ee-9b9b-6b906cc39558",
   "metadata": {},
   "source": [
    "### Query 1: What are the models of the vehicles with licence plate numbers from QueryLicences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "949ce841-db85-4913-8705-b1636a14ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytext1 = \"\"\"\n",
    "    SELECT l.licence, v.model\n",
    "    FROM licences l, vehicles v\n",
    "    WHERE l.licence = v.licence\n",
    "\"\"\"\n",
    "#q1, q1stats, plan1 = query_exec(querytext1, spark, explain=True)\n",
    "#if plan1:\n",
    "#    print(plan1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb68fd4-24d2-4146-9092-9d35f5a33142",
   "metadata": {},
   "source": [
    "### Query 2: How many vehicles exist that are 'passenger' cars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aeaa634-3b72-416c-93a5-45ee013df74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytext2 = \"\"\"\n",
    "    SELECT COUNT(licence) AS PassengerCarCount\n",
    "    FROM vehicles\n",
    "    WHERE type='passenger'\n",
    "\"\"\"\n",
    "#q2, q2stats, plan2 = query_exec(querytext2, spark, explain=True)\n",
    "#if plan2:\n",
    "#    print(plan2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17ee5d-5ab1-4018-8db6-1c2a7e61fb9c",
   "metadata": {},
   "source": [
    "### Query 3: Where have the vehicles with licences from QueryLicences1 been at each of the instants from QueryInstants1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0156d33-b304-4e1a-a917-331e6e0e3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "querytext3 = \"\"\"\n",
    "    WITH \n",
    "    veh_w_lic AS (\n",
    "        SELECT v.vehid, l.licence, v.model\n",
    "        FROM licences l, vehicles v\n",
    "        WHERE l.licence = v.licence\n",
    "    ),\n",
    "    veh_trips AS (\n",
    "        SELECT t.* \n",
    "        FROM veh_w_lic vw, trips t\n",
    "        WHERE t.vehid = vw.vehid\n",
    "    )\n",
    "    SELECT vt.vehid, vt.tripid, vt.trip, i.instant, tpoint_at(vt.trip, i.instant) AS pos\n",
    "    FROM veh_trips vt, instants i\n",
    "    WHERE temporally_contains(vt.trip, i.instant) = TRUE\n",
    "\"\"\"\n",
    "#q3, q3stats, plan3 = query_exec(querytext3, spark, explain=True)\n",
    "#if plan3:\n",
    "#    print(plan3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f096b-ddd3-4130-a07d-844f39e6a0be",
   "metadata": {},
   "source": [
    "### Query 4: Which licence plate numbers belong to vehicles that have passed the points from QueryPoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dabe818-ac5b-4e65-96ff-55e251791161",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytext4 = \"\"\"\n",
    "    WITH \n",
    "    vehids_intersect AS (\n",
    "        SELECT t.vehid\n",
    "        FROM trips t, points p\n",
    "        WHERE ever_touches(t.trip, p.geom) = TRUE\n",
    "    )\n",
    "    SELECT vi.vehid, v.licence\n",
    "    FROM vehids_intersect vi, vehicles v\n",
    "\"\"\"\n",
    "#q4, q4stats, plan4 = query_exec(querytext4, spark, explain=True)\n",
    "#if plan4:\n",
    "#    print(plan4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0302e785-2667-4d07-ad31-b046a0f7aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/08 19:46:50 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|temp_clm|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df = spark.createDataFrame([\"0\"], \"string\").toDF(\"temp_clm\")\n",
    "dummy_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c9775-97ba-4e8e-b65b-15e35be83845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a4cff-57fb-4d63-8795-5f3372854252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebba818e-647c-4d2a-a4bd-4aa6abca9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [querytext1, querytext2, querytext3, querytext4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdffb7-9ce6-4891-8d02-bb87ef1c8df6",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Experiment 1: Run Queries ASIS\n",
    "First we are going to run the queries without any improvement or partitioning strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e7317-8ef6-4a00-9500-34c4fd56cb84",
   "metadata": {},
   "source": [
    "#### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f5451e8-2349-444f-ba0f-9a79710c3fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/rm: cannot remove 'spark-warehouse/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d732540-70d2-4da7-a458-d660cc7486ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp1 = {\n",
    "    'instants': {'spark': spark, 'path': 'instants.csv', 'tablename': 'instants', 'inferSchema': True, 'header': True},\n",
    "    'licences': {'spark': spark, 'path': 'licences.csv', 'tablename': 'licences', 'inferSchema': True, 'header': True},\n",
    "    'periods':  {'spark': spark, 'path': 'periods.csv', 'tablename': 'periods', 'transformation_query': transperiod, 'inferSchema': True, 'header': True},\n",
    "    'points':   {'spark': spark, 'path': 'points.csv', 'tablename': 'points', 'transformation_query': transpoints, 'inferSchema': True, 'header': True},\n",
    "    'regions':  {'spark': spark, 'path': 'regions.csv', 'tablename': 'regions', 'transformation_query':transregions, 'inferSchema': True, 'header': True},\n",
    "    'trips':    {'spark': spark, 'path': 'trips_small.csv', 'tablename': 'trips', 'transformation_query':transtrips, 'inferSchema': True, 'header': True},\n",
    "    'vehicles': {'spark': spark, 'path': 'vehicles_small.csv', 'tablename': 'vehicles', 'inferSchema': True, 'header': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2daf0c1-03dd-4813-9afd-62e3ab4b3d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw csv  instants.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- instantid: integer (nullable = true)\n",
      " |-- instant: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         instantid|\n",
      "+-------+------------------+\n",
      "|  count|               100|\n",
      "|   mean|              50.5|\n",
      "| stddev|29.011491975882016|\n",
      "|    min|                 1|\n",
      "|    max|               100|\n",
      "+-------+------------------+\n",
      "\n",
      "Creating final table instants based on instantsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 3.2364754676818848 seconds\n",
      "Final table instants schema:\n",
      "root\n",
      " |-- instantid: integer (nullable = true)\n",
      " |-- instant: timestamp (nullable = true)\n",
      "\n",
      "Reading raw csv  licences.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- licenceid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------+------------------+\n",
      "|summary|         licenceid| licence|             vehid|\n",
      "+-------+------------------+--------+------------------+\n",
      "|  count|               101|     101|               101|\n",
      "|   mean|              51.0|    NULL|319.46534653465346|\n",
      "| stddev|29.300170647967224|    NULL| 175.0106604956644|\n",
      "|    min|                 1|B-BJ 115|                 9|\n",
      "|    max|               101|B-[U 177|               622|\n",
      "+-------+------------------+--------+------------------+\n",
      "\n",
      "Creating final table licences based on licencesRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 3.7374958992004395 seconds\n",
      "Final table licences schema:\n",
      "root\n",
      " |-- licenceid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      "\n",
      "Reading raw csv  periods.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- periodid: integer (nullable = true)\n",
      " |-- beginp: timestamp (nullable = true)\n",
      " |-- endp: timestamp (nullable = true)\n",
      " |-- period: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+\n",
      "|summary|          periodid|              period|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|               100|                 100|\n",
      "|   mean|              50.5|                NULL|\n",
      "| stddev|29.011491975882016|                NULL|\n",
      "|    min|                 1|[2020-06-01 00:45...|\n",
      "|    max|               100|[2020-06-11 21:18...|\n",
      "+-------+------------------+--------------------+\n",
      "\n",
      "Creating final table periods based on periodsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 5.186887741088867 seconds\n",
      "Final table periods schema:\n",
      "root\n",
      " |-- periodid: integer (nullable = true)\n",
      " |-- beginp: timestamp (nullable = true)\n",
      " |-- endp: timestamp (nullable = true)\n",
      " |-- period: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  points.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- pointid: integer (nullable = true)\n",
      " |-- posx: double (nullable = true)\n",
      " |-- posy: double (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/08 19:47:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "|summary|           pointid|             posx|             posy|                geom|\n",
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "|  count|               100|              100|              100|                 100|\n",
      "|   mean|              50.5|486384.3413598945|6594038.933758076|                NULL|\n",
      "| stddev|29.011491975882016|7200.526060474747|6552.156274876073|                NULL|\n",
      "|    min|                 1|472428.0634008836|6577421.541139536|0101000020110F000...|\n",
      "|    max|               100| 498913.875699313|6607119.513588189|0101000020110F000...|\n",
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "\n",
      "Creating final table points based on pointsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 3.018003225326538 seconds\n",
      "Final table points schema:\n",
      "root\n",
      " |-- pointid: integer (nullable = true)\n",
      " |-- posx: double (nullable = true)\n",
      " |-- posy: double (nullable = true)\n",
      " |-- geom: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  regions.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- regionid: integer (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      "\n",
      "+-------+------------------+--------------------+\n",
      "|summary|          regionid|                geom|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|               100|                 100|\n",
      "|   mean|              50.5|                NULL|\n",
      "| stddev|29.011491975882016|                NULL|\n",
      "|    min|                 1|0103000020110F000...|\n",
      "|    max|               100|0103000020110F000...|\n",
      "+-------+------------------+--------------------+\n",
      "\n",
      "Creating final table regions based on regionsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 2.9201602935791016 seconds\n",
      "Final table regions schema:\n",
      "root\n",
      " |-- regionid: integer (nullable = true)\n",
      " |-- geom: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  trips_small.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- day: date (nullable = true)\n",
      " |-- seqno: integer (nullable = true)\n",
      " |-- sourcenode: integer (nullable = true)\n",
      " |-- targetnode: integer (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- trajectory: string (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+-----------------+-----------------+--------------------+--------------------+-------+\n",
      "|summary|           tripid|             vehid|             seqno|       sourcenode|       targetnode|                trip|          trajectory|licence|\n",
      "+-------+-----------------+------------------+------------------+-----------------+-----------------+--------------------+--------------------+-------+\n",
      "|  count|               91|                91|                91|               91|               91|                  91|                  91|      0|\n",
      "|   mean|304.3296703296703|10.956043956043956|2.5934065934065935|39454.89010989011|39454.89010989011|                NULL|                NULL|   NULL|\n",
      "| stddev|204.9905176638967| 7.067786213065726| 1.666520140079134|28341.44503743612|28341.44503743612|                NULL|                NULL|   NULL|\n",
      "|    min|                1|                 1|                 1|             1160|             1160|[0101000020110F00...|0102000020110F000...|   NULL|\n",
      "|    max|              489|                17|                 9|            79113|            79113|[0101000020110F00...|0102000020110F000...|   NULL|\n",
      "+-------+-----------------+------------------+------------------+-----------------+-----------------+--------------------+--------------------+-------+\n",
      "\n",
      "Creating final table trips based on tripsRaw, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 7.305429697036743 seconds\n",
      "Final table trips schema:\n",
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- day: date (nullable = true)\n",
      " |-- seqno: integer (nullable = true)\n",
      " |-- sourcenode: integer (nullable = true)\n",
      " |-- targetnode: integer (nullable = true)\n",
      " |-- trip: pythonuserdefined (nullable = true)\n",
      " |-- trajectory: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  vehicles_small.csv\n",
      "Creating temp view of raw table\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------+-----+--------+\n",
      "|summary|            vehid|licence| type|   model|\n",
      "+-------+-----------------+-------+-----+--------+\n",
      "|  count|                3|      3|    3|       3|\n",
      "|   mean|             11.0|   NULL| NULL|    NULL|\n",
      "| stddev|8.717797887081348|   NULL| NULL|    NULL|\n",
      "|    min|                1|B-CJ 17|  bus|    Opel|\n",
      "|    max|               17|B-PZ 15|truck|Wartburg|\n",
      "+-------+-----------------+-------+-----+--------+\n",
      "\n",
      "Creating final table vehicles based on vehiclesRaw, partitioned by None.\n",
      "Final table created in 1.6831741333007812 seconds\n",
      "Final table vehicles schema:\n",
      "root\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables, stats = load_all_tables(configs_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd607345-7614-4fcf-94c6-3d273236a70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|num|squared|\n",
      "+---+-------+\n",
      "|  1|      1|\n",
      "|  2|      4|\n",
      "|  3|      9|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@F.udtf(returnType=\"num: int, squared: int\")\n",
    "class SquareNumbers:\n",
    "    def eval(self, start: int, end: int):\n",
    "        for num in range(start, end + 1):\n",
    "            yield (num, num * num)\n",
    "\n",
    "SquareNumbers(F.lit(1), F.lit(3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "860d705f-3abd-4290-9edd-cd4520d1d837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.udtf.UserDefinedTableFunction at 0x7fffdb7ed160>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " |-- tripid: integer (nullable = true)\n",
    " |-- vehid: integer (nullable = true)\n",
    " |-- day: date (nullable = true)\n",
    " |-- seqno: integer (nullable = true)\n",
    " |-- sourcenode: integer (nullable = true)\n",
    " |-- targetnode: integer (nullable = true)\n",
    " |-- trip: pythonuserdefined (nullable = true)\n",
    " |-- trajectory: pythonuserdefined (nullable = true)\n",
    "\"\"\"\n",
    "schema = StructType([\n",
    "    StructField(\"point\", TGeomPointInstUDT())\n",
    "])\n",
    "@F.udtf(returnType=schema)\n",
    "class ExplodeGeomSeq:\n",
    "    def eval(self, trip: TGeomPointSeqWrap):\n",
    "        #print(trip['trip'])\n",
    "        #trip = trip.trip\n",
    "        pymeos_initialize()\n",
    "        instants = trip['trip'].instants()\n",
    "        for i in instants:\n",
    "            yield i,\n",
    "\n",
    "spark.udtf.register(\"explodeGeomSeq\", ExplodeGeomSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68e5f9b1-9f49-472a-94a4-ba7ffba68132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               point|\n",
      "+--------------------+\n",
      "|POINT(496253.8408...|\n",
      "|POINT(496250.4704...|\n",
      "|POINT(496231.2318...|\n",
      "|POINT(496254.3235...|\n",
      "|POINT(496254.4530...|\n",
      "|POINT(496259.5870...|\n",
      "|POINT(496262.1540...|\n",
      "|POINT(496269.8550...|\n",
      "|POINT(496270.4051...|\n",
      "|POINT(496283.2368...|\n",
      "|POINT(496283.6076...|\n",
      "|POINT(496291.7883...|\n",
      "|POINT(496291.7883...|\n",
      "|POINT(496293.4705...|\n",
      "|POINT(496310.5692...|\n",
      "|POINT(496318.5173...|\n",
      "|POINT(496320.9330...|\n",
      "|POINT(496324.3406...|\n",
      "|POINT(496331.1558...|\n",
      "|POINT(496331.6642...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM explodeGeomSeq(TABLE(SELECT trip FROM trips))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf04af-6edb-4119-aa86-133a3afd1d5b",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba12c8d-0d99-447a-b278-3b7276a3c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdfs_exp1, stats_exp1 = run_all_queries(queries, spark, explain=True, printplan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d15ac-93c5-4e9e-9418-2b8424c70ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (_id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "    rdd.unpersist()\n",
    "    print(\"Unpersisted {} rdd\".format(_id))\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b770606-3841-401a-b721-eafabd3e7cbb",
   "metadata": {},
   "source": [
    "### Experiment 2: Partition Trips by vehid, HashPartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00958e-6711-4cc8-9924-2d5f3f829dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -R rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ddff5-99bf-4e67-9612-f4b3bc3d30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = startspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10871d-410c-48f3-98a6-53700dc7450c",
   "metadata": {},
   "source": [
    "#### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b9d3b-84d7-4934-adf0-83046a8bf7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp2 = {\n",
    "    'instants': {'spark': spark, 'path': 'instants.csv', 'tablename': 'instants', 'inferSchema': True, 'header': True},\n",
    "    'licences': {'spark': spark, 'path': 'licences.csv', 'tablename': 'licences', 'inferSchema': True, 'header': True},\n",
    "    'periods':  {'spark': spark, 'path': 'periods.csv', 'tablename': 'periods', 'transformation_query': transperiod, 'inferSchema': True, 'header': True},\n",
    "    'points':   {'spark': spark, 'path': 'points.csv', 'tablename': 'points', 'transformation_query': transpoints, 'inferSchema': True, 'header': True},\n",
    "    'regions':  {'spark': spark, 'path': 'regions.csv', 'tablename': 'regions', 'transformation_query':transregions, 'inferSchema': True, 'header': True},\n",
    "    'trips':    {'spark': spark, 'path': 'trips_small.csv', 'tablename': 'trips', 'partition_key': 'vehid', 'transformation_query':transtrips, 'inferSchema': True, 'header': True},\n",
    "    'vehicles': {'spark': spark, 'path': 'vehicles_small.csv', 'tablename': 'vehicles', 'inferSchema': True, 'header': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33623a-819a-4f8b-83fe-a5348df6b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables, stats = load_all_tables(configs_exp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed2844-15fd-4b75-ab07-b70995d3ab5c",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e9f39-27de-4e99-86c4-0d002f859c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdfs_exp2, stats_exp2 = run_all_queries(queries, spark, explain=True, printplan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154ab01-e8f4-4992-b6a3-147a421ede36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7cf691-12fa-42f7-9c8d-373ad17b67ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
