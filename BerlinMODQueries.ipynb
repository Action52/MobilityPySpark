{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fe117f-48aa-4ab6-987c-49039d119911",
   "metadata": {},
   "source": [
    "# BerlinMOD Queries\n",
    "\n",
    "So far we have replicated the BerlinMOD Pymeos tutorial using Pyspark. Now we will execute a subset of the BerlinMOD queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22dd950a-e0c5-4d0c-b9a2-b53527a91ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mobilitydb-berlinmod-sf0.1\n"
     ]
    }
   ],
   "source": [
    "cd \"../mobilitydb-berlinmod-sf0.1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1076035-c1e1-44e3-a9dd-d2c8fffac29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.9G\n",
      "-rw-r--r-- 1 root root 4.0M May 31 09:09 Archivo.zip\n",
      "-rw-r--r-- 1 root root  86K May 30 17:15 brussels_region.csv\n",
      "-rwxr-xr-x 1 root root 3.3K May  3 17:17 \u001b[0m\u001b[01;32minstants.csv\u001b[0m*\n",
      "-rwxr-xr-x 1 root root 1.6K May  6 08:42 \u001b[01;32mlicences.csv\u001b[0m*\n",
      "-rwxr-xr-x 1 root root  13K May  3 17:18 \u001b[01;32mperiods.csv\u001b[0m*\n",
      "-rwxr-xr-x 1 root root 8.9K May  3 17:20 \u001b[01;32mpoints.csv\u001b[0m*\n",
      "-rwxr-xr-x 1 root root 148K May  3 17:20 \u001b[01;32mregions.csv\u001b[0m*\n",
      "drwxr-xr-x 3 root root   96 Jul 12 12:16 \u001b[01;34mspark-warehouse\u001b[0m/\n",
      "-rwxr-xr-x 1 root root 2.8G May  3 17:23 \u001b[01;32mtrips.csv\u001b[0m*\n",
      "-rw-r--r-- 1 root root 139K Jun  2 08:30 trips_sample_pymeos.csv\n",
      "-rw-r--r-- 1 root root  14M May 28 09:19 trips_small.csv\n",
      "-rw-r--r-- 1 root root   11 May 28 09:11 vehicle_ids.txt\n",
      "-rwxr-xr-x 1 root root  20K May  3 17:21 \u001b[01;32mvehicles.csv\u001b[0m*\n",
      "-rw-r--r-- 1 root root  128 May 28 09:09 vehicles_small.csv\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fc1f9b-a67a-45df-8d9a-d1d82a581c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86057f-63c8-4fb4-9561-f20db763733d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468925bf-ca78-4cf0-aa80-bbb83894b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymeos import *\n",
    "from pymeos.plotters import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pysparkmeos.UDT.MeosDatatype import *\n",
    "\n",
    "from pysparkmeos.partitions.grid.grid_partitioner import GridPartition\n",
    "from pysparkmeos.partitions.kdtree_partitioner import KDTreePartition\n",
    "from pysparkmeos.partitions.adaptive_partitioner_spark import AdaptiveBinsPartitionerSpark\n",
    "from pysparkmeos.partitions.approx_adaptive_partitioner import ApproximateAdaptiveBinsPartitioner\n",
    "\n",
    "from pysparkmeos.utils.udt_appender import *\n",
    "from pysparkmeos.utils.utils import *\n",
    "\n",
    "from pysparkmeos.UDF.udf import *\n",
    "from pysparkmeos.UDTF.BerlinMOD import *\n",
    "\n",
    "from pysparkmeos.BerlinMOD.config import load_config\n",
    "from pysparkmeos.BerlinMOD.queries import *\n",
    "from pysparkmeos.BerlinMOD.transformation_queries import *\n",
    "from pysparkmeos.BerlinMOD.partition_queries import *\n",
    "from pysparkmeos.BerlinMOD.func import *\n",
    "\n",
    "import random, datetime, os, sys\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone\n",
    "import contextily as cx\n",
    "import distinctipy\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import shapely.geometry as shp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely import wkb, box, from_wkb\n",
    "from typing import Union\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ada590-2fc7-4b1b-bad3-df9609ccf51e",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4df066b-5a3f-483e-a5e8-7546c7d5068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/12 12:19:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/12 12:19:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.default.parallelism: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/12 12:19:18 WARN SimpleFunctionRegistry: The function length replaced a previously registered function.\n",
      "24/07/12 12:19:18 WARN SimpleFunctionRegistry: The function nearest_approach_distance replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "def startspark():\n",
    "    # Initialize PyMEOS\n",
    "    pymeos_initialize(\"UTC\")\n",
    "    \n",
    "    os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    \n",
    "    # Initialize a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BerlinMOD with PySpark\") \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .config(\"spark.default.parallelism\", 12) \\\n",
    "        .config(\"spark.executor.memory\", \"3g\") \\\n",
    "        .config(\"spark.executor.cores\", 1) \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", 0) \\\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"500\") \\\n",
    "        .config(\"spark.sql.allowMultipleTableArguments.enabled\", True) \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    # Append the UDT mapping to the PyMEOS classes\n",
    "    udt_append()\n",
    "    \n",
    "    # Get the value of 'spark.default.parallelism'\n",
    "    default_parallelism = spark.sparkContext.getConf().get(\"spark.default.parallelism\")\n",
    "    print(f\"spark.default.parallelism: {default_parallelism}\")\n",
    "\n",
    "    # Register udfs in Spark SQL\n",
    "    register_udfs_under_spark_sql(spark)\n",
    "\n",
    "    # Register the udtfs in Spark SQL\n",
    "    register_udtfs_under_spark_sql(spark)\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = startspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132474c-76cd-47b5-ab64-3cd314014e6e",
   "metadata": {},
   "source": [
    "## Load Tables\n",
    "We will use the power of Spark SQL to read in the raw dataframes and then create the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717e9053-a9b1-4760-b3da-400d05c0639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo.zip          \u001b[0m\u001b[01;32mperiods.csv\u001b[0m*  trips_sample_pymeos.csv  vehicles_small.csv\n",
      "brussels_region.csv  \u001b[01;32mpoints.csv\u001b[0m*   trips_small.csv\n",
      "\u001b[01;32minstants.csv\u001b[0m*        \u001b[01;32mregions.csv\u001b[0m*  vehicle_ids.txt\n",
      "\u001b[01;32mlicences.csv\u001b[0m*        \u001b[01;32mtrips.csv\u001b[0m*    \u001b[01;32mvehicles.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdffb7-9ce6-4891-8d02-bb87ef1c8df6",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aaacd5-0f68-4451-b02e-7aff50c9bce5",
   "metadata": {},
   "source": [
    "Here you can run an experiment, select the experiment to run in this notebook.  \n",
    "Available experiments:\n",
    "1. Run Queries AS-IS (default PySpark partitioning).\n",
    "2. Run Queries with Trips partitioned by vehid, using Hash Partitioning.\n",
    "3. Run Queries with Trips partitioned by trip, using RegularGrid.\n",
    "4. Run Queries with Trips partitioned by trip, using KDTreePartitioning.\n",
    "5. Run Queries with Trips partitioned by trip, using AdaptiveBinsPartitioning with Spark background.\n",
    "6. Run Queries with Trips partitioned by trip, using ApproximateAdaptiveBinsPartitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfece9a8-ab34-4daf-be40-afe7c381d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your desired experiment number.\n",
    "run_exp_number = 3\n",
    "\n",
    "# Select the queries to run \n",
    "#querynumbers = [1, 2, 3, 4, 5, 6, 11, 12, 13, 15, 18, 20]\n",
    "querynumbers = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a83da-bb56-4e3e-829a-806206ed2e3d",
   "metadata": {},
   "source": [
    "### Set up the configurations for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d170b2de-2c4c-4784-bf63-285e29d587d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'trips': 'trips_small.csv',\n",
    "    'instants': 'instants.csv',\n",
    "    'licences': 'licences.csv',\n",
    "    'periods': 'periods.csv',\n",
    "    'points': 'points.csv',\n",
    "    'regions': 'regions.csv',\n",
    "    'vehicles': 'vehicles_small.csv'\n",
    "}\n",
    "\n",
    "transformation_queries_simple = {\n",
    "    'trips': transtripssimple,\n",
    "    'instants': transinstantssimple,\n",
    "    'periods': transperiodsimple,\n",
    "    'points': transpointssimple,\n",
    "    'regions': transregionssimple\n",
    "}\n",
    "\n",
    "transformation_queries = {\n",
    "    'trips': transtrips,\n",
    "    'instants': transinstants,\n",
    "    'periods': transperiod,\n",
    "    'points': transpoints,\n",
    "    'regions': transregions\n",
    "}\n",
    "\n",
    "partition_queries = {\n",
    "    'trips': parttrips\n",
    "}\n",
    "\n",
    "partition_keys = {\n",
    "    'trips': 'tileid'\n",
    "}\n",
    "\n",
    "num_buckets = 8\n",
    "inferSchema = True\n",
    "header = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9694a04-402d-4cdd-bfe4-aeba5cd8e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp1 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries_simple, \n",
    "    part_queries=None, \n",
    "    partition_keys=None,\n",
    "    partitioner_class=None,\n",
    "    partitioner_args=None,\n",
    "    num_buckets = None,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff5bc77-9d9a-4c16-b357-c265632aa33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp2 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries_simple, \n",
    "    part_queries=None, \n",
    "    partition_keys={'trips': 'vehid'},\n",
    "    partitioner_class=None,\n",
    "    partitioner_args=None,\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf60390-b11c-4e87-9a9b-0bd990d3ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp3 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=GridPartition,\n",
    "    partitioner_args={'cells_per_side': 8},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60025d6b-8538-4d28-98b5-b147f9cac3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp4 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=KDTreePartition,\n",
    "    partitioner_args={\n",
    "        'moving_objects': None, \n",
    "        'dimensions': ['x', 'y', 't'], \n",
    "        'max_depth': 11},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f06a812f-6f6a-4f70-8ef4-8625f8591f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp5 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=AdaptiveBinsPartitionerSpark,\n",
    "    partitioner_args={\n",
    "        'spark': spark, \n",
    "        'dfname': 'tripsRaw', \n",
    "        'colname': 'trip',\n",
    "        'num_tiles': 8, \n",
    "        'dimensions': ['x', 'y', 't'], \n",
    "        'utc': \"UTC\"},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d497b8-828d-491c-9135-e4364e7eff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp6 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=ApproximateAdaptiveBinsPartitioner,\n",
    "    partitioner_args={\n",
    "        'spark': spark,\n",
    "        'df': None, \n",
    "        'colname': 'trip',\n",
    "        'num_tiles': 8, \n",
    "        'dimensions': ['x', 'y', 't'], \n",
    "        'utc': \"UTC\",\n",
    "        'tablename': \"tripsRaw\"},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b2ac2f-c558-42a6-97b2-0a8f022e15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_configs = {\n",
    "    i+1: config \n",
    "    for i, config in enumerate([configs_exp1, configs_exp2, configs_exp3, configs_exp4, configs_exp5, configs_exp6])\n",
    "}\n",
    "config = experiment_configs[run_exp_number]\n",
    "\n",
    "queries = {\n",
    "    1: querytext1,\n",
    "    2: querytext2,\n",
    "    3: querytext3,\n",
    "    4: querytext4,\n",
    "    5: querytext5,\n",
    "    6: querytext6,\n",
    "    11: querytext11,\n",
    "    12: querytext12,\n",
    "    13: querytext13,\n",
    "    15: querytext15,\n",
    "    18: querytext18,\n",
    "    20: querytext20\n",
    "}\n",
    "\n",
    "descriptions = {\n",
    "    1: querydesc1,\n",
    "    2: querydesc2,\n",
    "    3: querydesc3,\n",
    "    4: querydesc4,\n",
    "    5: querydesc5,\n",
    "    6: querydesc6,\n",
    "    11: querydesc11,\n",
    "    12: querydesc12,\n",
    "    13: querydesc13,\n",
    "    15: querydesc15,\n",
    "    18: querydesc18,\n",
    "    20: querydesc20\n",
    "}\n",
    "\n",
    "queries_to_run = [queries[querynum] for querynum in querynumbers if querynum in queries]\n",
    "descriptions_to_run = [descriptions[querynum] for querynum in querynumbers if querynum in descriptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739a576-c606-4a54-a4cf-988ffc14e33f",
   "metadata": {},
   "source": [
    "### Run the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142c26c-c62f-470d-8d28-ec4c90cd1465",
   "metadata": {},
   "source": [
    "#### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1916b729-bf5b-4606-a4db-cab04432466b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw csv  trips_small.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temp view of raw table\n",
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "|tripid|vehid|       day|seqno|sourcenode|targetnode|                trip|          trajectory|licence|\n",
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "|     1|    1|2020-06-01|    1|     79113|     66276|[0101000020110F00...|0102000020110F000...|   NULL|\n",
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- day: date (nullable = true)\n",
      " |-- seqno: integer (nullable = true)\n",
      " |-- sourcenode: integer (nullable = true)\n",
      " |-- targetnode: integer (nullable = true)\n",
      " |-- trip: string (nullable = true)\n",
      " |-- trajectory: string (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/12 12:19:37 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+--------------------+--------------------+-------+\n",
      "|summary|            tripid|            vehid|             seqno|        sourcenode|        targetnode|                trip|          trajectory|licence|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+--------------------+--------------------+-------+\n",
      "|  count|               124|              124|               124|               124|               124|                 124|                 124|      0|\n",
      "|   mean| 405.9032258064516|14.42741935483871| 2.943548387096774|          36600.25|          36600.25|                NULL|                NULL|   NULL|\n",
      "| stddev|243.83028948596683|8.369683374548373|2.3798012465853966|26819.557657976246|26819.557657976246|                NULL|                NULL|   NULL|\n",
      "|    min|                 1|                1|                 1|              1160|              1160|[0101000020110F00...|0102000020110F000...|   NULL|\n",
      "|    max|               702|               24|                13|             79113|             79113|[0101000020110F00...|0102000020110F000...|   NULL|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+--------------------+--------------------+-------+\n",
      "\n",
      "Creating final table trips based on tripsRawNoCache, partitioned by tileid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "|tripid|vehid|       day|seqno|sourcenode|targetnode|                trip|          trajectory|licence|\n",
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "|     1|    1|2020-06-01|    1|     79113|     66276|[POINT(496253.840...|LINESTRING (49625...|   NULL|\n",
      "|     2|    1|2020-06-01|    2|     66276|     79113|[POINT(481241.171...|LINESTRING (48124...|   NULL|\n",
      "|     3|    1|2020-06-02|    1|     79113|     66276|[POINT(496253.840...|LINESTRING (49625...|   NULL|\n",
      "|     4|    1|2020-06-02|    2|     66276|     79113|[POINT(481241.171...|LINESTRING (48124...|   NULL|\n",
      "|     5|    1|2020-06-03|    1|     79113|     66276|[POINT(496253.840...|LINESTRING (49625...|   NULL|\n",
      "+------+-----+----------+-----+----------+----------+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounds:  STBOX XT(((473277.05262936745,6579811.389156611),(498784.34433982597,6606871.682578203)),[2020-06-01 06:01:41.054+00, 2020-06-11 19:49:59.511256+00])\n",
      "Time to create partitioning grid:  0.2826817035675049  seconds.\n",
      "+------+--------------------+\n",
      "|tileid|                tile|\n",
      "+------+--------------------+\n",
      "|     0|STBOX XT(((473277...|\n",
      "|     1|STBOX XT(((473277...|\n",
      "|     2|STBOX XT(((473277...|\n",
      "|     3|STBOX XT(((473277...|\n",
      "|     4|STBOX XT(((473277...|\n",
      "|     5|STBOX XT(((473277...|\n",
      "|     6|STBOX XT(((473277...|\n",
      "|     7|STBOX XT(((473277...|\n",
      "|     8|STBOX XT(((473277...|\n",
      "|     9|STBOX XT(((473277...|\n",
      "|    10|STBOX XT(((473277...|\n",
      "|    11|STBOX XT(((473277...|\n",
      "|    12|STBOX XT(((473277...|\n",
      "|    13|STBOX XT(((473277...|\n",
      "|    14|STBOX XT(((473277...|\n",
      "|    15|STBOX XT(((473277...|\n",
      "|    16|STBOX XT(((473277...|\n",
      "|    17|STBOX XT(((473277...|\n",
      "|    18|STBOX XT(((473277...|\n",
      "|    19|STBOX XT(((473277...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Creating partitioned table... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 18.91962742805481 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table trips schema:\n",
      "root\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- seqno: integer (nullable = true)\n",
      " |-- sourcenode: integer (nullable = true)\n",
      " |-- targetnode: string (nullable = true)\n",
      " |-- trajectory: pythonuserdefined (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- movingobjectid: string (nullable = true)\n",
      " |-- tileid: integer (nullable = true)\n",
      " |-- movingobject: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  instants.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temp view of raw table\n",
      "+---------+--------------------+\n",
      "|instantid|             instant|\n",
      "+---------+--------------------+\n",
      "|        1|2020-06-01 19:44:...|\n",
      "+---------+--------------------+\n",
      "\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- instantid: integer (nullable = true)\n",
      " |-- instant: timestamp (nullable = true)\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|         instantid|\n",
      "+-------+------------------+\n",
      "|  count|               100|\n",
      "|   mean|              50.5|\n",
      "| stddev|29.011491975882016|\n",
      "|    min|                 1|\n",
      "|    max|               100|\n",
      "+-------+------------------+\n",
      "\n",
      "Creating final table instants based on instantsRawNoCache, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------------------+\n",
      "|instantid|tileid|             instant|\n",
      "+---------+------+--------------------+\n",
      "|        1|     0|t@2020-06-01 19:4...|\n",
      "|        1|     8|t@2020-06-01 19:4...|\n",
      "|        1|    16|t@2020-06-01 19:4...|\n",
      "|        1|    24|t@2020-06-01 19:4...|\n",
      "|        1|    32|t@2020-06-01 19:4...|\n",
      "+---------+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 1.6655898094177246 seconds\n",
      "Final table instants schema:\n",
      "root\n",
      " |-- instantid: integer (nullable = true)\n",
      " |-- tileid: integer (nullable = true)\n",
      " |-- instant: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  licences.csv\n",
      "Creating temp view of raw table\n",
      "+---------+--------+-----+\n",
      "|licenceid| licence|vehid|\n",
      "+---------+--------+-----+\n",
      "|        1|B-QS 276|  276|\n",
      "+---------+--------+-----+\n",
      "\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- licenceid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------+------------------+\n",
      "|summary|         licenceid| licence|             vehid|\n",
      "+-------+------------------+--------+------------------+\n",
      "|  count|               101|     101|               101|\n",
      "|   mean|              51.0|    NULL|319.46534653465346|\n",
      "| stddev|29.300170647967224|    NULL| 175.0106604956644|\n",
      "|    min|                 1|B-BJ 115|                 9|\n",
      "|    max|               101|B-[U 177|               622|\n",
      "+-------+------------------+--------+------------------+\n",
      "\n",
      "Creating final table licences based on licencesRawNoCache, partitioned by None.\n",
      "Final table created in 1.2520778179168701 seconds\n",
      "Final table licences schema:\n",
      "root\n",
      " |-- licenceid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      "\n",
      "Reading raw csv  periods.csv\n",
      "Creating temp view of raw table\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|periodid|              beginp|                endp|              period|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|       1|2020-06-09 11:15:...|2020-06-09 20:38:...|[2020-06-09 13:15...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- periodid: integer (nullable = true)\n",
      " |-- beginp: timestamp (nullable = true)\n",
      " |-- endp: timestamp (nullable = true)\n",
      " |-- period: string (nullable = true)\n",
      "\n",
      "+-------+------------------+--------------------+\n",
      "|summary|          periodid|              period|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|               100|                 100|\n",
      "|   mean|              50.5|                NULL|\n",
      "| stddev|29.011491975882016|                NULL|\n",
      "|    min|                 1|[2020-06-01 00:45...|\n",
      "|    max|               100|[2020-06-11 21:18...|\n",
      "+-------+------------------+--------------------+\n",
      "\n",
      "Creating final table periods based on periodsRawNoCache, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+------+--------------------+\n",
      "|              beginp|                endp|periodid|tileid|              period|\n",
      "+--------------------+--------------------+--------+------+--------------------+\n",
      "|2020-06-09 11:15:...|2020-06-09 20:38:...|       1|     6|[2020-06-09 11:15...|\n",
      "|2020-06-09 11:15:...|2020-06-09 20:38:...|       1|    14|[2020-06-09 11:15...|\n",
      "|2020-06-09 11:15:...|2020-06-09 20:38:...|       1|    22|[2020-06-09 11:15...|\n",
      "|2020-06-09 11:15:...|2020-06-09 20:38:...|       1|    30|[2020-06-09 11:15...|\n",
      "|2020-06-09 11:15:...|2020-06-09 20:38:...|       1|    38|[2020-06-09 11:15...|\n",
      "+--------------------+--------------------+--------+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 1.4912188053131104 seconds\n",
      "Final table periods schema:\n",
      "root\n",
      " |-- beginp: timestamp (nullable = true)\n",
      " |-- endp: timestamp (nullable = true)\n",
      " |-- periodid: integer (nullable = true)\n",
      " |-- tileid: integer (nullable = true)\n",
      " |-- period: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  points.csv\n",
      "Creating temp view of raw table\n",
      "+-------+-----------------+-----------------+--------------------+\n",
      "|pointid|             posx|             posy|                geom|\n",
      "+-------+-----------------+-----------------+--------------------+\n",
      "|      1|476191.0852037612|6589454.831155596|0101000020110F000...|\n",
      "+-------+-----------------+-----------------+--------------------+\n",
      "\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- pointid: integer (nullable = true)\n",
      " |-- posx: double (nullable = true)\n",
      " |-- posy: double (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "|summary|           pointid|             posx|             posy|                geom|\n",
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "|  count|               100|              100|              100|                 100|\n",
      "|   mean|              50.5|486384.3413598945|6594038.933758076|                NULL|\n",
      "| stddev|29.011491975882016|7200.526060474747|6552.156274876073|                NULL|\n",
      "|    min|                 1|472428.0634008836|6577421.541139536|0101000020110F000...|\n",
      "|    max|               100| 498913.875699313|6607119.513588189|0101000020110F000...|\n",
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "\n",
      "Creating final table points based on pointsRawNoCache, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-------+------+--------------------+\n",
      "|             posx|             posy|pointid|tileid|                geom|\n",
      "+-----------------+-----------------+-------+------+--------------------+\n",
      "|476191.0852037612|6589454.831155596|      1|    16|POINT (476191.085...|\n",
      "|476191.0852037612|6589454.831155596|      1|    17|POINT (476191.085...|\n",
      "|476191.0852037612|6589454.831155596|      1|    18|POINT (476191.085...|\n",
      "|476191.0852037612|6589454.831155596|      1|    19|POINT (476191.085...|\n",
      "|476191.0852037612|6589454.831155596|      1|    20|POINT (476191.085...|\n",
      "+-----------------+-----------------+-------+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 1.9005284309387207 seconds\n",
      "Final table points schema:\n",
      "root\n",
      " |-- posx: double (nullable = true)\n",
      " |-- posy: double (nullable = true)\n",
      " |-- pointid: integer (nullable = true)\n",
      " |-- tileid: integer (nullable = true)\n",
      " |-- geom: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  regions.csv\n",
      "Creating temp view of raw table\n",
      "+--------+--------------------+\n",
      "|regionid|                geom|\n",
      "+--------+--------------------+\n",
      "|       1|0103000020110F000...|\n",
      "+--------+--------------------+\n",
      "\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- regionid: integer (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      "\n",
      "+-------+------------------+--------------------+\n",
      "|summary|          regionid|                geom|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|               100|                 100|\n",
      "|   mean|              50.5|                NULL|\n",
      "| stddev|29.011491975882016|                NULL|\n",
      "|    min|                 1|0103000020110F000...|\n",
      "|    max|               100|0103000020110F000...|\n",
      "+-------+------------------+--------------------+\n",
      "\n",
      "Creating final table regions based on regionsRawNoCache, partitioned by None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+\n",
      "|regionid|tileid|                geom|\n",
      "+--------+------+--------------------+\n",
      "|       1|   200|POLYGON ((483571....|\n",
      "|       1|   201|POLYGON ((483571....|\n",
      "|       1|   202|POLYGON ((483571....|\n",
      "|       1|   203|POLYGON ((483571....|\n",
      "|       1|   204|POLYGON ((483571....|\n",
      "+--------+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table created in 1.5360839366912842 seconds\n",
      "Final table regions schema:\n",
      "root\n",
      " |-- regionid: integer (nullable = true)\n",
      " |-- tileid: integer (nullable = true)\n",
      " |-- geom: pythonuserdefined (nullable = true)\n",
      "\n",
      "Reading raw csv  vehicles_small.csv\n",
      "Creating temp view of raw table\n",
      "+-----+-------+---------+-----------+\n",
      "|vehid|licence|     type|      model|\n",
      "+-----+-------+---------+-----------+\n",
      "|    1| B-EF 1|passenger|Sachsenring|\n",
      "+-----+-------+---------+-----------+\n",
      "\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------+-----+--------+\n",
      "|summary|           vehid|licence| type|   model|\n",
      "+-------+----------------+-------+-----+--------+\n",
      "|  count|               4|      4|    4|       4|\n",
      "|   mean|           14.25|   NULL| NULL|    NULL|\n",
      "| stddev|9.63932916061417|   NULL| NULL|    NULL|\n",
      "|    min|               1|B-CJ 17|  bus|Multicar|\n",
      "|    max|              24|B-PZ 15|truck|Wartburg|\n",
      "+-------+----------------+-------+-----+--------+\n",
      "\n",
      "Creating final table vehicles based on vehiclesRawNoCache, partitioned by None.\n",
      "Final table created in 1.154970645904541 seconds\n",
      "Final table vehicles schema:\n",
      "root\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- licence: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables, stats = load_all_tables(spark, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f7826-560b-4d20-b1a0-4078066ca137",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9777987-5187-4e83-b384-a513b9dfd5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/12 13:53:28 WARN SimpleFunctionRegistry: The function pandas_ever_intersects replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 4: Which licence plate numbers belong to vehicles that have passed the points from QueryPoints?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|vehid|licence|\n",
      "+-----+-------+\n",
      "+-----+-------+\n",
      "\n",
      "Query execution time:  5.410356760025024  seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DataFrame[vehid: int, licence: string],\n",
       " (1720792408.5752165, 1720792413.9855733, 5.410356760025024),\n",
       " None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f_ever_intersects(traj, other):\n",
    "    return traj.ever\n",
    "\n",
    "@F.pandas_udf(returnType=BooleanType())\n",
    "def pandas_ever_intersects(trajs: pd.Series, others: pd.Series) -> pd.Series: \n",
    "    pymeos_initialize()\n",
    "    df = pd.DataFrame()\n",
    "    df['trajs'] = trajs.apply(TGeomPointSeqSetWrap)\n",
    "    df['others'] = others.str.replace(\"POINT (\", \"\").str.replace(\")\", \"\")\n",
    "    df['others'] = df['others'].apply(lambda x: Point(float(x.split()[0]), float(x.split()[1])))\n",
    "    resp = df.apply(lambda x: x.trajs.ever_intersects(x.others), axis=1)\n",
    "    return resp\n",
    "\n",
    "spark.udf.register(\"pandas_ever_intersects\", pandas_ever_intersects)\n",
    "#spark.sql(\"SELECT pandas_ever_intersects(t.movingobject, p.geom) FROM trips t INNER JOIN points p ON (t.tileid = p.tileid)\").show()\n",
    "\n",
    "querydesc4 = \"Query 4: Which licence plate numbers belong to vehicles that have passed the points from QueryPoints?\"\n",
    "querytext4 = \"\"\"\n",
    "    WITH vehids_intersect AS (\n",
    "        SELECT t.vehid, pandas_ever_intersects(t.movingobject, p.geom)\n",
    "        FROM trips t INNER JOIN points p ON (t.tileid=p.tileid)\n",
    "        WHERE \n",
    "            pandas_ever_intersects(t.movingobject, p.geom) = TRUE\n",
    "    )\n",
    "    SELECT DISTINCT vi.vehid, v.licence\n",
    "    FROM vehids_intersect vi INNER JOIN vehicles v ON (vi.vehid=v.vehid)\n",
    "\"\"\"\n",
    "\n",
    "query_exec(querytext4, querydesc4, spark, explain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b33322-6641-47d3-8187-a13506ce5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdfs_exp, stats_exp = run_all_queries(\n",
    "    queries_to_run, \n",
    "    descriptions_to_run, \n",
    "    spark, \n",
    "    explain=True, \n",
    "    printplan=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954bea63-7268-453d-9723-f50819d2e3a8",
   "metadata": {},
   "source": [
    "## Mapping the regions and trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b67e4-0998-44e9-883a-7c47243d7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "brussels = pd.read_csv(\n",
    "    \"brussels_region.csv\", converters={\"geom\": partial(wkb.loads, hex=True)}\n",
    ")\n",
    "brussels = gpd.GeoDataFrame(brussels, geometry=\"geom\")\n",
    "brussels_geom = brussels[\"geom\"][0]\n",
    "brussels.plot(ax=ax, alpha=0.3, color='black')\n",
    "cx.add_basemap(ax, alpha=0.3)\n",
    "grid = spark.table('grid')\n",
    "\n",
    "for gridrow in grid.toLocalIterator():\n",
    "    gridrow.tile.plot_xy(axes=ax, color=\"black\", draw_filling=False)\n",
    "\n",
    "regions = spark.table('regions').select(\"regionid\", \"geom\").distinct()\n",
    "\n",
    "for regionrow in regions.toLocalIterator():\n",
    "    myPoly = gpd.GeoSeries([regionrow.geom])\n",
    "    myPoly.plot(ax=ax, alpha=0.6, color='lightgreen')\n",
    "    \n",
    "#trips = spark.table('trips').sample(0.1, seed=3).select('movingobjectid', 'movingobject')\n",
    "trips = spark.table('trips').select('movingobjectid', 'movingobject')\n",
    "colors = ['orange', 'red', 'yellow', 'blue', 'purple']\n",
    "for triprow in trips.toLocalIterator():\n",
    "    TemporalPointSequenceSetPlotter.plot_xy(\n",
    "        triprow.movingobject, axes=ax, show_markers=True, show_grid=False, color=colors[int(triprow.movingobjectid) % len(colors)]\n",
    "    )\n",
    "\n",
    "#extent = ax.get_tightbbox(fig.canvas.get_renderer()).transformed(fig.dpi_scale_trans.inverted())\n",
    "#fig.savefig(f'BerlinMODSampleplot.svg', bbox_inches=extent)  # Adjust expanded() parameters as needed\n",
    "\n",
    "plt.title(\"BerlinMOD Sample Trajectories Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c150833-a37d-401f-a7e1-8c9662b85605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
