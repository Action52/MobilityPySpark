{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fe117f-48aa-4ab6-987c-49039d119911",
   "metadata": {},
   "source": [
    "# BerlinMOD Queries\n",
    "\n",
    "So far we have replicated the BerlinMOD Pymeos tutorial using Pyspark. Now we will execute a subset of the BerlinMOD queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd950a-e0c5-4d0c-b9a2-b53527a91ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"../mobilitydb-berlinmod-sf0.1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1076035-c1e1-44e3-a9dd-d2c8fffac29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc1f9b-a67a-45df-8d9a-d1d82a581c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86057f-63c8-4fb4-9561-f20db763733d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468925bf-ca78-4cf0-aa80-bbb83894b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymeos import *\n",
    "from pymeos.plotters import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pysparkmeos.UDT.MeosDatatype import *\n",
    "\n",
    "from pysparkmeos.partitions.grid.grid_partitioner import GridPartition\n",
    "from pysparkmeos.partitions.kdtree_partitioner import KDTreePartition\n",
    "from pysparkmeos.partitions.adaptive_partitioner_spark import AdaptiveBinsPartitionerSpark\n",
    "from pysparkmeos.partitions.approx_adaptive_partitioner import ApproximateAdaptiveBinsPartitioner\n",
    "\n",
    "from pysparkmeos.utils.udt_appender import *\n",
    "from pysparkmeos.utils.utils import *\n",
    "\n",
    "from pysparkmeos.UDF.udf import *\n",
    "from pysparkmeos.UDTF.BerlinMOD import *\n",
    "\n",
    "from pysparkmeos.BerlinMOD.config import load_config\n",
    "from pysparkmeos.BerlinMOD.queries import *\n",
    "from pysparkmeos.BerlinMOD.transformation_queries import *\n",
    "from pysparkmeos.BerlinMOD.partition_queries import *\n",
    "from pysparkmeos.BerlinMOD.func import *\n",
    "\n",
    "import random, datetime, os, sys\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone\n",
    "import contextily as cx\n",
    "import distinctipy\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import shapely.geometry as shp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely import wkb, box, from_wkb\n",
    "from typing import Union\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ada590-2fc7-4b1b-bad3-df9609ccf51e",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df066b-5a3f-483e-a5e8-7546c7d5068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startspark():\n",
    "    # Initialize PyMEOS\n",
    "    pymeos_initialize(\"UTC\")\n",
    "    \n",
    "    os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    \n",
    "    # Initialize a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BerlinMOD with PySpark\") \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .config(\"spark.default.parallelism\", 12) \\\n",
    "        .config(\"spark.executor.memory\", \"3g\") \\\n",
    "        .config(\"spark.executor.cores\", 1) \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", 0) \\\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"500\") \\\n",
    "        .config(\"spark.sql.allowMultipleTableArguments.enabled\", True) \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    # Append the UDT mapping to the PyMEOS classes\n",
    "    udt_append()\n",
    "    \n",
    "    # Get the value of 'spark.default.parallelism'\n",
    "    default_parallelism = spark.sparkContext.getConf().get(\"spark.default.parallelism\")\n",
    "    print(f\"spark.default.parallelism: {default_parallelism}\")\n",
    "\n",
    "    # Register udfs in Spark SQL\n",
    "    register_udfs_under_spark_sql(spark)\n",
    "\n",
    "    # Register the udtfs in Spark SQL\n",
    "    register_udtfs_under_spark_sql(spark)\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = startspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132474c-76cd-47b5-ab64-3cd314014e6e",
   "metadata": {},
   "source": [
    "## Load Tables\n",
    "We will use the power of Spark SQL to read in the raw dataframes and then create the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e9053-a9b1-4760-b3da-400d05c0639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdffb7-9ce6-4891-8d02-bb87ef1c8df6",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aaacd5-0f68-4451-b02e-7aff50c9bce5",
   "metadata": {},
   "source": [
    "Here you can run an experiment, select the experiment to run in this notebook.  \n",
    "Available experiments:\n",
    "1. Run Queries AS-IS (default PySpark partitioning).\n",
    "2. Run Queries with Trips partitioned by vehid, using Hash Partitioning.\n",
    "3. Run Queries with Trips partitioned by trip, using RegularGrid.\n",
    "4. Run Queries with Trips partitioned by trip, using KDTreePartitioning.\n",
    "5. Run Queries with Trips partitioned by trip, using AdaptiveBinsPartitioning with Spark background.\n",
    "6. Run Queries with Trips partitioned by trip, using ApproximateAdaptiveBinsPartitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfece9a8-ab34-4daf-be40-afe7c381d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your desired experiment number.\n",
    "run_exp_number = 3\n",
    "\n",
    "# Select the queries to run \n",
    "querynumbers = [1, 2, 3, 4, 5, 6, 11, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a83da-bb56-4e3e-829a-806206ed2e3d",
   "metadata": {},
   "source": [
    "### Set up the configurations for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170b2de-2c4c-4784-bf63-285e29d587d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'trips': 'trips_small.csv',\n",
    "    'instants': 'instants.csv',\n",
    "    'licences': 'licences.csv',\n",
    "    'periods': 'periods.csv',\n",
    "    'points': 'points.csv',\n",
    "    'regions': 'regions.csv',\n",
    "    'vehicles': 'vehicles_small.csv'\n",
    "}\n",
    "\n",
    "transformation_queries_simple = {\n",
    "    'trips': transtripssimple,\n",
    "    'instants': transinstantssimple,\n",
    "    'periods': transperiodsimple,\n",
    "    'points': transpointssimple,\n",
    "    'regions': transregionssimple\n",
    "}\n",
    "\n",
    "transformation_queries = {\n",
    "    'trips': transtrips,\n",
    "    'instants': transinstants,\n",
    "    'periods': transperiod,\n",
    "    'points': transpoints,\n",
    "    'regions': transregions\n",
    "}\n",
    "\n",
    "partition_queries = {\n",
    "    'trips': parttrips\n",
    "}\n",
    "\n",
    "partition_keys = {\n",
    "    'trips': 'tileid'\n",
    "}\n",
    "\n",
    "num_buckets = 64\n",
    "inferSchema = True\n",
    "header = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9694a04-402d-4cdd-bfe4-aeba5cd8e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp1 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries_simple, \n",
    "    part_queries=None, \n",
    "    partition_keys=None,\n",
    "    partitioner_class=None,\n",
    "    partitioner_args=None,\n",
    "    num_buckets = None,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5bc77-9d9a-4c16-b357-c265632aa33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp2 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries_simple, \n",
    "    part_queries=None, \n",
    "    partition_keys={'trips': 'vehid'},\n",
    "    partitioner_class=None,\n",
    "    partitioner_args=None,\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf60390-b11c-4e87-9a9b-0bd990d3ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp3 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=GridPartition,\n",
    "    partitioner_args={'cells_per_side': 8},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60025d6b-8538-4d28-98b5-b147f9cac3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp4 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=KDTreePartition,\n",
    "    partitioner_args={\n",
    "        'moving_objects': None, \n",
    "        'dimensions': ['x', 'y', 't'], \n",
    "        'max_depth': 11},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a812f-6f6a-4f70-8ef4-8625f8591f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp5 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=AdaptiveBinsPartitionerSpark,\n",
    "    partitioner_args={\n",
    "        'spark': spark, \n",
    "        'dfname': 'tripsRaw', \n",
    "        'colname': 'trip',\n",
    "        'num_tiles': 8, \n",
    "        'dimensions': ['x', 'y', 't'], \n",
    "        'utc': \"UTC\"},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d497b8-828d-491c-9135-e4364e7eff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp6 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=ApproximateAdaptiveBinsPartitioner,\n",
    "    partitioner_args={\n",
    "        'spark': spark,\n",
    "        'df': None, \n",
    "        'colname': 'trip',\n",
    "        'num_tiles': 8, \n",
    "        'dimensions': ['x', 'y', 't'], \n",
    "        'utc': \"UTC\",\n",
    "        'tablename': \"tripsRaw\"},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2ac2f-c558-42a6-97b2-0a8f022e15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_configs = {\n",
    "    i+1: config \n",
    "    for i, config in enumerate([configs_exp1, configs_exp2, configs_exp3, configs_exp4, configs_exp5, configs_exp6])\n",
    "}\n",
    "config = experiment_configs[run_exp_number]\n",
    "\n",
    "queries = {\n",
    "    1: querytext1,\n",
    "    2: querytext2,\n",
    "    3: querytext3,\n",
    "    4: querytext4,\n",
    "    5: querytext5,\n",
    "    6: querytext6,\n",
    "    11: querytext11,\n",
    "    13: querytext13\n",
    "}\n",
    "\n",
    "queries_to_run = [queries[querynum] for querynum in querynumbers if querynum in queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739a576-c606-4a54-a4cf-988ffc14e33f",
   "metadata": {},
   "source": [
    "### Run the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142c26c-c62f-470d-8d28-ec4c90cd1465",
   "metadata": {},
   "source": [
    "#### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916b729-bf5b-4606-a4db-cab04432466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables, stats = load_all_tables(spark, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f7826-560b-4d20-b1a0-4078066ca137",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b33322-6641-47d3-8187-a13506ce5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdfs_exp, stats_exp = run_all_queries(queries_to_run, spark, explain=True, printplan=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954bea63-7268-453d-9723-f50819d2e3a8",
   "metadata": {},
   "source": [
    "## Mapping the regions and trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b67e4-0998-44e9-883a-7c47243d7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "brussels = pd.read_csv(\n",
    "    \"brussels_region.csv\", converters={\"geom\": partial(wkb.loads, hex=True)}\n",
    ")\n",
    "brussels = gpd.GeoDataFrame(brussels, geometry=\"geom\")\n",
    "brussels_geom = brussels[\"geom\"][0]\n",
    "brussels.plot(ax=ax, alpha=0.3, color='black')\n",
    "cx.add_basemap(ax, alpha=0.3)\n",
    "grid = spark.table('grid')\n",
    "\n",
    "for gridrow in grid.toLocalIterator():\n",
    "    gridrow.tile.plot_xy(axes=ax, color=\"black\", draw_filling=False)\n",
    "\n",
    "regions = spark.table('regions').select(\"regionid\", \"geom\").distinct()\n",
    "\n",
    "for regionrow in regions.toLocalIterator():\n",
    "    myPoly = gpd.GeoSeries([regionrow.geom])\n",
    "    myPoly.plot(ax=ax, alpha=0.7, color='lightblue')\n",
    "    \n",
    "#trips = spark.table('trips').sample(0.1, seed=3).select('movingobjectid', 'movingobject')\n",
    "trips = spark.table('trips').select('movingobjectid', 'movingobject')\n",
    "colors = ['orange', 'red', 'pink', 'green', 'purple']\n",
    "for triprow in trips.toLocalIterator():\n",
    "    TemporalPointSequenceSetPlotter.plot_xy(\n",
    "        triprow.movingobject, axes=ax, show_markers=True, show_grid=False, color=colors[int(triprow.movingobjectid) % len(colors)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c150833-a37d-401f-a7e1-8c9662b85605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
