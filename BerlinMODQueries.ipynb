{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fe117f-48aa-4ab6-987c-49039d119911",
   "metadata": {},
   "source": [
    "# BerlinMOD Queries\n",
    "\n",
    "So far we have replicated the BerlinMOD Pymeos tutorial using Pyspark. Now we will execute a subset of the BerlinMOD queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22dd950a-e0c5-4d0c-b9a2-b53527a91ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/sf0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd \"../sf0.1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1076035-c1e1-44e3-a9dd-d2c8fffac29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.7G\n",
      "-rw-r--r-- 1 root root  86K Jul 13 18:29 brussels_region.csv\n",
      "-rw-rw-r-- 1 root root 4.0K May 16 15:23 input_berlinmod.sql\n",
      "-rw-rw-r-- 1 root root 3.3K May 16 09:47 instants.csv\n",
      "-rw-rw-r-- 1 root root 1.6K May 16 09:47 licences.csv\n",
      "-rw-rw-r-- 1 root root 279K May 22 17:00 municipalities.csv\n",
      "-rw-rw-r-- 1 root root 1.5K May 16 15:22 output_berlinmod.sql\n",
      "-rw-rw-r-- 1 root root  13K Jul 13 16:06 periods.csv\n",
      "-rw-rw-r-- 1 root root 5.6K Jul 13 18:19 points.csv\n",
      "-rw-rw-r-- 1 root root 148K May 16 09:47 regions.csv\n",
      "-rw-rw-r-- 1 root root 2.6G May 16 09:41 tripsinput.csv\n",
      "-rw-r--r-- 1 root root  16M Jul 13 16:25 tripsinputsmall.csv\n",
      "-rw-rw-r-- 1 root root  20K May 16 09:47 vehicles.csv\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fc1f9b-a67a-45df-8d9a-d1d82a581c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/rm: cannot remove 'spark-warehouse/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm -R spark-warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86057f-63c8-4fb4-9561-f20db763733d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468925bf-ca78-4cf0-aa80-bbb83894b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymeos import *\n",
    "from pymeos.plotters import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pysparkmeos.UDT.MeosDatatype import *\n",
    "\n",
    "from pysparkmeos.partitions.grid.grid_partitioner import GridPartition\n",
    "from pysparkmeos.partitions.kdtree_partitioner import KDTreePartition\n",
    "from pysparkmeos.partitions.adaptive_partitioner_spark import AdaptiveBinsPartitionerSpark\n",
    "from pysparkmeos.partitions.approx_adaptive_partitioner import ApproximateAdaptiveBinsPartitioner\n",
    "\n",
    "from pysparkmeos.utils.udt_appender import *\n",
    "from pysparkmeos.utils.utils import *\n",
    "\n",
    "from pysparkmeos.UDF.udf import *\n",
    "from pysparkmeos.UDTF.BerlinMOD import *\n",
    "\n",
    "from pysparkmeos.BerlinMOD.config import load_config\n",
    "from pysparkmeos.BerlinMOD.queries import *\n",
    "from pysparkmeos.BerlinMOD.transformation_queries import *\n",
    "from pysparkmeos.BerlinMOD.partition_queries import *\n",
    "from pysparkmeos.BerlinMOD.func import *\n",
    "\n",
    "import random, datetime, os, sys\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone\n",
    "import contextily as cx\n",
    "import distinctipy\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import shapely.geometry as shp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely import wkb, box, from_wkb\n",
    "from typing import Union\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ada590-2fc7-4b1b-bad3-df9609ccf51e",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4df066b-5a3f-483e-a5e8-7546c7d5068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/13 20:41:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.default.parallelism: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/13 20:42:00 WARN SimpleFunctionRegistry: The function length replaced a previously registered function.\n",
      "24/07/13 20:42:00 WARN SimpleFunctionRegistry: The function nearest_approach_distance replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "def startspark():\n",
    "    # Initialize PyMEOS\n",
    "    pymeos_initialize(\"UTC\")\n",
    "    \n",
    "    os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    \n",
    "    # Initialize a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BerlinMOD with PySpark\") \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .config(\"spark.default.parallelism\", 12) \\\n",
    "        .config(\"spark.executor.memory\", \"3g\") \\\n",
    "        .config(\"spark.executor.cores\", 1) \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", 0) \\\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"500\") \\\n",
    "        .config(\"spark.sql.allowMultipleTableArguments.enabled\", True) \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    # Append the UDT mapping to the PyMEOS classes\n",
    "    udt_append()\n",
    "    \n",
    "    # Get the value of 'spark.default.parallelism'\n",
    "    default_parallelism = spark.sparkContext.getConf().get(\"spark.default.parallelism\")\n",
    "    print(f\"spark.default.parallelism: {default_parallelism}\")\n",
    "\n",
    "    # Register udfs in Spark SQL\n",
    "    register_udfs_under_spark_sql(spark)\n",
    "\n",
    "    # Register the udtfs in Spark SQL\n",
    "    register_udtfs_under_spark_sql(spark)\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = startspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132474c-76cd-47b5-ab64-3cd314014e6e",
   "metadata": {},
   "source": [
    "## Load Tables\n",
    "We will use the power of Spark SQL to read in the raw dataframes and then create the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717e9053-a9b1-4760-b3da-400d05c0639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brussels_region.csv  licences.csv          periods.csv  tripsinput.csv\n",
      "input_berlinmod.sql  municipalities.csv    points.csv   tripsinputsmall.csv\n",
      "instants.csv         output_berlinmod.sql  regions.csv  vehicles.csv\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdffb7-9ce6-4891-8d02-bb87ef1c8df6",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aaacd5-0f68-4451-b02e-7aff50c9bce5",
   "metadata": {},
   "source": [
    "Here you can run an experiment, select the experiment to run in this notebook.  \n",
    "Available experiments:\n",
    "1. Run Queries AS-IS (default PySpark partitioning).\n",
    "2. Run Queries with Trips partitioned by vehid, using Hash Partitioning.\n",
    "3. Run Queries with Trips partitioned by trip, using RegularGrid.\n",
    "4. Run Queries with Trips partitioned by trip, using KDTreePartitioning.\n",
    "5. Run Queries with Trips partitioned by trip, using AdaptiveBinsPartitioning with Spark background.\n",
    "6. Run Queries with Trips partitioned by trip, using ApproximateAdaptiveBinsPartitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfece9a8-ab34-4daf-be40-afe7c381d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your desired experiment number.\n",
    "run_exp_number = 3\n",
    "\n",
    "# Select the queries to run \n",
    "#querynumbers = [1, 2, 3, 4, 5, 6, 11, 12, 13, 15, 18, 20]\n",
    "querynumbers = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a83da-bb56-4e3e-829a-806206ed2e3d",
   "metadata": {},
   "source": [
    "### Set up the configurations for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d170b2de-2c4c-4784-bf63-285e29d587d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'trips': 'tripsinputsmall.csv',\n",
    "    'instants': 'instants.csv',\n",
    "    'licences': 'licences.csv',\n",
    "    'periods': 'periods.csv',\n",
    "    'points': 'points.csv',\n",
    "    'regions': 'regions.csv',\n",
    "    'vehicles': 'vehicles.csv',\n",
    "    'municipalities': 'municipalities.csv'\n",
    "}\n",
    "\n",
    "transformation_queries_simple = {\n",
    "    'trips': transtripssimple2,\n",
    "    'instants': transinstantssimple,\n",
    "    'periods': transperiodsimple,\n",
    "    'points': transpointssimple,\n",
    "    'regions': transregionssimple,\n",
    "    'municipalities': transmunsimple\n",
    "}\n",
    "\n",
    "transformation_queries = {\n",
    "    'trips': transtrips2,\n",
    "    'instants': transinstants,\n",
    "    'periods': transperiod,\n",
    "    'points': transpoints,\n",
    "    'regions': transregions,\n",
    "    'municipalities': transmun\n",
    "}\n",
    "\n",
    "partition_queries = {\n",
    "    'trips': parttrips\n",
    "}\n",
    "\n",
    "partition_keys = {\n",
    "    'trips': 'tileid'\n",
    "}\n",
    "\n",
    "num_buckets = 8\n",
    "inferSchema = True\n",
    "header = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9694a04-402d-4cdd-bfe4-aeba5cd8e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp1 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries_simple, \n",
    "    part_queries=None, \n",
    "    partition_keys=None,\n",
    "    partitioner_class=None,\n",
    "    partitioner_args=None,\n",
    "    num_buckets = None,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff5bc77-9d9a-4c16-b357-c265632aa33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp2 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries_simple, \n",
    "    part_queries=None, \n",
    "    partition_keys={'trips': 'vehid'},\n",
    "    partitioner_class=None,\n",
    "    partitioner_args=None,\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf60390-b11c-4e87-9a9b-0bd990d3ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp3 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=GridPartition,\n",
    "    partitioner_args={'cells_per_side': 8},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60025d6b-8538-4d28-98b5-b147f9cac3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp4 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=KDTreePartition,\n",
    "    partitioner_args={\n",
    "        'moving_objects': None, \n",
    "        'dimensions': ['x', 'y', 't'], \n",
    "        'max_depth': 11},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f06a812f-6f6a-4f70-8ef4-8625f8591f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp5 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=AdaptiveBinsPartitionerSpark,\n",
    "    partitioner_args={\n",
    "        'spark': spark, \n",
    "        'dfname': 'tripsRaw', \n",
    "        'colname': 'trip',\n",
    "        'num_tiles': 8, \n",
    "        'dimensions': ['x', 'y', 't'], \n",
    "        'utc': \"UTC\"},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d497b8-828d-491c-9135-e4364e7eff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_exp6 = load_config(\n",
    "    spark=spark, \n",
    "    paths=paths, \n",
    "    trans_queries=transformation_queries, \n",
    "    part_queries=partition_queries, \n",
    "    partition_keys=partition_keys,\n",
    "    partitioner_class=ApproximateAdaptiveBinsPartitioner,\n",
    "    partitioner_args={\n",
    "        'spark': spark,\n",
    "        'df': None, \n",
    "        'colname': 'trip',\n",
    "        'num_tiles': 8, \n",
    "        'dimensions': ['x', 'y', 't'], \n",
    "        'utc': \"UTC\",\n",
    "        'tablename': \"tripsRaw\"},\n",
    "    num_buckets = num_buckets,\n",
    "    inferSchema = inferSchema,\n",
    "    header=header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b2ac2f-c558-42a6-97b2-0a8f022e15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_configs = {\n",
    "    i+1: config \n",
    "    for i, config in enumerate([configs_exp1, configs_exp2, configs_exp3, configs_exp4, configs_exp5, configs_exp6])\n",
    "}\n",
    "config = experiment_configs[run_exp_number]\n",
    "\n",
    "queries = {\n",
    "    1: querytext1,\n",
    "    2: querytext2,\n",
    "    3: querytext3,\n",
    "    4: querytext4,\n",
    "    5: querytext5,\n",
    "    6: querytext6,\n",
    "    11: querytext11,\n",
    "    12: querytext12,\n",
    "    13: querytext13,\n",
    "    15: querytext15,\n",
    "    18: querytext18,\n",
    "    20: querytext20\n",
    "}\n",
    "\n",
    "descriptions = {\n",
    "    1: querydesc1,\n",
    "    2: querydesc2,\n",
    "    3: querydesc3,\n",
    "    4: querydesc4,\n",
    "    5: querydesc5,\n",
    "    6: querydesc6,\n",
    "    11: querydesc11,\n",
    "    12: querydesc12,\n",
    "    13: querydesc13,\n",
    "    15: querydesc15,\n",
    "    18: querydesc18,\n",
    "    20: querydesc20\n",
    "}\n",
    "\n",
    "queries_to_run = [queries[querynum] for querynum in querynumbers if querynum in queries]\n",
    "descriptions_to_run = [descriptions[querynum] for querynum in querynumbers if querynum in descriptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739a576-c606-4a54-a4cf-988ffc14e33f",
   "metadata": {},
   "source": [
    "### Run the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142c26c-c62f-470d-8d28-ec4c90cd1465",
   "metadata": {},
   "source": [
    "#### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916b729-bf5b-4606-a4db-cab04432466b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw csv  tripsinputsmall.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temp view of raw table\n",
      "+------+-----+----------+-----+--------------------+--------------------+\n",
      "|tripid|vehid| startdate|seqno|               point|                   t|\n",
      "+------+-----+----------+-----+--------------------+--------------------+\n",
      "|     1|    1|2020-06-01|    1|0101000020110F000...|2020-06-01 06:01:...|\n",
      "+------+-----+----------+-----+--------------------+--------------------+\n",
      "\n",
      "Schema and statistics of raw table\n",
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- vehid: integer (nullable = true)\n",
      " |-- startdate: date (nullable = true)\n",
      " |-- seqno: integer (nullable = true)\n",
      " |-- point: string (nullable = true)\n",
      " |-- t: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/13 20:42:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "|summary|            tripid|             vehid|             seqno|               point|\n",
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "|  count|            159418|            159418|            159418|              159418|\n",
      "|   mean|114.69066228405826|4.5976740393180195| 1.995075838362042|                NULL|\n",
      "| stddev|  85.6792387574541| 3.093698731004946|1.1111676314993488|                NULL|\n",
      "|    min|                 1|                 1|                 1|0101000020110F000...|\n",
      "|    max|               250|                 9|                 8|0101000020110F000...|\n",
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "\n",
      "Creating final table trips based on tripsRawNoCache, partitioned by tileid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================>                                   (72 + 3) / 200]\r"
     ]
    }
   ],
   "source": [
    "tables, stats = load_all_tables(spark, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f7826-560b-4d20-b1a0-4078066ca137",
   "metadata": {},
   "source": [
    "#### Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b33322-6641-47d3-8187-a13506ce5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdfs_exp, stats_exp = run_all_queries(\n",
    "    queries_to_run, \n",
    "    descriptions_to_run, \n",
    "    spark, \n",
    "    explain=True, \n",
    "    printplan=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954bea63-7268-453d-9723-f50819d2e3a8",
   "metadata": {},
   "source": [
    "## Mapping the regions and trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dce49b-17b1-4ed6-8073-57696b710c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b67e4-0998-44e9-883a-7c47243d7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "brussels = pd.read_csv(\n",
    "    \"brussels_region.csv\", converters={\"geom\": partial(wkb.loads, hex=True)}\n",
    ")\n",
    "brussels = gpd.GeoDataFrame(brussels, geometry=\"geom\")\n",
    "brussels_geom = brussels[\"geom\"][0]\n",
    "brussels.plot(ax=ax, alpha=0.3, color='black')\n",
    "cx.add_basemap(ax, alpha=0.3)\n",
    "grid = spark.table('grid')\n",
    "\n",
    "for gridrow in grid.toLocalIterator():\n",
    "    gridrow.tile.plot_xy(axes=ax, color=\"black\", draw_filling=False)\n",
    "\n",
    "regions = spark.table('regions').select(\"regionid\", \"geom\").distinct()\n",
    "\n",
    "for regionrow in regions.toLocalIterator():\n",
    "    myPoly = gpd.GeoSeries([regionrow.geom])\n",
    "    myPoly.plot(ax=ax, alpha=0.6, color='lightgreen')\n",
    "    \n",
    "#trips = spark.table('trips').sample(0.1, seed=3).select('movingobjectid', 'movingobject')\n",
    "trips = spark.table('trips').select('movingobjectid', 'movingobject')\n",
    "colors = ['orange', 'red', 'yellow', 'blue', 'purple']\n",
    "for triprow in trips.toLocalIterator():\n",
    "    TemporalPointSequenceSetPlotter.plot_xy(\n",
    "        triprow.movingobject, axes=ax, show_markers=True, show_grid=False, color=colors[int(triprow.movingobjectid) % len(colors)]\n",
    "    )\n",
    "\n",
    "#extent = ax.get_tightbbox(fig.canvas.get_renderer()).transformed(fig.dpi_scale_trans.inverted())\n",
    "#fig.savefig(f'BerlinMODSampleplot.svg', bbox_inches=extent)  # Adjust expanded() parameters as needed\n",
    "\n",
    "plt.title(\"BerlinMOD Sample Trajectories Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c150833-a37d-401f-a7e1-8c9662b85605",
   "metadata": {},
   "outputs": [],
   "source": [
    "jette_tiles = [\n",
    "    row.tile \n",
    "    for row in spark.sql(\"\"\"\n",
    "        WITH tiles AS (\n",
    "            SELECT DISTINCT tileid\n",
    "            FROM municipalities\n",
    "            WHERE name = 'Jette'\n",
    "        )\n",
    "        SELECT grid.* FROM grid INNER JOIN tiles ON (grid.tileid = tiles.tileid)\n",
    "        \"\"\").collect()\n",
    "]\n",
    "\n",
    "auderghem_tiles = [\n",
    "    row.tile \n",
    "    for row in spark.sql(\"\"\"\n",
    "        WITH tiles AS (\n",
    "            SELECT DISTINCT tileid\n",
    "            FROM municipalities\n",
    "            WHERE name = 'Auderghem - Oudergem'\n",
    "        )\n",
    "        SELECT grid.* FROM grid INNER JOIN tiles ON (grid.tileid = tiles.tileid)\n",
    "        \"\"\").collect()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2741a9a-ea4c-4847-803b-adea01ac1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "brussels = pd.read_csv(\n",
    "    \"brussels_region.csv\", converters={\"geom\": partial(wkb.loads, hex=True)}\n",
    ")\n",
    "brussels = gpd.GeoDataFrame(brussels, geometry=\"geom\")\n",
    "brussels_geom = brussels[\"geom\"][0]\n",
    "brussels.plot(ax=ax, alpha=0.3, color='black')\n",
    "cx.add_basemap(ax, alpha=0.3)\n",
    "grid = spark.table('grid')\n",
    "\n",
    "for gridrow in grid.toLocalIterator():\n",
    "    gridrow.tile.plot_xy(axes=ax, color=\"black\", draw_filling=False)\n",
    "\n",
    "for tile in jette_tiles:\n",
    "    tile.plot_xy(axes=ax, color=\"lightblue\", draw_filling=True)\n",
    "\n",
    "for tile in auderghem_tiles:\n",
    "    tile.plot_xy(axes=ax, color=\"lightblue\", draw_filling=True)\n",
    "\n",
    "munis = gpd.GeoDataFrame(spark.sql(\"SELECT DISTINCT name, geom FROM municipalities WHERE name = 'Jette' OR name = 'Auderghem - Oudergem' LIMIT 2\").toPandas(), geometry=\"geom\")\n",
    "munis.plot(ax=ax)\n",
    "\n",
    "trips = spark.table('trips').select('movingobjectid', 'movingobject')\n",
    "colors = ['orange', 'red', 'yellow', 'blue', 'purple']\n",
    "for triprow in trips.toLocalIterator():\n",
    "    TemporalPointSequenceSetPlotter.plot_xy(\n",
    "        triprow.movingobject, axes=ax, show_markers=True, show_grid=False, color=colors[int(triprow.movingobjectid) % len(colors)]\n",
    "    )\n",
    "\n",
    "#extent = ax.get_tightbbox(fig.canvas.get_renderer()).transformed(fig.dpi_scale_trans.inverted())\n",
    "#fig.savefig(f'BerlinMODSampleplot.svg', bbox_inches=extent)  # Adjust expanded() parameters as needed\n",
    "\n",
    "plt.title(\"BerlinMOD Sample Trajectories Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00e384-2355-4832-96ad-887cdcf13c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "jette_trajs = spark.sql(\"\"\"\n",
    "    WITH jette AS (\n",
    "        SELECT tileid, geom, municipalityid\n",
    "        FROM municipalities\n",
    "        WHERE name = 'Jette'\n",
    "    )\n",
    "    SELECT vehid, movingobjectid, movingobject, j.geom, t.tileid, j.municipalityid, at_geom(movingobject, geom) AS atgeom\n",
    "    FROM trips t INNER JOIN jette j ON (t.tileid = j.tileid)\n",
    "    WHERE ever_intersects(t.movingobject, j.geom) = TRUE\n",
    "    ORDER BY vehid, movingobjectid, tileid\n",
    "        \"\"\")\n",
    "\n",
    "jette_tiles = [\n",
    "    row.tile \n",
    "    for row in spark.sql(\"\"\"\n",
    "        WITH tiles AS (\n",
    "            SELECT DISTINCT tileid\n",
    "            FROM municipalities\n",
    "            WHERE name = 'Jette'\n",
    "        )\n",
    "        SELECT grid.* FROM grid INNER JOIN tiles ON (grid.tileid = tiles.tileid)\n",
    "        \"\"\").collect()\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "\n",
    "grid = spark.table('grid')\n",
    "\n",
    "for tile in jette_tiles:\n",
    "    tile.plot_xy(axes=ax, color=\"gray\", draw_filling=False)\n",
    "\n",
    "colors = ['orange', 'red', 'yellow', 'blue', 'purple']\n",
    "\n",
    "for i, row in enumerate(jette_trajs.toLocalIterator()):\n",
    "    if i == 0:\n",
    "        geom = gpd.GeoDataFrame(jette_trajs.limit(1).toPandas(), geometry=\"geom\")\n",
    "        geom.plot(ax=ax, alpha=0.5, color='blue')\n",
    "    TemporalPointSequenceSetPlotter.plot_xy(\n",
    "        row.atgeom, axes=ax, show_markers=True, show_grid=False, color='#700c04'\n",
    "    )\n",
    "cx.add_basemap(ax, alpha=0.3)\n",
    "\n",
    "#extent = ax.get_tightbbox(fig.canvas.get_renderer()).transformed(fig.dpi_scale_trans.inverted())\n",
    "#fig.savefig(f'BerlinMODSampleplot.svg', bbox_inches=extent)  # Adjust expanded() parameters as needed\n",
    "\n",
    "plt.title(\"BerlinMOD Sample Trajectories Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79d488-8fa4-4b86-961a-52bbbac1f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DISTINCT name FROM mun\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c01c4-e882-419e-ac48-80b9e4706a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
