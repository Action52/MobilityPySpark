{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9880d2e4-36f4-4540-b70e-25b385a6939a",
   "metadata": {},
   "source": [
    "# Pymeos and Pyspark Partitioning Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f9d2d-8292-4d17-9e97-21ee3df3d7e6",
   "metadata": {},
   "source": [
    "First we perform the corresponding imports for the libraries to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058c02bf-4670-4b08-b4e8-5355c2355b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymeos import *\n",
    "from pysparkmeos.UDT.MeosDatatype import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.serializers import PickleSerializer\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "from pysparkmeos.partitions.grid.grid_partitioner import GridPartition\n",
    "\n",
    "from pysparkmeos.utils.udt_appender import udt_append\n",
    "from pysparkmeos.UDF.udf import *\n",
    "from pysparkmeos.partitions.mobilityrdd import MobilityRDD\n",
    "\n",
    "import random, datetime\n",
    "\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone\n",
    "import contextily as cx\n",
    "import distinctipy\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shapely.geometry as shp\n",
    "from pymeos.plotters import (\n",
    "    TemporalPointSequenceSetPlotter,\n",
    "    TemporalPointSequencePlotter,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from shapely import wkb, box\n",
    "import pandas as pd\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a97ffcb-ae38-4f21-8b67-454b492c0d6f",
   "metadata": {},
   "source": [
    "## Initialize Pymeos and setup Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f917d538-dcf0-409d-b908-bb1d2aaa3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/15 10:05:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.default.parallelism: 3\n",
      "spark.sql.allowMultipleTableArguments.enabled: true\n"
     ]
    }
   ],
   "source": [
    "# Initialize PyMEOS\n",
    "pymeos_initialize(\"UTC\")\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "#.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark UDF Example with PyMEOS\") \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .config(\"spark.default.parallelism\", 3) \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.allowMultipleTableArguments.enabled\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "# Append the UDT mapping to the PyMEOS classes\n",
    "udt_append()\n",
    "\n",
    "# Get the value of 'spark.default.parallelism'\n",
    "default_parallelism = spark.sparkContext.getConf().get(\"spark.default.parallelism\")\n",
    "print(f\"spark.default.parallelism: {default_parallelism}\")\n",
    "print(f\"spark.sql.allowMultipleTableArguments.enabled: {spark.sparkContext.getConf().get('spark.sql.allowMultipleTableArguments.enabled')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184c30b-7ee8-4b06-b3af-216ae9518766",
   "metadata": {},
   "source": [
    "## Read the DataFrame and create the columns of Pymeos data objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d7cc32-1fc4-4f83-b84c-50cd9777a7dd",
   "metadata": {},
   "source": [
    "First, we are going to read a dataset from OpenSky, this dataset cointains the trajectory information of multiple flights within a timeframe.  \n",
    "\n",
    "Using the latitude, longitude and timestamp we can create a PyMEOS TGeogPointInst, that in PySpark will be wrapped into a TGeogPointInstUDT object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7acdd67-9b63-4bd3-a032-8cb063e48fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 10:06:07 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(icao24='ad05ce', time='2022-06-27 00:00:50', lat=32.31065135891154, lon=-97.41846201371173, Point=TGeogPointInstWrap(POINT(-97.41846201371173 32.31065135891154)@2022-06-27 00:00:50+00), x=-97.41846466064453, y=32.31064987182617, t=datetime.datetime(2022, 6, 27, 0, 0, 50), id=8589935540),\n",
       " Row(icao24='c8806a', time='2022-06-27 00:00:50', lat=-26.871002197265625, lon=154.14751160819577, Point=TGeogPointInstWrap(POINT(154.14751160819577 -26.871002197265625)@2022-06-27 00:00:50+00), x=154.1475067138672, y=-26.871002197265625, t=datetime.datetime(2022, 6, 27, 0, 0, 50), id=8589935541),\n",
       " Row(icao24='a726fb', time='2022-06-27 00:00:50', lat=61.546417236328125, lon=-151.04876926967074, Point=TGeogPointInstWrap(POINT(-151.04876926967074 61.546417236328125)@2022-06-27 00:00:50+00), x=-151.04876708984375, y=61.546417236328125, t=datetime.datetime(2022, 6, 27, 0, 0, 50), id=8589935542),\n",
       " Row(icao24='ad225b', time='2022-06-27 00:00:50', lat=37.73808288574219, lon=-121.37812188331117, Point=TGeogPointInstWrap(POINT(-121.37812188331117 37.73808288574219)@2022-06-27 00:00:50+00), x=-121.37812042236328, y=37.73808288574219, t=datetime.datetime(2022, 6, 27, 0, 0, 50), id=8589935543),\n",
       " Row(icao24='a00529', time='2022-06-27 00:00:50', lat=28.69061279296875, lon=-81.47883488581732, Point=TGeogPointInstWrap(POINT(-81.47883488581732 28.69061279296875)@2022-06-27 00:00:50+00), x=-81.47883605957031, y=28.69061279296875, t=datetime.datetime(2022, 6, 27, 0, 0, 50), id=8589935544)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from a CSV file\n",
    "data_path = \"../../small_mid_states_2022-06-27-00.csv\"  # Update this with your CSV file path\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True).select(\"icao24\", \"time\", \"lat\", \"lon\")\n",
    "\n",
    "# Clean nans, if not Points can't be created\n",
    "df = df.dropna(subset=[\"lat\", \"lon\", \"time\", \"icao24\"])\n",
    "\n",
    "    #.withColumn(\"Point\", F.concat(F.lit(\"Point(\"), F.col(\"lat\"), F.lit(\" \"), F.col(\"lon\"), F.lit(\")@\"), F.col(\"time\"))) \\\n",
    "# Convert the 'time' column to the correct format\n",
    "df = df \\\n",
    "    .withColumn(\"time\", F.from_unixtime(F.col(\"time\"), \"yyyy-MM-dd' 'HH:mm:ss\")) \\\n",
    "    .withColumn(\"Point\", create_point_udf(\"lat\", \"lon\", \"time\")) \\\n",
    "    .withColumn(\"x\", get_point_x(\"Point\")) \\\n",
    "    .withColumn(\"y\", get_point_y(\"Point\")) \\\n",
    "    .withColumn(\"t\", get_point_timestamp(\"Point\")) \\\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "#    .withColumn(\"x\", get_point_x(\"Point\")) \\\n",
    "#    .withColumn(\"y\", get_point_y(\"Point\")) \\\n",
    "#    .withColumn(\"t\", get_point_timestamp(\"Point\")) \\\n",
    "# df.createOrReplaceTempView(\"rawPoints\")\n",
    "\n",
    "df.tail(5)\n",
    "\n",
    "#for row in df.toLocalIterator():\n",
    "#    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b706c651-5fd4-4455-8d44-a4901f5571b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e2553-ab4b-4397-a99e-d5b45b0d7320",
   "metadata": {},
   "source": [
    "Handling a dataset like this gives us granularity over the datapoints, but it would be better to group the trajectories together using another function, this will return the TGeogPointSeq objects representing each trajectory. Let's create a UDTF that takes the icao24, and Point, and creates a new table with icao24, and PointSeq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7e8a34-38b9-4073-be63-77e7a1817481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|icao24|cnt|\n",
      "+------+---+\n",
      "|406471|  6|\n",
      "|34718e|  6|\n",
      "|a04417|  6|\n",
      "|ac6364|  6|\n",
      "|a054e1|  5|\n",
      "|a51b96|  5|\n",
      "|a6cf94|  5|\n",
      "|7c7aac|  5|\n",
      "|a95c2f|  5|\n",
      "|c05ee5|  5|\n",
      "|3455d9|  5|\n",
      "|ac7ac9|  5|\n",
      "|ad60b7|  5|\n",
      "|4952a5|  5|\n",
      "|a77ae0|  5|\n",
      "|a0c4a1|  5|\n",
      "|8013d8|  5|\n",
      "|a03cb6|  5|\n",
      "|700024|  5|\n",
      "|740736|  5|\n",
      "+------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "5187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|icao24|            PointSeq|\n",
      "+------+--------------------+\n",
      "|0100f6|[POINT(24.7634696...|\n",
      "|010109|[POINT(51.2653681...|\n",
      "|01013d|[POINT(10.5608825...|\n",
      "|0101a7|[POINT(12.1170895...|\n",
      "|0101bb|[POINT(51.6333065...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"icao24\", StringType()),\n",
    "    StructField(\"PointSeq\", TGeogPointSeqUDT())\n",
    "])\n",
    "\n",
    "\n",
    "@F.udf(returnType=TGeogPointSeqUDT())\n",
    "def pointSeqFromPoints(pointgroup):\n",
    "    if not pointgroup:\n",
    "        return None\n",
    "    pymeos_initialize()\n",
    "    if len(pointgroup) == 1:\n",
    "        pointgroup = f'[{pointgroup[0].__str__()}]'\n",
    "        return TGeogPointSeq(pointgroup)\n",
    "    pointgroup = sorted(pointgroup)\n",
    "    pointseq = TGeogPointSeq(instant_list=pointgroup)\n",
    "    return pointseq\n",
    "\n",
    "print(df.count())\n",
    "\n",
    "#df.select(\"Point\").tail(5)\n",
    "df.groupBy(\"icao24\").agg(F.count(\"Point\").alias(\"cnt\")).orderBy(\"cnt\",ascending=False).show()\n",
    "\n",
    "trajectories = df.groupBy(\"icao24\").agg(\n",
    "    F.collect_list(F.col(\"Point\")).alias(\"PointSeq\")\n",
    ").select(\"icao24\", \"PointSeq\").withColumn(\"PointSeq\", pointSeqFromPoints(\"PointSeq\"))\n",
    "\n",
    "print(trajectories.count())\n",
    "\n",
    "trajectories.show(5)\n",
    "\n",
    "#trajectories.rdd.getNumPartitions()\n",
    "\n",
    "#spark.sparkContext.setCheckpointDir(\"./checkpoint\")\n",
    "#trajectories = trajectories.checkpoint()\n",
    "#trajectories.tail(1).limit(1).collect()[0].PointSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb8cf7-1592-424f-bc57-0dd992930968",
   "metadata": {},
   "source": [
    "This has reduced the table from 530k Points to 7k PointSeq!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4dfb517-db6f-4d79-b36f-d0a8728f004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectories.createOrReplaceTempView(\"trajectories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29378c-7551-4e27-88f8-85225c3ff932",
   "metadata": {},
   "source": [
    "Now we need to calculate the boundaries of the whole space of trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633b489f-8c34-42f3-a63a-0c1957762d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-----+\n",
      "|icao24|            PointSeq|               STBox|seqId|\n",
      "+------+--------------------+--------------------+-----+\n",
      "|0100f6|[POINT(24.7634696...|SRID=4326;GEODSTB...|    0|\n",
      "|010109|[POINT(51.2653681...|SRID=4326;GEODSTB...|    1|\n",
      "|01013d|[POINT(10.5608825...|SRID=4326;GEODSTB...|    2|\n",
      "|0101a7|[POINT(12.1170895...|SRID=4326;GEODSTB...|    3|\n",
      "|0101bb|[POINT(51.6333065...|SRID=4326;GEODSTB...|    4|\n",
      "|0101bd|[POINT(46.7433022...|SRID=4326;GEODSTB...|    5|\n",
      "|0101dd|[POINT(55.5086263...|SRID=4326;GEODSTB...|    6|\n",
      "|010205|[POINT(13.5366892...|SRID=4326;GEODSTB...|    7|\n",
      "|010207|[POINT(26.7178734...|SRID=4326;GEODSTB...|    8|\n",
      "|01020b|[POINT(30.5806831...|SRID=4326;GEODSTB...|    9|\n",
      "|01022e|[POINT(19.7302592...|SRID=4326;GEODSTB...|   10|\n",
      "|017073|[POINT(120.854524...|SRID=4326;GEODSTB...|   11|\n",
      "|01802e|[POINT(29.6798639...|SRID=4326;GEODSTB...|   12|\n",
      "|02006f|[POINT(-1.7363175...|SRID=4326;GEODSTB...|   13|\n",
      "|02009f|[POINT(-7.1850585...|SRID=4326;GEODSTB...|   14|\n",
      "|020118|[POINT(19.4874364...|SRID=4326;GEODSTB...|   15|\n",
      "|020123|[POINT(-79.194523...|SRID=4326;GEODSTB...|   16|\n",
      "|020124|[POINT(3.45388932...|SRID=4326;GEODSTB...|   17|\n",
      "|020140|[POINT(7.62860107...|SRID=4326;GEODSTB...|   18|\n",
      "|020145|[POINT(5.95256260...|SRID=4326;GEODSTB...|   19|\n",
      "+------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymeos import TPoint\n",
    "\n",
    "@F.udf(returnType=STBoxUDT())\n",
    "def point_to_stbox(tpoint: TPoint) -> STBox:\n",
    "    pymeos_initialize()\n",
    "    return tpoint.bounding_box()\n",
    "\n",
    "trajectories = trajectories.withColumn(\"STBox\", point_to_stbox(\"PointSeq\")).withColumn(\"seqId\", F.monotonically_increasing_id())\n",
    "trajectories.createOrReplaceTempView(\"trajectories\")\n",
    "trajectories.show()\n",
    "trajectories.cache()\n",
    "print(trajectories.count())\n",
    "trajectories.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be545b0c-982e-4561-921c-cbdcb34f0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"Point\").write.mode(\"overwrite\").csv(\"../../small_states_2022-06-27-00_only_points\")\n",
    "\n",
    "# df.show(3, truncate=False)\n",
    "\n",
    "@F.udf(returnType=FloatType())\n",
    "def get_box_dim(box: STBox, dim: str, category: str):\n",
    "    pymeos_initialize()\n",
    "    try:\n",
    "        if dim == 'x':\n",
    "            if category == 'max':\n",
    "                return box.xmax()\n",
    "            else:\n",
    "                return box.xmin()\n",
    "        if dim == 'y':\n",
    "            if category == 'max':\n",
    "                return box.ymax()\n",
    "            else:\n",
    "                return box.ymin()\n",
    "        if dim == 'z':\n",
    "            if category == 'max':\n",
    "                return box.zmax()\n",
    "            else:\n",
    "                return box.zmin()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "@F.udf(returnType=TimestampType())\n",
    "def get_box_time(box: STBox, category: str):\n",
    "    pymeos_initialize()\n",
    "    try:\n",
    "        if category=='max':\n",
    "            return box.tmax()\n",
    "        else:\n",
    "            return box.tmin()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "#boundsdf = trajectories.agg(\n",
    "#        F.max(get_box_dim(\"STBox\", F.lit(\"x\"), F.lit(\"max\"))).alias(\"xmax\"),\n",
    "#        F.min(get_box_dim(\"STBox\", F.lit(\"x\"), F.lit(\"min\"))).alias(\"xmin\"),\n",
    "#        F.max(get_box_dim(\"STBox\", F.lit(\"y\"), F.lit(\"max\"))).alias(\"ymax\"),\n",
    "#        F.min(get_box_dim(\"STBox\", F.lit(\"y\"), F.lit(\"min\"))).alias(\"ymin\"),\n",
    "#        F.max(get_box_time(\"STBox\", F.lit(\"max\"))).alias(\"tmax\"),\n",
    "#        F.min(get_box_time(\"STBox\", F.lit(\"min\"))).alias(\"tmin\"),\n",
    "#    ).select(bounds_as_box(\"xmin\", \"xmax\", \"ymin\", \"ymax\", \"tmin\", \"tmax\").alias(\"bounds\"))\n",
    "\n",
    "\n",
    "boundsdf = df.agg(\n",
    "    F.max(F.col(\"x\")).alias(\"max_x\"),\n",
    "    F.min(F.col(\"x\")).alias(\"min_x\"),\n",
    "    F.max(F.col(\"y\")).alias(\"max_y\"),\n",
    "    F.min(F.col(\"y\")).alias(\"min_y\"),\n",
    "    F.max(F.col(\"t\").cast(\"timestamp\")).alias(\"max_t\"),\n",
    "    F.min(F.col(\"t\").cast(\"timestamp\")).alias(\"min_t\")\n",
    ").select(bounds_as_box(\"min_x\", \"max_x\", \"min_y\", \"max_y\", \"min_t\", \"max_t\").alias(\"bounds\"))\n",
    "\n",
    "#boundsdf.show(truncate=False)\n",
    "# bounds = STBoxWrap(boundsdf.collect()[0].bounds.__str__(), geodetic=True)\n",
    "#bounds = STBoxWrap('STBOX XT(((-177.02969360351562,-46.421356201171875),(177.816650390625,70.29727935791016)),[2022-06-27 00:00:00+00, 2022-06-27 00:15:00+00])')\n",
    "#boundsdf.unpersist()\n",
    "#df.unpersist()\n",
    "# bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38bff28d-e117-47e1-ad2f-b1dffc1b470a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STBoxWrap(STBOX XT(((-177.02969360351562,-46.421356201171875),(177.816650390625,70.29727935791016)),[2022-06-27 00:00:00+00, 2022-06-27 00:15:00+00]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = STBoxWrap(\n",
    "        \"STBOX XT(((-177.02969360351562,-46.421356201171875),(177.816650390625,70.29727935791016)),[2022-06-27 00:00:00+00, 2022-06-27 00:15:00+00])\",\n",
    "        geodetic=True)\n",
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921756e9-1848-45e6-a3e6-75a53b814649",
   "metadata": {},
   "source": [
    "## Generate the partitioning scheme and repartition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3d1e257-b1e6-4cd2-81fc-81e4a9405d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|tileid|                tile|\n",
      "+------+--------------------+\n",
      "|     0|SRID=4326;GEODSTB...|\n",
      "|     1|SRID=4326;GEODSTB...|\n",
      "|     2|SRID=4326;GEODSTB...|\n",
      "|     3|SRID=4326;GEODSTB...|\n",
      "|     4|SRID=4326;GEODSTB...|\n",
      "|     5|SRID=4326;GEODSTB...|\n",
      "|     6|SRID=4326;GEODSTB...|\n",
      "|     7|SRID=4326;GEODSTB...|\n",
      "|     8|SRID=4326;GEODSTB...|\n",
      "|     9|SRID=4326;GEODSTB...|\n",
      "|    10|SRID=4326;GEODSTB...|\n",
      "|    11|SRID=4326;GEODSTB...|\n",
      "|    12|SRID=4326;GEODSTB...|\n",
      "|    13|SRID=4326;GEODSTB...|\n",
      "|    14|SRID=4326;GEODSTB...|\n",
      "|    15|SRID=4326;GEODSTB...|\n",
      "|    16|SRID=4326;GEODSTB...|\n",
      "|    17|SRID=4326;GEODSTB...|\n",
      "|    18|SRID=4326;GEODSTB...|\n",
      "|    19|SRID=4326;GEODSTB...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 10:09:56 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we calculate the grid and partition accordingly\n",
    "gp = GridPartition(cells_per_side=3, bounds=bounds)\n",
    "grid = gp.as_spark_table()\n",
    "grid.cache()\n",
    "grid.show()\n",
    "grid.createOrReplaceTempView(\"grid\")\n",
    "grid.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2fd6c9-4f00-4a21-964f-d2c7a5c7c1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+\n",
      "|trajectoryId|partitionKey|          trajectory|\n",
      "+------------+------------+--------------------+\n",
      "|           0|           3|[POINT(-88.287482...|\n",
      "|           1|          15|[POINT(20.7604980...|\n",
      "|           1|          15|[POINT(20.7817632...|\n",
      "|           1|          15|[POINT(20.8010864...|\n",
      "|           1|          15|[POINT(20.8200905...|\n",
      "|           2|          15|[POINT(-3.5767822...|\n",
      "|           2|          15|[POINT(-3.5771636...|\n",
      "|           2|          15|[POINT(-3.5773138...|\n",
      "|           3|          15|[POINT(11.0194316...|\n",
      "|           3|          15|[POINT(11.0303476...|\n",
      "|           3|          15|[POINT(11.0403201...|\n",
      "|           4|          15|[POINT(27.8621292...|\n",
      "|           4|          15|[POINT(27.8548415...|\n",
      "|           4|          15|[POINT(27.8479957...|\n",
      "|           5|          15|[POINT(23.9701240...|\n",
      "|           5|          15|[POINT(23.9912662...|\n",
      "|           5|          15|[POINT(24.0126176...|\n",
      "|           5|          15|[POINT(24.0334231...|\n",
      "|           6|          12|[POINT(55.3646418...|\n",
      "|           6|          12|[POINT(55.3655878...|\n",
      "+------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "200\n",
      "root\n",
      " |-- trajectoryId: integer (nullable = true)\n",
      " |-- partitionKey: integer (nullable = true)\n",
      " |-- trajectory: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 10:10:05 ERROR Executor: Exception in task 2.0 in stage 43.0 (TID 434)]\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\t... 20 more\n",
      "24/05/15 10:10:05 WARN TaskSetManager: Lost task 2.0 in stage 43.0 (TID 434) (d57adcaf5d97 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\t... 20 more\n",
      "\n",
      "24/05/15 10:10:05 ERROR TaskSetManager: Task 2 in stage 43.0 failed 1 times; aborting job\n",
      "24/05/15 10:10:06 WARN TaskSetManager: Lost task 3.0 in stage 43.0 (TID 435) (d57adcaf5d97 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 43.0 failed 1 times, most recent failure: Lost task 2.0 in stage 43.0 (TID 434) (d57adcaf5d97 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\t... 20 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/09/temp_shuffle_92cff8ed-a685-4604-a16c-4c5fe76b46d8\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/3f/temp_shuffle_d954ea7b-ea16-46b1-8449-20018fdea9ca\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/08/temp_shuffle_75b35856-0ebe-467e-9a98-7171b30210fc\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/33/temp_shuffle_448bb144-5108-41b8-8d25-54dfe9fd171a\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/2c/temp_shuffle_00ba3325-bd51-4ef6-901d-dd992bb88354\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/1f/temp_shuffle_4e48172c-8d3f-4da7-a4d2-113ba81e05cb\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/15/temp_shuffle_16120ea1-ef38-4cdd-9c99-5024c6350bd0\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/29/temp_shuffle_8f48d6d1-7752-4cfa-91a7-3ca3b910ebab\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/25/temp_shuffle_2150ce02-3004-41db-bb35-ce3d7980c763\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/3d/temp_shuffle_d19d7c78-d6c9-44d0-8359-c24c6598fd19\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/2e/temp_shuffle_4a6c2212-2f3b-43bd-9f0f-6d5bdc7b9627\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/3b/temp_shuffle_44f7e958-b938-4db6-9ee3-f14daf680e96\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/24/temp_shuffle_b1433d43-1519-45f2-8d0c-71aa7ba029e5\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/03/temp_shuffle_626db43f-c9ea-43f5-aaad-3bba82e3196b\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/21/temp_shuffle_7f9b89e0-b4ef-4aad-acb5-c19a997c3410\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/15/temp_shuffle_0bde8d66-031f-4fa2-8f78-deb9ce1c196a\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/3c/temp_shuffle_f11637bf-c1a9-4cc9-bc20-51fa0978aed2\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/0d/temp_shuffle_25ad2442-0cc2-4f35-bef3-3b85114d90de\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/17/temp_shuffle_18611d5e-9d20-4e35-962f-3f2e6daec5d4\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/28/temp_shuffle_ea6d3639-1fd8-4c17-9651-46307e9055eb\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/2a/temp_shuffle_3c65c945-71c0-4035-9035-362ef43d1e8d\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/28/temp_shuffle_46a538bf-4519-4428-b40c-8d7fdab18474\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/0e/temp_shuffle_d88baa10-c50e-42bc-80d1-6dabb1d1f370\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/2d/temp_shuffle_cae2a7d8-01dd-4ac5-8584-4a8cbdb07d4a\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/1e/temp_shuffle_74ec7846-ffcf-49a2-a155-475b35d83416\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/16/temp_shuffle_6b084743-700d-40a0-b1c7-69590a9103de\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/3c/temp_shuffle_f3b5472e-62f4-460e-82c8-72e68945913d\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/3a/temp_shuffle_6e7feaf0-31fa-414d-8faf-65f82062c1b9\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/13/temp_shuffle_95a70413-1c9f-4d7a-bfae-f511259ea136\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/1b/temp_shuffle_04cdb09e-e381-4481-a351-ce7faee3b891\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/33/temp_shuffle_350055bd-5456-42f5-8980-46352dcc0bea\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/20/temp_shuffle_b0e3573e-185a-4fe1-a0d5-fd72283ef85f\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/2d/temp_shuffle_88ac3ac0-d8c0-4afb-9c4b-106640a951dd\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/37/temp_shuffle_808f367e-4200-4cc8-b1bc-7785e592de1c\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/3e/temp_shuffle_a4134de0-bbdd-404e-ae48-5a78fd63c399\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/11/temp_shuffle_60d0a4af-b626-400c-a1a0-98955b791d8c\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/15/temp_shuffle_c885d28b-ced6-4813-8bd3-e74d18c92bf8\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/31/temp_shuffle_e8c98e8a-4591-4efb-a187-8e22b81503c3\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/29/temp_shuffle_0d5dc8ce-4d29-4b78-8eae-4b76f1b95b61\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/39/temp_shuffle_b2d24252-614a-441d-812e-c771cd387982\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/34/temp_shuffle_6c4dafed-e517-4818-8b72-01e2a86e40bd\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/3b/temp_shuffle_e76a55cb-dedf-4f81-a7c4-7f4a190e0b46\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/3d/temp_shuffle_c69ee3ec-6dbc-4651-8c58-f5b08e63bf50\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/2c/temp_shuffle_0578afea-41ee-4076-a5f3-fce9169ac4f6\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/11/temp_shuffle_0b1a5277-13da-4bac-8302-a5198126e6be\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/3a/temp_shuffle_a979c330-2496-4ef9-8915-7b20a9535745\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/39/temp_shuffle_3bac0310-582d-4130-93b8-e3601a1d2a17\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/27/temp_shuffle_e9acc3b8-7695-4446-98e5-e3fc1bf59b4b\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/09/temp_shuffle_e8c0d428-ed44-4292-a438-5ee7bcd32c4a\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/21/temp_shuffle_d1b7fea4-a4c7-4e06-a287-08356bda32f0\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/14/temp_shuffle_911290f0-2719-4925-9788-a85d81abe0de\n",
      "24/05/15 10:10:06 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-71f11e65-77fd-40db-8fc7-7c149acb6de2/0c/temp_shuffle_0c0478f6-bfc8-4cd4-8fcb-328b926c9b68\n",
      "24/05/15 10:10:07 WARN TaskSetManager: Lost task 0.0 in stage 43.0 (TID 432) (d57adcaf5d97 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 43.0 failed 1 times, most recent failure: Lost task 2.0 in stage 43.0 (TID 434) (d57adcaf5d97 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\t... 20 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/15 10:10:07 WARN TaskSetManager: Lost task 1.0 in stage 43.0 (TID 433) (d57adcaf5d97 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 43.0 failed 1 times, most recent failure: Lost task 2.0 in stage 43.0 (TID 434) (d57adcaf5d97 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\t... 20 more\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 43.0 failed 1 times, most recent failure: Lost task 2.0 in stage 43.0 (TID 434) (d57adcaf5d97 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\nCaused by: java.io.EOFException\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\nCaused by: java.io.EOFException\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartitionKey\u001b[39m\u001b[38;5;124m'\u001b[39m], row)\n\u001b[1;32m     65\u001b[0m trajectoriesPartMapRdd \u001b[38;5;241m=\u001b[39m trajectoriesPartMap\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row: (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartitionKey\u001b[39m\u001b[38;5;124m'\u001b[39m], row))\u001b[38;5;241m.\u001b[39mpartitionBy(num_partitions)\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrajectoriesPartMapRdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#trajectoriesPartMap = trajectoriesPartMapRdd.toDF([\"partitionKey\", \"trajectorydata\"]) #.withColumn(\"trajectory\", F.col(\"trajectorydata.trajectory\").cast(TGeogPointSeqUDT())).withColumn(\"trajectoryId\", F.col(\"trajectorydata.trajectoryId\")).drop(\"trajectorydata\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 43.0 failed 1 times, most recent failure: Lost task 2.0 in stage 43.0 (TID 434) (d57adcaf5d97 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\nCaused by: java.io.EOFException\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\nCaused by: java.io.EOFException\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from typing import Iterator\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"trajectoryId\", IntegerType()),\n",
    "    StructField(\"partitionKey\", IntegerType()),\n",
    "    StructField(\"trajectory\", TGeogPointSeqUDT())\n",
    "])\n",
    "\n",
    "@F.udtf(returnType=schema)\n",
    "class RegularPartition:\n",
    "    def eval(self, row: Row):\n",
    "        pymeos_initialize()\n",
    "        sequence_id = row.seqId\n",
    "        trajectory = row.trajectory\n",
    "        grid = row.grid\n",
    "        partitioned = [(key, trajectory.at(tile)) for key, tile in enumerate(grid)]\n",
    "        # print(trajectory)\n",
    "        count = 0\n",
    "        responses = []\n",
    "        for partition_key, partition_traj in partitioned:\n",
    "            count += 1\n",
    "            if partition_traj is None:\n",
    "                continue\n",
    "            else:\n",
    "                seqs = partition_traj.segments()\n",
    "                #print(seqs)\n",
    "                for partition_traj_seq in seqs:\n",
    "                    responses.append((sequence_id, partition_key, partition_traj_seq))\n",
    "        for response in responses:\n",
    "            yield response\n",
    "\n",
    "spark.udtf.register(\"regularPartition\", RegularPartition)\n",
    "\n",
    "# spark.sql(\"SELECT collect_list(tile) FROM grid\").show()\n",
    "\n",
    "trajectoriesPartMap = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM regularPartition(\n",
    "        TABLE(\n",
    "                SELECT seqId, PointSeq AS trajectory, (SELECT collect_list(tile) FROM grid) AS grid\n",
    "                FROM trajectories\n",
    "        )\n",
    "    )\n",
    "\"\"\")\n",
    "trajectoriesPartMap.show()\n",
    "\n",
    "\n",
    "#spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "#print(trajectoriesPartMap.tail(1))\n",
    "#print(trajectoriesPartMap.count())\n",
    "#trajectoriesPartMap.createOrReplaceTempView(\"trajectoriesPartMap\")\n",
    "print(trajectoriesPartMap.rdd.getNumPartitions())\n",
    "num_partitions = gp.num_partitions()\n",
    "trajectoriesPartMap = trajectoriesPartMap.withColumn(\"trajectory\", F.col(\"trajectory\").cast(\"string\"))\n",
    "trajectoriesPartMap.printSchema()\n",
    "\n",
    "def partitionMapper(partitionIterator):\n",
    "    data = []\n",
    "    for row in partitionIterator:\n",
    "        #print(row)\n",
    "        yield (row['partitionKey'], row)\n",
    "\n",
    "trajectoriesPartMapRdd = trajectoriesPartMap.rdd.map(lambda row: (row['partitionKey'], row)).partitionBy(num_partitions)\n",
    "for row in trajectoriesPartMapRdd.take(1):\n",
    "    print(row)\n",
    "#trajectoriesPartMap = trajectoriesPartMapRdd.toDF([\"partitionKey\", \"trajectorydata\"]) #.withColumn(\"trajectory\", F.col(\"trajectorydata.trajectory\").cast(TGeogPointSeqUDT())).withColumn(\"trajectoryId\", F.col(\"trajectorydata.trajectoryId\")).drop(\"trajectorydata\")\n",
    "trajectoriesPartMap.printSchema()\n",
    "#trajectoriesPart.groupBy(\"trajectoryId\").agg(F.count(F.col(\"trajectory\")).alias(\"cnt\")).orderBy(\"cnt\", ascending=False).show()\n",
    "trajectoriesPartMap.orderBy(\"trajectoryId\")\n",
    "trajectoriesPartMap.show()\n",
    "print(trajectoriesPartMap.rdd.getNumPartitions())\n",
    "\n",
    "#RegularPartition(trajectories, trajectories.select(\"PointSeq\"), F.lit(gp.gridstr)).show()\n",
    "\n",
    "\n",
    "#spark.udtf.register(\"regularPartition\", RegularPartition)\n",
    "\n",
    "\n",
    "#print(gp.num_partitions())\n",
    "#num_partitions = gp.num_partitions()\n",
    "#gridstr = gp.gridstr\n",
    "\n",
    "# Register the udf\n",
    "#get_partition_key_udf = udf(gp.get_partition, IntegerType())\n",
    "\n",
    "#dfpoint = df.select(\"id\", \"Point\", \"PointStr\").withColumn(\"partitionKey\", get_partition_key_udf(col(\"Point\"), lit(gridstr)))\n",
    "#dfpoint.show()\n",
    "\n",
    "# dfpoint.select(\"_metadata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574254e-be8e-436c-893f-06b0f55e2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "trajectoriesPart = spark.table(\"trajectoriesPart\")\n",
    "\n",
    "#print(trajectoriesPart.tail(5))\n",
    "print(trajectoriesPart.count())\n",
    "print(trajectoriesPart.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "# This operation can be costly, for testing do with few data points (<= 1000).\n",
    "num_partitions = gp.num_partitions()\n",
    "#trajectoriesPart = trajectoriesPart.repartition(num_partitions, \"partitionKey\")\n",
    "#print(trajectoriesPart.rdd.getNumPartitions())\n",
    "trajectoriesPartRdd = trajectoriesPart.rdd.map(lambda x: (x['partitionKey'], x)).partitionBy(num_partitions)\n",
    "trajectoriesPart = trajectoriesPartRdd.toDF([\"partitionKey\", \"trajectorydata\"]).withColumn(\"trajectory\", F.col(\"trajectorydata.trajectory\")).withColumn(\"trajectoryId\", F.col(\"trajectorydata.trajectoryId\")).drop(\"trajectorydata\")\n",
    "#trajectoriesPart.groupBy(\"trajectoryId\").agg(F.count(F.col(\"trajectory\")).alias(\"cnt\")).orderBy(\"cnt\", ascending=False).show()\n",
    "trajectoriesPart.orderBy(\"trajectoryId\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb56d5b-23d4-45da-b879-6c34e3ffe99d",
   "metadata": {},
   "source": [
    "## Show the partition distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a3aff-afb7-4788-aa13-9a110f68f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectoriesPart.groupBy(\"partitionKey\").count().show()\n",
    "\n",
    "#for idx, item in datardd.take(5):\n",
    "#    print(idx, item)\n",
    "\n",
    "trajectoriesPartRdd = trajectoriesPart.rdd\n",
    "print(trajectoriesPartRdd.getNumPartitions())\n",
    "\n",
    "# Function to count rows per partition\n",
    "def count_in_partition(idx, iterator):\n",
    "    cnt = 0\n",
    "    for _ in iterator:\n",
    "        cnt += 1\n",
    "    return [(idx, cnt)]\n",
    "\n",
    "# Using mapPartitionsWithIndex to count rows per partition\n",
    "partition_counts = trajectoriesPartRdd.mapPartitionsWithIndex(count_in_partition).collect()\n",
    "    \n",
    "# Print the results\n",
    "for partition_id, cnt in partition_counts:\n",
    "    print(f\"Partition {partition_id} has {cnt} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13bba3-41e6-43bb-b426-14209c03fde4",
   "metadata": {},
   "source": [
    "## Plot the Projection of the grid and the distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d3863-0dd1-4887-9166-b6e994d95a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "# dfpoint = datardd.toDF(['partitionKey', 'Point'])\n",
    "trajectoriesPart.printSchema()\n",
    "\n",
    "# Create a bounding box\n",
    "# bounding_box = box(bounds.xmin(), bounds.ymin(), bounds.xmax(), bounds.ymax())\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world.plot(ax=ax, alpha=0.3)\n",
    "\n",
    "gridstr = [row.tile.__str__() for row in grid.collect()]\n",
    "\n",
    "colors = plt.cm.Accent(np.linspace(0, 1, len(gridstr)))\n",
    "# Adding an alpha value when creating the color ma<p\n",
    "alpha_value = 1.0\n",
    "color_map = {i: colors[i][:3].tolist() + [alpha_value] for i, tile in enumerate(gridstr)}\n",
    "for i, tilestr in enumerate(gridstr):\n",
    "    tile: STBox = STBoxWrap(tilestr).set_srid(0)\n",
    "    tile.plot_xy(axes=ax, color=\"black\", draw_filling=False)\n",
    "\n",
    "for idx, row in enumerate(trajectoriesPart.toLocalIterator()):\n",
    "    if i == 100:\n",
    "        break\n",
    "    if row.trajectory.num_instants() <= 1:\n",
    "        continue\n",
    "    traj = row.trajectory\n",
    "    #if type(point)==str:\n",
    "    #    point = TGeogPointInst(point)\n",
    "    #tpointseq = point.to_sequence(interp).plot(axes=ax,label=\"tpoint\", color=color_map[row.partitionKey % gp.num_partitions()], facecolors='none')\n",
    "    traj.plot(axes=ax,label=\"trajectory\", color=color_map[row.partitionKey % gp.num_partitions()])\n",
    "plt.title(\"XY Tile Partition and Trajectory Projection\")\n",
    "plt.xlabel(\"Lon\")\n",
    "plt.ylabel(\"Lat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f0f02-2f63-4c09-80c8-a44a53c1de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from pymeos import *\n",
    "\n",
    "class MeosWrap:  \n",
    "    def __setstate__(self, state):\n",
    "        pymeos_initialize()\n",
    "        #print(\"Im being unpickled: \", state)\n",
    "        self._inner = self(state)._inner\n",
    "\n",
    "    def __getstate__(self):\n",
    "        pymeos_initialize()\n",
    "        #print(\"Im being pickled: \", self.__str__())\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class TGeogPointInstWrap(TGeogPointInst, MeosWrap):\n",
    "    def __setstate__(self, state):\n",
    "        pymeos_initialize()\n",
    "        #print(\"Im being unpickled: \", state)\n",
    "        self._inner = TGeogPointInst(state)._inner\n",
    "\n",
    "class TGeogPointInstWrap2(TGeogPointInst):\n",
    "    def __setstate__(self, state):\n",
    "        print(\"Im being unpickled: \", state)\n",
    "        self._inner = TGeogPointInst(state, srid=0)._inner\n",
    "\n",
    "    def __getstate__(self):\n",
    "        print(\"Im being pickled: \", self.__str__())\n",
    "        return self.__str__()\n",
    "\n",
    "pymeos_initialize()\n",
    "\n",
    "tpoint = TGeogPointInstWrap(\"POINT(40.87294006347656 1.9229736328125)@2022-06-27 00:00:00+00\", srid=4326)\n",
    "tpoint2 = TGeogPointInstWrap(\"POINT(100.87294006347656 105.9229736328125)@2022-06-28 23:05:00+00\", srid=4326)\n",
    "tpoint3 = TGeogPointInstWrap(\"POINT(200.87294006347656 300.9229736328125)@2022-06-29 12:30:00+00\", srid=4326)\n",
    "tpointseq = TGeogPointSeq(\"\"\"\n",
    "[POINT(26.717873431266625 35.62088788565943)@2022-06-27 00:00:10+00, POINT(26.737689971923828 35.60289001464844)@2022-06-27 00:00:20+00, POINT(26.757373809814453 35.58494567871094)@2022-06-27 00:00:30+00, POINT(26.775684356689453 35.5682373046875)@2022-06-27 00:00:40+00, POINT(26.796297113946146 35.54947675284693)@2022-06-27 00:00:50+00, POINT(26.81591033935547 35.5316162109375)@2022-06-27 00:01:00+00, POINT(26.83492457613032 35.514236708818856)@2022-06-27 00:01:10+00, POINT(26.853098767869017 35.49771066439354)@2022-06-27 00:01:20+00, POINT(26.87454548287899 35.47811217227225)@2022-06-27 00:01:30+00, POINT(26.8938299950133 35.4604688741393)@2022-06-27 00:01:40+00, POINT(26.913289820894285 35.4426859192929)@2022-06-27 00:01:50+00, POINT(26.932048391788566 35.42536848682468)@2022-06-27 00:02:00+00, POINT(26.95144977975399 35.407585531978285)@2022-06-27 00:02:10+00, POINT(26.971492767333984 35.38920593261719)@2022-06-27 00:02:20+00, POINT(26.990135679853726 35.372159278998936)@2022-06-27 00:02:30+00, POINT(27.009770819481385 35.35419011520127)@2022-06-27 00:02:40+00, POINT(27.02870470412234 35.33682613049523)@2022-06-27 00:02:50+00, POINT(27.04782485961914 35.31925964355469)@2022-06-27 00:03:00+00, POINT(27.066981538813167 35.30177229541843)@2022-06-27 00:03:10+00, POINT(27.086266050947476 35.284082445047666)@2022-06-27 00:03:20+00, POINT(27.10470199584961 35.26702880859375)@2022-06-27 00:03:30+00, POINT(27.12261199951172 35.25054931640625)@2022-06-27 00:03:40+00, POINT(27.143783569335938 35.23109436035156)@2022-06-27 00:03:50+00, POINT(27.162967604033803 35.21342468261719)@2022-06-27 00:04:00+00, POINT(27.1823618363361 35.19561767578125)@2022-06-27 00:04:10+00, POINT(27.200522909359055 35.17881774902344)@2022-06-27 00:04:20+00, POINT(27.220745086669922 35.16025349245233)@2022-06-27 00:04:30+00, POINT(27.239112854003906 35.14326192564884)@2022-06-27 00:04:40+00, POINT(27.25788116455078 35.12599104541843)@2022-06-27 00:04:50+00, POINT(27.277507781982422 35.10783567266949)@2022-06-27 00:05:00+00, POINT(27.295703887939453 35.09093721034163)@2022-06-27 00:05:10+00, POINT(27.315150669642858 35.073028564453125)@2022-06-27 00:05:20+00, POINT(27.333412170410156 35.056116136453916)@2022-06-27 00:05:30+00, POINT(27.351248604910715 35.039520263671875)@2022-06-27 00:05:40+00, POINT(27.370698889907526 35.02143859863281)@2022-06-27 00:05:50+00, POINT(27.389373779296875 35.004024182335804)@2022-06-27 00:06:00+00, POINT(27.408599853515625 34.98605501853813)@2022-06-27 00:06:10+00, POINT(27.427139282226562 34.96878413830773)@2022-06-27 00:06:20+00, POINT(27.445865553252553 34.95135498046875)@2022-06-27 00:06:30+00, POINT(27.464618682861328 34.9339165121822)@2022-06-27 00:06:40+00, POINT(27.484760284423828 34.91515596034163)@2022-06-27 00:06:50+00, POINT(27.50343167051977 34.897796630859375)@2022-06-27 00:07:00+00, POINT(27.52246856689453 34.88010212526483)@2022-06-27 00:07:10+00, POINT(27.539806365966797 34.863901946504235)@2022-06-27 00:07:20+00, POINT(27.55550462372449 34.84918212890625)@2022-06-27 00:07:30+00, POINT(27.57473069794324 34.83119201660156)@2022-06-27 00:07:40+00, POINT(27.59176254272461 34.81525485798464)@2022-06-27 00:07:50+00, POINT(27.59176254272461 34.81525485798464)@2022-06-27 00:12:40+00)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "tpointseq2 = TGeogPointSeq(\"\"\"\n",
    "[POINT(26.717873431266625 35.62088788565943)@2022-06-27 00:00:10+00, POINT(26.737689971923828 35.60289001464844)@2022-06-27 00:00:20+00, POINT(26.757373809814453 35.58494567871094)@2022-06-27 00:00:30+00, POINT(26.775684356689453 35.5682373046875)@2022-06-27 00:00:40+00, POINT(26.796297113946146 35.54947675284693)@2022-06-27 00:00:50+00, POINT(26.81591033935547 35.5316162109375)@2022-06-27 00:01:00+00, POINT(26.83492457613032 35.514236708818856)@2022-06-27 00:01:10+00, POINT(26.853098767869017 35.49771066439354)@2022-06-27 00:01:20+00, POINT(26.87454548287899 35.47811217227225)@2022-06-27 00:01:30+00, POINT(26.8938299950133 35.4604688741393)@2022-06-27 00:01:40+00, POINT(26.913289820894285 35.4426859192929)@2022-06-27 00:01:50+00, POINT(26.932048391788566 35.42536848682468)@2022-06-27 00:02:00+00, POINT(26.95144977975399 35.407585531978285)@2022-06-27 00:02:10+00, POINT(26.971492767333984 35.38920593261719)@2022-06-27 00:02:20+00, POINT(26.990135679853726 35.372159278998936)@2022-06-27 00:02:30+00, POINT(27.009770819481385 35.35419011520127)@2022-06-27 00:02:40+00, POINT(27.02870470412234 35.33682613049523)@2022-06-27 00:02:50+00, POINT(27.04782485961914 35.31925964355469)@2022-06-27 00:03:00+00, POINT(27.066981538813167 35.30177229541843)@2022-06-27 00:03:10+00, POINT(27.086266050947476 35.284082445047666)@2022-06-27 00:03:20+00, POINT(27.10470199584961 35.26702880859375)@2022-06-27 00:03:30+00, POINT(27.12261199951172 35.25054931640625)@2022-06-27 00:03:40+00, POINT(27.143783569335938 35.23109436035156)@2022-06-27 00:03:50+00, POINT(27.162967604033803 35.21342468261719)@2022-06-27 00:04:00+00, POINT(27.1823618363361 35.19561767578125)@2022-06-27 00:04:10+00, POINT(27.200522909359055 35.17881774902344)@2022-06-27 00:04:20+00, POINT(27.220745086669922 35.16025349245233)@2022-06-27 00:04:30+00, POINT(27.239112854003906 35.14326192564884)@2022-06-27 00:04:40+00, POINT(27.25788116455078 35.12599104541843)@2022-06-27 00:04:50+00, POINT(27.277507781982422 35.10783567266949)@2022-06-27 00:05:00+00, POINT(27.295703887939453 35.09093721034163)@2022-06-27 00:05:10+00, POINT(27.315150669642858 35.073028564453125)@2022-06-27 00:05:20+00, POINT(27.333412170410156 35.056116136453916)@2022-06-27 00:05:30+00, POINT(27.351248604910715 35.039520263671875)@2022-06-27 00:05:40+00, POINT(27.370698889907526 35.02143859863281)@2022-06-27 00:05:50+00, POINT(27.389373779296875 35.004024182335804)@2022-06-27 00:06:00+00, POINT(27.408599853515625 34.98605501853813)@2022-06-27 00:06:10+00, POINT(27.427139282226562 34.96878413830773)@2022-06-27 00:06:20+00, POINT(27.445865553252553 34.95135498046875)@2022-06-27 00:06:30+00, POINT(27.464618682861328 34.9339165121822)@2022-06-27 00:06:40+00, POINT(27.484760284423828 34.91515596034163)@2022-06-27 00:06:50+00, POINT(27.50343167051977 34.897796630859375)@2022-06-27 00:07:00+00, POINT(27.52246856689453 34.88010212526483)@2022-06-27 00:07:10+00, POINT(27.539806365966797 34.863901946504235)@2022-06-27 00:07:20+00, POINT(27.55550462372449 34.84918212890625)@2022-06-27 00:07:30+00, POINT(27.57473069794324 34.83119201660156)@2022-06-27 00:07:40+00, POINT(27.59176254272461 34.81525485798464)@2022-06-27 00:07:50+00, POINT(27.59176254272461 34.81525485798464)@2022-06-27 00:12:40+00)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def bds(boxes: pd.Series):\n",
    "    return reduce(lambda b1, b2: b1.union(b2), boxes)\n",
    "\n",
    "def bb(x):\n",
    "    pymeos_initialize()\n",
    "    return x.bounding_box()\n",
    "\n",
    "dftest = [tpoint, tpoint2, tpoint3]\n",
    "tagg = TemporalPointExtentAggregator.start_aggregation()\n",
    "tagg.add(tpointseq)\n",
    "tagg.add(tpointseq2)\n",
    "\n",
    "agg = tagg.aggregation()\n",
    "print(agg)\n",
    "print(\"Im the bounds..\", )\n",
    "\n",
    "hexa = pickle.dumps(tpoint)\n",
    "pickle.loads(hexa)\n",
    "\n",
    "TGeogPointInst('Point(44.53415175615731 -88.8822081030869)@2022-06-27 00:00:10')\n",
    "\n",
    "#print(dftest)\n",
    "\n",
    "#TemporalPointExtentAggregator.aggregate(dftest)\n",
    "#aggregator.add(tpoint)\n",
    "#print(sorted([tpoint2, tpoint]))\n",
    "#aggregator.aggregation()\n",
    "#print(tseq, type(tseq))\n",
    "#dftest.sum().__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f7af5-7837-4acd-95a5-d9fce6a72de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a68d55-c89b-4bb8-ab74-115562602b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
